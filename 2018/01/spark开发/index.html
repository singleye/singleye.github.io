<!DOCTYPE html>


<html lang="zh-cn" data-theme="">
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Spark开发 - singleye</title>

<meta name="description" content="# 使用scala进行开发 ## Step1: 安装sbt $ curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo $ sudo yum install sbt 配置源：
# cat ~/.">


    <meta name="keywords" content="spark,scala,python">




<link rel="icon" type="image/x-icon" href="http://www.singleye.net/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://www.singleye.net/favicon.png">








    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css" integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos&#43;CZt8XtPejb&#43;nJdVE=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css" integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT&#43;/cHwdlfBEzZwqiI=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css" integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00=">
    












    

    





    
    
        
    
    

    
        <script src="/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js" type="text/javascript" charset="utf-8" integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script>
    



















    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header">
            
                <div class="header-top">
    <div class="header-top-left">
        <h1 class="site-title noselect">
    <a href="/">singleye</a>
</h1>

        







    
        <div class="theme-switcher">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>


</span>

        </div>
    

    <script>
        const STORAGE_KEY = 'user-color-scheme'
        const defaultTheme = "light"

        let currentTheme
        let switchButton
        let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

        function switchTheme(e) {
            currentTheme = (currentTheme === 'dark') ? 'light' : 'dark';
            if (localStorage) localStorage.setItem(STORAGE_KEY, currentTheme);
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        const autoChangeScheme = e => {
            currentTheme = e.matches ? 'dark' : 'light'
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        document.addEventListener('DOMContentLoaded', function () {
            switchButton = document.querySelector('.theme-switcher')
            currentTheme = detectCurrentScheme()

            if (currentTheme === 'auto') {
                autoChangeScheme(autoDefinedScheme);
                autoDefinedScheme.addListener(autoChangeScheme);
            } else {
                document.documentElement.setAttribute('data-theme', currentTheme)
            }

            if (switchButton) {
                switchButton.addEventListener('click', switchTheme, false)
            }

            showContent();
        })

        function detectCurrentScheme() {
            if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
                return localStorage.getItem(STORAGE_KEY)
            }
            if (defaultTheme) {
                return defaultTheme
            }
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function showContent() {
            document.body.style.visibility = 'visible';
            document.body.style.opacity = 1;
        }

        function changeGiscusTheme (theme) {
            function sendMessage(message) {
              const iframe = document.querySelector('iframe.giscus-frame');
              if (!iframe) return;
              iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
            }

            sendMessage({
              setConfig: {
                theme: theme
              }
            });
        }
    </script>


        <ul class="social-icons noselect">







    <li>
            <a href="/index.xml" title="RSS" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 1 0 2 0a1 1 0 1 0 -2 0" /><path d="M4 4a16 16 0 0 1 16 16" /><path d="M4 11a9 9 0 0 1 9 9" /></svg>


</span>

            </a>
        </li>
    

</ul>

    </div>
    <div class="header-top-right">

    </div>
</div>


    <nav class="noselect">
        
        
        <a class="" href="http://www.singleye.net/" title="">Home</a>
        
        <a class="" href="http://www.singleye.net/categories" title="">Categories</a>
        
        <a class="" href="http://www.singleye.net/tags" title="">Tags</a>
        
        <a class="" href="https://github.com/singleye" title="">GitHub</a>
        
    </nav>



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>





            
        </header>
        <main id="main" tabindex="-1">
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">Spark开发</h1>
                

            </header>
            



<div class="post-info noselect">
    
        <div class="post-date dt-published">
            <time datetime="2018-01-11">2018-01-11</time>
            
        </div>
    

    <a class="post-hidden-url u-url" href="/2018/01/spark%E5%BC%80%E5%8F%91/">/2018/01/spark%E5%BC%80%E5%8F%91/</a>
    <a href="http://www.singleye.net/" class="p-name p-author post-hidden-author h-card" rel="me"></a>


    <div class="post-taxonomies">
        
            <ul class="post-categories">
                
                    
                    <li><a href="/categories/bigdata/">BigData</a></li>
                
                    
                    <li><a href="/categories/spark/">Spark</a></li>
                
            </ul>
        
        
            <ul class="post-tags">
                
                    
                    <li><a href="/tags/spark/">#Spark</a></li>
                
                    
                    <li><a href="/tags/scala/">#Scala</a></li>
                
                    
                    <li><a href="/tags/python/">#Python</a></li>
                
            </ul>
        
        
    </div>
</div>

        </div>
        

  
  




  
  
  
  <details class="toc noselect">
    <summary>Table of Contents</summary>
    <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#使用scala进行开发">使用scala进行开发</a>
      <ul>
        <li><a href="#step1-安装sbt">Step1: 安装sbt</a></li>
        <li><a href="#step2-创建项目">Step2: 创建项目</a></li>
        <li><a href="#step3-编写代码">Step3: 编写代码</a></li>
        <li><a href="#step4-编译打包">Step4: 编译打包</a></li>
        <li><a href="#step5-部署运行">Step5: 部署运行</a></li>
      </ul>
    </li>
    <li><a href="#使用python进行开发">使用python进行开发</a>
      <ul>
        <li><a href="#使用hdp环境提交任务">使用HDP环境提交任务</a></li>
        <li><a href="#在hdp环境运行jupyter-server">在HDP环境运行jupyter server</a></li>
        <li><a href="#非hdp环境开发">非HDP环境开发</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#api">API</a></li>
    <li><a href="#sparksession-vs-sparkcontext">SparkSession vs SparkContext</a>
      <ul>
        <li><a href="#sparkconf">SparkConf</a></li>
        <li><a href="#sparksql">SparkSQL</a></li>
        <li><a href="#访问catalog-metadata">访问Catalog Metadata</a></li>
      </ul>
    </li>
    <li><a href="#dataframe-和-datasets">DataFrame 和 Datasets</a>
      <ul>
        <li><a href="#dataset">Dataset：</a></li>
        <li><a href="#dataframe">DataFrame：</a></li>
        <li><a href="#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow"><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row">Row</a></a></li>
        <li><a href="#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn"><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a></a></li>
        <li><a href="#处理时间">处理时间</a></li>
        <li><a href="#import-sparkimplicits_">import spark.implicits._</a></li>
      </ul>
    </li>
    <li><a href="#编程陷阱">编程陷阱</a>
      <ul>
        <li><a href="#陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表">陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
  </details>
  



<script>
  var toc = document.querySelector(".toc");
  if (toc) {
    toc.addEventListener("click", function () {
      if (event.target.tagName !== "A") {
        event.preventDefault();
        if (this.open) {
          this.open = false;
          this.classList.remove("expanded");
        } else {
          this.open = true;
          this.classList.add("expanded");
        }
      }
    });
  }
</script>

        <div class="content e-content">
            <h2 id="使用scala进行开发" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8scala%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91">
        #
    </a>
    使用scala进行开发
</div>
</h2>
<h3 id="step1-安装sbt" >
<div>
    <a href="#step1-%e5%ae%89%e8%a3%85sbt">
        ##
    </a>
    Step1: 安装sbt
</div>
</h3>
<pre><code>
$ curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
$ sudo yum install sbt
</code></pre>
<p>配置源：</p>
<pre><code>
# cat ~/.sbt/repositories
[repositories]
  # 本地源
  local
  # 阿里源
  aliyun: http://maven.aliyun.com/nexus/content/groups/public/
  typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly
  sonatype-oss-releases
  maven-central
  sonatype-oss-snapshots
</code></pre>
<h3 id="step2-创建项目" >
<div>
    <a href="#step2-%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae">
        ##
    </a>
    Step2: 创建项目
</div>
</h3>
<p>可以使用Giter8模版创建项目。</p>
<pre><code>
$ sbt new imarios/frameless.g8
</code></pre>
<p>spark相关的几个模版：</p>
<ul>
<li>holdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project).</li>
<li>imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark)</li>
<li>nttdata-oss/basic-spark-project.g8 (Spark basic project.)</li>
<li>spark-jobserver/spark-jobserver.g8 (Template for Spark Jobserver)</li>
</ul>
<h3 id="step3-编写代码" >
<div>
    <a href="#step3-%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81">
        ##
    </a>
    Step3: 编写代码
</div>
</h3>
<p>创建出的项目目录中包含一下主要条目：</p>
<pre><code>
$ ls
build.sbt  project  src  target
</code></pre>
<p>编写的代码放在 &lsquo;src/main/scala/&rsquo; 目录中：</p>
<pre><code>
$ ls src/main/scala/
SimpleApp.scala

$ cat src/main/scala/SimpleApp.scala
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "people.txt" // Should be some file on your system
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println(s"Lines with a: $numAs, Lines with b: $numBs")
    spark.stop()
  }
}
</code></pre>
<h3 id="step4-编译打包" >
<div>
    <a href="#step4-%e7%bc%96%e8%af%91%e6%89%93%e5%8c%85">
        ##
    </a>
    Step4: 编译打包
</div>
</h3>
<p>程序开发好之后首先需要编译打包：</p>
<pre><code>
$ sbt package
[info] Loading project definition from /home/spark/scala/frameless/project
[info] Updating {file:/home/spark/scala/frameless/project/}frameless-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Set current project to frameless (in build file:/home/spark/scala/frameless/)
[info] Updating {file:/home/spark/scala/frameless/}root...
[info] Resolving org.sonatype.oss#oss-parent;9 ...
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.11/scala-library-2.11.11.jar ...
[info]  [SUCCESSFUL ] org.scala-lang#scala-library;2.11.11!scala-library.jar (94788ms)

...

[info] downloading https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar ...
[info]  [SUCCESSFUL ] jline#jline;2.14.3!jline.jar (4836ms)
[info] Done updating.
[info] Compiling 1 Scala source to /home/spark/scala/frameless/target/scala-2.11/classes...
[info] 'compiler-interface' not yet compiled for Scala 2.11.11. Compiling...
[info]   Compilation completed in 24.981 s
[info] Packaging /home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar ...
[info] Done packaging.
[success] Total time: 2795 s, completed Jan 16, 2018 3:03:12 PM
</code></pre>
<p>编译打包成功后的文件输出在 &rsquo;target/scala-2.11/&rsquo; 目录中：</p>
<p>/home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar</p>
<h3 id="step5-部署运行" >
<div>
    <a href="#step5-%e9%83%a8%e7%bd%b2%e8%bf%90%e8%a1%8c">
        ##
    </a>
    Step5: 部署运行
</div>
</h3>
<pre><code>
# su - spark
$ export SPARK_MAJOR_VERSION=2

$ spark-submit --class SimpleApp --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 target/scala-2.11/frameless_2.11-0.1.jar
SPARK_MAJOR_VERSION is set to 2, using Spark2
Warning: Master yarn-client is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
18/01/17 15:20:18 INFO SparkContext: Running Spark version 2.2.0.2.6.3.0-235
18/01/17 15:20:19 INFO SparkContext: Submitted application: Simple Application
18/01/17 15:20:19 INFO SecurityManager: Changing view acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing view acls groups to:
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls groups to:
18/01/17 15:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view
 permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
18/01/17 15:20:20 INFO Utils: Successfully started service 'sparkDriver' on port 43573.

...

18/01/17 15:21:30 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
18/01/17 15:21:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, bdtest-002, executor 1, partition 0, NODE_LOCAL, 4737 bytes)
18/01/17 15:21:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bdtest-002:44065 (size: 3.7 KB, free: 93.2 MB)
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.4:44312
18/01/17 15:21:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 141 bytes
18/01/17 15:21:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 96 ms on bdtest-002 (executor 1) (1/1)
18/01/17 15:21:30 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool
18/01/17 15:21:30 INFO DAGScheduler: ResultStage 3 (count at SimpleApp.scala:10) finished in 0.096 s
18/01/17 15:21:30 INFO DAGScheduler: Job 1 finished: count at SimpleApp.scala:10, took 0.289165 s
Lines with a: 1, Lines with b: 0
18/01/17 15:21:30 INFO AbstractConnector: Stopped Spark@17b03218{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
18/01/17 15:21:30 INFO SparkUI: Stopped Spark web UI at http://192.168.1.5:4042
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Shutting down all executors
18/01/17 15:21:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
18/01/17 15:21:30 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Stopped
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/01/17 15:21:31 INFO MemoryStore: MemoryStore cleared
18/01/17 15:21:31 INFO BlockManager: BlockManager stopped
18/01/17 15:21:31 INFO BlockManagerMaster: BlockManagerMaster stopped
18/01/17 15:21:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/01/17 15:21:31 INFO SparkContext: Successfully stopped SparkContext
18/01/17 15:21:31 INFO ShutdownHookManager: Shutdown hook called
18/01/17 15:21:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-730219ef-2995-4225-8743-5769fa6269db
</code></pre>
<ul>
<li>&ndash;master: 指定spark driver的运行模式。
<ul>
<li>yarn-cluster: spark driver运行在yarn的application master进程中。如果应用只是进行计算可以使用这种方式运行，如果需要跟前台进行交互则可以考虑yarn-client模式。</li>
<li>yarn-client: spark driver运行在client进程中，此时application master只负责向yarn申请资源。</li>
</ul>
</li>
</ul>
<h2 id="使用python进行开发" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8python%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91">
        #
    </a>
    使用python进行开发
</div>
</h2>
<h3 id="使用hdp环境提交任务" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8hdp%e7%8e%af%e5%a2%83%e6%8f%90%e4%ba%a4%e4%bb%bb%e5%8a%a1">
        ##
    </a>
    使用HDP环境提交任务
</div>
</h3>
<p>创建python文件<code>pi.py</code></p>
<pre><code>import sys
from random import random
from operator import add

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;PythonPi&quot;)\.getOrCreate()

partitions = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 2
n = 100000 * partitions

def f(any):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0

count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
print(&quot;Pi is roughly %f&quot; % (4.0 * count / n))

spark.stop()
</code></pre>
<p>运行<code>pi.py</code>，这里指定使用yarn的cluster模式</p>
<pre><code>SPARK_MAJOR_VERSION=2 spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --deploy pi.py 10
</code></pre>
<h3 id="在hdp环境运行jupyter-server" >
<div>
    <a href="#%e5%9c%a8hdp%e7%8e%af%e5%a2%83%e8%bf%90%e8%a1%8cjupyter-server">
        ##
    </a>
    在HDP环境运行jupyter server
</div>
</h3>
<p>使用yarn的cluster模式运行，在运行期间会始终在yarn中占用资源，如果需要长期运行，使用<code>--master local</code>比较好</p>
<pre><code>XDG_RUNTIME_DIR=&quot;&quot; SPARK_MAJOR_VERSION=2 PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 121.43.171.231' pyspark --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m
</code></pre>
<p>为jupyter server设置密码等操作参考<a href="https://jupyter-notebook.readthedocs.io/en/stable/public_server.html">jupyter文档</a></p>
<p>运行后访问服务器8888端口即可访问jupyter服务</p>
<h3 id="非hdp环境开发" >
<div>
    <a href="#%e9%9d%9ehdp%e7%8e%af%e5%a2%83%e5%bc%80%e5%8f%91">
        ##
    </a>
    非HDP环境开发
</div>
</h3>
<p>确保本机有一份spark二进制文件，比如<code>spark-2.2.1-bin-hadoop2.7</code></p>
<p>安装pyspark</p>
<pre><code>pip install pyspark
</code></pre>
<p>设置SPARK_HOME（可选）</p>
<pre><code>export SPARK_HOME=/home/xxx/spark-2.2.1-bin-hadoop2.7
</code></pre>
<p>通过代码提交任务</p>
<pre><code>import os
from pyspark import SparkConf, SparkContext

# if `SPARK_HOME` is undefined yet
if 'SPARK_HOME' not in os.environ:
    os.environ['SPARK_HOME'] = '/home/xxx/spark-2.2.1-bin-hadoop2.7'

conf = SparkConf().setAppName('Demo').setMaster('yarn').set('spark.yarn.deploy.mode', 'cluster')
sc = SparkContext(conf=conf)

# Do something with sc...
</code></pre>
<p>或者使用<code>SparkSession</code> API</p>
<pre><code>from spark.sql import SparkSession
spark = (SparkSession.builder
          .master(&quot;yarn&quot;)
          .appName(&quot;Demo&quot;)
          .config(&quot;spark.yarn.deploy.mode&quot;, &quot;cluster&quot;)
          .getOrCreate())

# Do something with spark...
</code></pre>
<h1 id="oozie自动化" >
<div>
    <a href="#oozie%e8%87%aa%e5%8a%a8%e5%8c%96">
        ##
    </a>
    oozie自动化
</div>
</h1>
<p>TODO</p>
<h1 id="development" >
<div>
    <a href="#development">
        ##
    </a>
    Development
</div>
</h1>
<p>Spark 2.0之前的开发接口主要是RDD（Resilient Distributed Dataset），从2.0之后原有<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD接口</a>仍然支持，但RDD被Dataset取代，如果使用Dataset编程的话需要使用<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>。</p>
<h2 id="api" >
<div>
    <a href="#api">
        #
    </a>
    API
</div>
</h2>
<p><a href="http://spark.apache.org/docs/1.3.1/api/scala/index.html">http://spark.apache.org/docs/1.3.1/api/scala/index.html</a></p>
<h2 id="sparksession-vs-sparkcontext" >
<div>
    <a href="#sparksession-vs-sparkcontext">
        #
    </a>
    SparkSession vs SparkContext
</div>
</h2>
<p>Spark 2.0之前有3个主要的连接对象：</p>
<ul>
<li>SparkContext: 建立与Spark执行环境相关的连接，用于创建RDD</li>
<li>SqlContext：利用SparkContext背后的SparkSQL建立连接</li>
<li>HiveContext：创建访问hive的接口</li>
</ul>
<p>从Spark 2.0开始，Datasets/Dataframes成为主要的数据访问接口，SparkSession(org.apache.spark.sql.SparkSession)成为主要的访问Spark执行环境的接口。</p>
<h3 id="sparkconf" >
<div>
    <a href="#sparkconf">
        ##
    </a>
    SparkConf
</div>
</h3>
<p>Spark 2.0之前需要先创建SparkConf，在创建SparkContext。</p>
<pre><code>
//set up the spark configuration and create contexts
val sparkConf = new SparkConf().setAppName("SparkSessionZipsExample").setMaster("local")
// your handle to SparkContext to access other context like SQLContext
val sc = new SparkContext(sparkConf).set("spark.some.config.option", "some-value")
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
</code></pre>
<p>Spark 2.0之后可以不用显示的创建SparkConf对象就可以创建SparkSession对象。</p>
<pre><code>
// Create a SparkSession. No need to create SparkContext
// You automatically get it as part of the SparkSession
val warehouseLocation = "file:${system:user.dir}/spark-warehouse"
val spark = SparkSession
   .builder()
   .appName("SparkSessionZipsExample")
   .config("spark.sql.warehouse.dir", warehouseLocation)
   .enableHiveSupport()
   .getOrCreate()
</code></pre>
<p>在SparkSession创建完后，Spark的运行时配置属性还可以通过SparkSession.conf()被修改：</p>
<pre><code>
//set new runtime options
spark.conf.set("spark.sql.shuffle.partitions", 6)
spark.conf.set("spark.executor.memory", "2g")
//get all settings
val configMap:Map[String, String] = spark.conf.getAll()
</code></pre>
<ul>
<li><a href="http://blog.javachen.com/2015/06/07/spark-configuration.html">Spark conf参考</a></li>
</ul>
<h3 id="sparksql" >
<div>
    <a href="#sparksql">
        ##
    </a>
    SparkSQL
</div>
</h3>
<p>2.0之前需要通过创建SqlContext来使用SparkSQL。</p>
<pre><code>
val sc = new SparkContext(sparkConf).set("spark.some.config.option", "some-value")
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
</code></pre>
<p>2.0后通过SparkSession可以很方便的访问SparkSession.sql()来使用SparkSQL。</p>
<pre><code>
// Now create an SQL table and issue SQL queries against it without
// using the sqlContext but through the SparkSession object.
// Creates a temporary view of the DataFrame
zipsDF.createOrReplaceTempView("zips_table")
zipsDF.cache()
val resultsDF = spark.sql("SELECT city, pop, state, zip FROM zips_table")
resultsDF.show(10)
</code></pre>
<h3 id="访问catalog-metadata" >
<div>
    <a href="#%e8%ae%bf%e9%97%aecatalog-metadata">
        ##
    </a>
    访问Catalog Metadata
</div>
</h3>
<p>SparkSession暴露了访问metastore的接口SparkSession.catalog，这个接口可以用来访问catalog metadata信息，比如（Hcatalog）</p>
<pre><code>
//fetch metadata data from the catalog
spark.catalog.listDatabases.show(false)
spark.catalog.listTables.show(false)
</code></pre>
<pre><code>
scala> spark.catalog.listDatabases.show(false)
+-------+---------------------+------------------------------------------+
|name   |description          |locationUri                               |
+-------+---------------------+------------------------------------------+
|default|Default Hive database|hdfs://bdtest-001:8020/apps/hive/warehouse|
+-------+---------------------+------------------------------------------+

scala> spark.catalog.listTables.show(false)
+-------------------------+--------+----------------------------------------+---------+-----------+
|name                     |database|description                             |tableType|isTemporary|
+-------------------------+--------+----------------------------------------+---------+-----------+
|activity_donate_record   |default |Imported by sqoop on 2018/01/08 19:03:33|MANAGED  |false      |
|activity_periods_config  |default |Imported by sqoop on 2018/01/08 19:04:20|MANAGED  |false      |
|activity_result          |default |Imported by sqoop on 2018/01/08 19:05:00|MANAGED  |false      |
|activity_school_info     |default |Imported by sqoop on 2018/01/08 19:05:46|MANAGED  |false      |
|activity_season_game_data|default |Imported by sqoop on 2018/01/08 19:06:23|MANAGED  |false      |
|activity_season_game_log |default |Imported by sqoop on 2018/01/08 19:07:00|MANAGED  |false      |
|ana_tag                  |default |Imported by sqoop on 2018/01/08 19:07:35|MANAGED  |false      |
|ana_user_tag             |default |Imported by sqoop on 2018/01/08 19:08:18|MANAGED  |false      |
|api_invoking_log         |default |Imported by sqoop on 2018/01/08 19:08:58|MANAGED  |false      |
|app_function             |default |Imported by sqoop on 2018/01/08 19:09:34|MANAGED  |false      |
|bank_card                |default |Imported by sqoop on 2018/01/08 19:10:09|MANAGED  |false      |
|bank_card_bin            |default |Imported by sqoop on 2018/01/08 19:10:46|MANAGED  |false      |
|command_invocation       |default |Imported by sqoop on 2018/01/08 19:11:26|MANAGED  |false      |
|coupon_grant_log         |default |Imported by sqoop on 2018/01/08 19:12:05|MANAGED  |false      |
|coupon_user              |default |Imported by sqoop on 2018/01/08 19:12:43|MANAGED  |false      |
|device_command           |default |Imported by sqoop on 2018/01/08 19:13:18|MANAGED  |false      |
|device_preorder          |default |Imported by sqoop on 2018/01/08 19:13:54|MANAGED  |false      |
|dro_dropdetails          |default |null                                    |MANAGED  |false      |
|dro_dropinfo             |default |Imported by sqoop on 2018/01/08 19:15:09|MANAGED  |false      |
|dro_exchange_record      |default |Imported by sqoop on 2018/01/08 19:15:43|MANAGED  |false      |
+-------------------------+--------+----------------------------------------+---------+-----------+
only showing top 20 rows
</code></pre>
<h2 id="dataframe-和-datasets" >
<div>
    <a href="#dataframe-%e5%92%8c-datasets">
        #
    </a>
    DataFrame 和 Datasets
</div>
</h2>
<p><img src="https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png" alt="DataFrame &amp; Dataset"></p>
<h3 id="dataset" >
<div>
    <a href="#dataset">
        ##
    </a>
    Dataset：
</div>
</h3>
<ul>
<li>
<p>强类型</p>
</li>
<li>
<p>支持functional和relational操作</p>
</li>
<li>
<p>操作分类</p>
<ul>
<li>转换（transformation）：用于产生新的Datasets
<ul>
<li>例如：map，filter，select，aggregate（groupBy）</li>
</ul>
</li>
<li>操作（actions）：触发计算并返回结果
<ul>
<li>例如：count，show，把数据写回文件系统</li>
</ul>
</li>
</ul>
</li>
<li>
<p>lazy：计算只有到action触发的时候才进行。Dataset内部可以理解为存储了如何进行计算的计划。当action执行的时候Spark query optimizer生成并优化计算方案后将其执行。</p>
</li>
<li>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder">Encoder</a>：把JVM中的类型T跟Spark SQL的数据表现形式进行转换的机制。（marshal/unmarshal?）</p>
</li>
<li>
<p>两种创建方法：</p>
<ul>
<li>
<p>使用SparkSession的read()方法</p>
  <pre><code>
  val people = spark.read.parquet("...").as[Person]  // Scala
  Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
  </code></pre>
</li>
<li>
<p>对现有Dataset做转换（transformation）</p>
  <pre><code>
  val names = people.map(_.name)  // in Scala; names is a Dataset[String]
</li>
</ul>
</li>
</ul>
<p>Dataset<String> names = people.map((Person p) -&gt; p.name, Encoders.STRING));
</code></pre></p>
<ul>
<li>
<p>Dataset操作也可以是untyped，通过：<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a>, <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a>, <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame的function</a>等。</p>
</li>
<li>
<p>列操作</p>
<ul>
<li>选择一列（抛弃其他列）：select(&ldquo;column&rdquo;)</li>
<li>增加一列：withColumn(&ldquo;newColumn&rdquo;, Column)</li>
<li>修改一列: withColumn(&ldquo;column&rdquo;, Column)</li>
<li>删除一列: drop(&ldquo;column&rdquo;)</li>
<li>类型转换：</li>
</ul>
</li>
<li>
<p>列高级操作：</p>
<ul>
<li>UDF:</li>
</ul>
</li>
</ul>
<h3 id="dataframe" >
<div>
    <a href="#dataframe">
        ##
    </a>
    DataFrame：
</div>
</h3>
<ul>
<li>Dataset的无类型view (untyped view)，对应Dataset的Row</li>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame操作Functions</a></li>
</ul>
<p>DataFrame是有名列（named columns）组成的数据集合，类似关系数据库中的表或者python/R中的pandas。</p>
<p>可以由多种数据源来构造DataFrame，比如Hive中的table，Spark的RDD（Resilient Distributed Datasets）。</p>
<p><strong>TODO: Datasets</strong></p>
<p>SparkSession有很多种方法创建DataFrame和Datasets。</p>
<table>
<thead>
<tr>
<th></th>
<th>DataFrame</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建API</td>
<td>SparkSession.createDataFrame</td>
<td></td>
</tr>
<tr>
<td>访问</td>
<td></td>
<td></td>
</tr>
<tr>
<td>其他</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th>Datasets</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建API</td>
<td></td>
<td></td>
</tr>
<tr>
<td>访问</td>
<td></td>
<td></td>
</tr>
<tr>
<td>其他</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow" >
<div>
    <a href="#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow">
        ##
    </a>
    <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row">Row</a>
</div>
</h3>
<ul>
<li>创建Row</li>
<li>访问
<ul>
<li>
<p>generic访问，使用ordinal序列访问</p>
<p>row(0)：访问列第一个成员</p>
</li>
<li>
<p>原始类型访问</p>
</li>
</ul>
</li>
</ul>
<h3 id="columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn" >
<div>
    <a href="#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn">
        ##
    </a>
    <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a>
</div>
</h3>
<ul>
<li>DataSet API:</li>
</ul>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset</a></p>
<h3 id="处理时间" >
<div>
    <a href="#%e5%a4%84%e7%90%86%e6%97%b6%e9%97%b4">
        ##
    </a>
    处理时间
</div>
</h3>
<h4 id="-ver--15" >
<div>
    <a href="#-ver--15">
        ###
    </a>
    * ver &lt;= 1.5
</div>
</h4>
<p>使用hiveContext来查询</p>
<h4 id="--15--ver--22" >
<div>
    <a href="#--15--ver--22">
        ###
    </a>
    *  1.5 &lt;= ver &lt; 2.2
</div>
</h4>
<p>Spark 1.5引入了unix_timestamp()，所以在1.5之后到2.2的spark版本中可以这样操作时间信息</p>
<pre><code>
import org.apache.spark.sql.functions.{unix_timestamp, to_date}

val df = Seq((1L, "01-APR-2015")).toDF("id", "ts")

df.select(to_date(unix_timestamp(
  $"ts", "dd-MMM-yyyy"
).cast("timestamp")).alias("timestamp"))
</code></pre>
<h4 id="-ver--22" >
<div>
    <a href="#-ver--22">
        ###
    </a>
    * ver &gt;= 2.2
</div>
</h4>
<pre><code>
import org.apache.spark.sql.functions.{to_date, to_timestamp}

df.select(to_date($"ts", "dd-MMM-yyyy").alias("date"))

df.select(to_timestamp($"ts", "dd-MM-yyyy HH:mm:ss").alias("timestamp"))
</code></pre>
<h3 id="import-sparkimplicits_" >
<div>
    <a href="#import-sparkimplicits_">
        ##
    </a>
    import spark.implicits._
</div>
</h3>
<p>引入toDF()方法和“$”操作符</p>
<h2 id="编程陷阱" >
<div>
    <a href="#%e7%bc%96%e7%a8%8b%e9%99%b7%e9%98%b1">
        #
    </a>
    编程陷阱
</div>
</h2>
<h3 id="陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表" >
<div>
    <a href="#%e9%99%b7%e9%98%b11dataframe%e6%9c%89%e5%85%83%e7%b4%a0%e4%bd%86%e9%80%9a%e8%bf%87%e5%be%aa%e7%8e%af%e6%8a%8a%e5%80%bc%e6%94%be%e5%85%a5listbuffer%e5%90%8elistbuffer%e5%8d%b4%e4%b8%ba%e7%a9%ba%e5%88%97%e8%a1%a8">
        ##
    </a>
    陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表
</div>
</h3>
<pre><code>
   val valueDF = allData.select("value").distinct()
    valueDF.show()
    //valueDF.foreach(r => valueList.append(r.getAs[Int]("value")))

    for (row <- valueDF) {
      println("value:" + row.getAs[Int]("value"))
      valueList.append(row.getAs[Int]("value"))
    }
    println("Values:" + valueList)
    valueList.foreach(println))

// 程序运行输出

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

Values:ListBuffer()
</code></pre>
<p>可以看出两个问题：</p>
<ul>
<li>问题1: 虽然可以知道valueDF不为空，可是&quot;println(&hellip;)&ldquo;却没有任何输出内容</li>
<li>问题2: 结果valueList为空</li>
</ul>
<h4 id="原因" >
<div>
    <a href="#%e5%8e%9f%e5%9b%a0">
        ###
    </a>
    原因：
</div>
</h4>
<p>Spark任务执行是通过Driver/Executor来完成的，Driver控制任务执行，Executor负责任务执行，foreach/for会被Driver分配给Executor来执行（分布式执行），每个Executor创建一个自己的ListBuffer拷贝，因此当任务结束后信息就丢失了。</p>
<p>参考Stackoverflow的讨论：<a href="https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop">https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop</a></p>
<h4 id="修复方法使用collect" >
<div>
    <a href="#%e4%bf%ae%e5%a4%8d%e6%96%b9%e6%b3%95%e4%bd%bf%e7%94%a8collect">
        ###
    </a>
    修复方法：使用collect()
</div>
</h4>
<p>collect()定义：</p>
<pre><code>
def collect(): Array[T]
Returns an array that contains all rows in this Dataset.
Running collect requires moving all the data into the application's driver
process, and doing so on a very large dataset can crash the driver process with
OutOfMemoryError.
For Java API, use collectAsList.
</code></pre>
<p>修复代码如下：</p>
<pre><code>
val valueDF = allData.select("value").distinct()
valueDF.show()
//valueDF.foreach(r => valueList.append(r.getAs[Int]("value")))

for (row <- valueDF.collect()) {
  println("value:" + row.getAs[Int]("value"))
  valueList.append(row.getAs[Int]("value"))
}
println("getAllChannels:" + valueList)


// 程序运行输出如下

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

value:1
value:3
value:5
value:4
value:10
value:2
value:0
getAllChannels:ListBuffer(1, 3, 5, 4, 10, 2, 0)
</code></pre>
<h1 id="reference" >
<div>
    <a href="#reference">
        ##
    </a>
    Reference
</div>
</h1>
<ul>
<li><a href="https://community.hortonworks.com/articles/84071/apache-ambari-workflow-manager-view-for-apache-ooz-2.html">Ambari workflow manager</a></li>
<li><a href="http://docs.scala-lang.org/tour/basics.html">Scala basics</a></li>
<li><a href="https://github.com/foundweekends/giter8/wiki/giter8-templates">Giter8项目模版</a></li>
<li><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_spark-component-guide/content/ch_oozie-spark-action.html">Automating Spark job with Oozie</a></li>
<li><a href="https://community.hortonworks.com/articles/104949/using-virtualenv-with-pyspark-1.html">Using VirtualEnv with PySpark</a></li>
</ul>

        </div>

    </article>

    
    

    
        
        
            <h3 class="read-next-title noselect">Read next</h3>
            <ul class="read-next-posts noselect">
                
                <li><a href="/2017/10/softmax%E8%BE%93%E5%87%BA%E5%B1%82%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E9%AA%8C/">softmax输出层公式推导及代码实验</a></li>
                
                <li><a href="/2017/06/python%E4%B8%AD__main__%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F%E5%8F%8A%E5%8F%98%E9%87%8F%E4%BD%BF%E7%94%A8/">python中__main__的作用域及变量使用</a></li>
                
            </ul>
        
    

    

    
        
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "singleye" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>











    

    

    

        </main>
        
            <footer class="common-footer noselect">
    
    

    <div class="common-footer-bottom">
        

        <div style="display: flex; align-items: center; gap:8px">
            © singleye, 2024
            
        </div>
        <div style="display:flex;align-items: center">
            
            
            
            
            
            
        </div>
        <div>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/Junyi-99/hugo-theme-anubis2">Anubis2</a>.<br>
            

        </div>
    </div>

    <p class="h-card vcard">

    <a href=http://www.singleye.net/ class="p-name u-url url fn" rel="me"></a>

    

    
</p>

</footer>

        
    </div>
</body>
</html>
