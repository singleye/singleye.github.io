<!DOCTYPE html>


<html lang="zh-cn" data-theme="">
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>用scrapy爬取京东商品信息 - singleye</title>

<meta name="description" content="这是个description">


    <meta name="keywords" content="scrapy,crawl">




<link rel="icon" type="image/x-icon" href="http://www.singleye.net/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://www.singleye.net/favicon.png">








    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css" integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos&#43;CZt8XtPejb&#43;nJdVE=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css" integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT&#43;/cHwdlfBEzZwqiI=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css" integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00=">
    












    

    





    
    
        
    
    

    
        <script src="/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js" type="text/javascript" charset="utf-8" integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script>
    



















    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header">
            
                <div class="header-top">
    <div class="header-top-left">
        <h1 class="site-title noselect">
    <a href="/">singleye</a>
</h1>

        







    
        <div class="theme-switcher">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>


</span>

        </div>
    

    <script>
        const STORAGE_KEY = 'user-color-scheme'
        const defaultTheme = "light"

        let currentTheme
        let switchButton
        let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

        function switchTheme(e) {
            currentTheme = (currentTheme === 'dark') ? 'light' : 'dark';
            if (localStorage) localStorage.setItem(STORAGE_KEY, currentTheme);
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        const autoChangeScheme = e => {
            currentTheme = e.matches ? 'dark' : 'light'
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        document.addEventListener('DOMContentLoaded', function () {
            switchButton = document.querySelector('.theme-switcher')
            currentTheme = detectCurrentScheme()

            if (currentTheme === 'auto') {
                autoChangeScheme(autoDefinedScheme);
                autoDefinedScheme.addListener(autoChangeScheme);
            } else {
                document.documentElement.setAttribute('data-theme', currentTheme)
            }

            if (switchButton) {
                switchButton.addEventListener('click', switchTheme, false)
            }

            showContent();
        })

        function detectCurrentScheme() {
            if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
                return localStorage.getItem(STORAGE_KEY)
            }
            if (defaultTheme) {
                return defaultTheme
            }
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function showContent() {
            document.body.style.visibility = 'visible';
            document.body.style.opacity = 1;
        }

        function changeGiscusTheme (theme) {
            function sendMessage(message) {
              const iframe = document.querySelector('iframe.giscus-frame');
              if (!iframe) return;
              iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
            }

            sendMessage({
              setConfig: {
                theme: theme
              }
            });
        }
    </script>


        <ul class="social-icons noselect">







    <li>
            <a href="/index.xml" title="RSS" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 1 0 2 0a1 1 0 1 0 -2 0" /><path d="M4 4a16 16 0 0 1 16 16" /><path d="M4 11a9 9 0 0 1 9 9" /></svg>


</span>

            </a>
        </li>
    

</ul>

    </div>
    <div class="header-top-right">

    </div>
</div>


    <nav class="noselect">
        
        
        <a class="" href="http://www.singleye.net/" title="">Home</a>
        
        <a class="" href="http://www.singleye.net/categories" title="">Categories</a>
        
        <a class="" href="http://www.singleye.net/tags" title="">Tags</a>
        
        <a class="" href="https://github.com/singleye" title="">GitHub</a>
        
    </nav>



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>





            
        </header>
        <main id="main" tabindex="-1">
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">用scrapy爬取京东商品信息</h1>
                

            </header>
            



<div class="post-info noselect">
    
        <div class="post-date dt-published">
            <time datetime="2017-08-31">2017-08-31</time>
            
        </div>
    

    <a class="post-hidden-url u-url" href="/2017/08/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E4%BA%AC%E4%B8%9C%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/">/2017/08/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E4%BA%AC%E4%B8%9C%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/</a>
    <a href="http://www.singleye.net/" class="p-name p-author post-hidden-author h-card" rel="me"></a>


    <div class="post-taxonomies">
        
            <ul class="post-categories">
                
                    
                    <li><a href="/categories/network/">Network</a></li>
                
                    
                    <li><a href="/categories/crawler/">Crawler</a></li>
                
            </ul>
        
        
            <ul class="post-tags">
                
                    
                    <li><a href="/tags/scrapy/">#Scrapy</a></li>
                
                    
                    <li><a href="">#爬虫</a></li>
                
                    
                    <li><a href="">#京东</a></li>
                
                    
                    <li><a href="/tags/crawl/">#Crawl</a></li>
                
            </ul>
        
        
    </div>
</div>

        </div>
        

  
  




  
  
  
  <details class="toc noselect">
    <summary>Table of Contents</summary>
    <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#爬虫代码框架">爬虫代码框架</a></li>
    <li><a href="#了解要爬取网页的结构">了解要爬取网页的结构</a></li>
    <li><a href="#通过xpath获取选取的页面元素">通过xpath获取选取的页面元素</a></li>
    <li><a href="#获取价格及评论信息">获取价格及评论信息</a></li>
    <li><a href="#分页处理">分页处理</a></li>
    <li><a href="#抓取项目进行pipeline处理">抓取项目进行pipeline处理</a></li>
    <li><a href="#编码问题">编码问题</a></li>
  </ul>
</nav></div>
  </details>
  



<script>
  var toc = document.querySelector(".toc");
  if (toc) {
    toc.addEventListener("click", function () {
      if (event.target.tagName !== "A") {
        event.preventDefault();
        if (this.open) {
          this.open = false;
          this.classList.remove("expanded");
        } else {
          this.open = true;
          this.classList.add("expanded");
        }
      }
    });
  }
</script>

        <div class="content e-content">
            <!-- more /-->
<p>scrapy是一个非常著名的爬虫框架，使用这个框架可以非常容易的生成一个网站爬虫程序框架，之后就可以在框架之上方便的进行爬虫的编写。</p>
<p>进来想要了解一些产品的市场信息，就用scrapy写了个简单的爬虫，写个笔记记录一下。</p>
<h1 id="安装" >
<div>
    <a href="#%e5%ae%89%e8%a3%85">
        ##
    </a>
    安装
</div>
</h1>
<p>使用python环境的话最好通过pip进行安装，这样操作简单方便，直接使用下面的命令即可：</p>
<pre><code>$ pip install scrapy
</code></pre>
<p>scrapy框架提供了&rsquo;scrapy&rsquo;命令进行项目的创建及运行管理，所以首先看一下</p>
<pre><code>$ scrapy --help
Scrapy 1.4.0 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command
</code></pre>
<h1 id="创建项目" >
<div>
    <a href="#%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae">
        ##
    </a>
    创建项目
</div>
</h1>
<p>首先使用&rsquo;scrapy startproject&rsquo;创建项目</p>
<pre><code>$ scrapy startproject crawler
New Scrapy project 'crawler', using template directory '/Users/wangq/virtualenv/test/lib/python2.7/site-packages/scrapy/templates/project', created in:
    /Users/wangq/tmp/crawler

You can start your first spider with:
    cd crawler
    scrapy genspider example example.com
</code></pre>
<p>使用&rsquo;scrapy genspider&rsquo;创建爬虫</p>
<pre><code>$ cd crawler
$ scrapy genspider jd list.jd.com
$ ls crawler/spiders/jd.py
crawler/spiders/jd.py
</code></pre>
<p>到此爬虫的框架就创建好了，爬虫的主要代码需要在jd.py中完成。</p>
<h1 id="编写爬虫" >
<div>
    <a href="#%e7%bc%96%e5%86%99%e7%88%ac%e8%99%ab">
        ##
    </a>
    编写爬虫
</div>
</h1>
<h2 id="爬虫代码框架" >
<div>
    <a href="#%e7%88%ac%e8%99%ab%e4%bb%a3%e7%a0%81%e6%a1%86%e6%9e%b6">
        #
    </a>
    爬虫代码框架
</div>
</h2>
<p>爬虫的代码主要是在crawler/spiders/jd.py中，打开这个文件看到内容如下：</p>
<pre><code># -*- coding: utf-8 -*-
import scrapy


class JdSpider(scrapy.Spider):
    name = 'jd'
    allowed_domains = ['list.jd.com']
    start_urls = ['http://list.jd.com/']

    def parse(self, response):
        pass
</code></pre>
<p>简单解释下：</p>
<ul>
<li>name: 爬虫的名字</li>
<li>allowed_domains: 当时用了OffsiteMiddleware的时候这个配置可以限定爬虫爬取的站点的域名列表</li>
<li>start_urls: 指定爬虫开始运行时的爬取URL</li>
</ul>
<p>在生成的代码中有一个parse()方法，每当爬虫获取一个新的页面爬去返回数据的时候就把这个数据通过response传递给parse()方法进行内容处理。</p>
<h2 id="了解要爬取网页的结构" >
<div>
    <a href="#%e4%ba%86%e8%a7%a3%e8%a6%81%e7%88%ac%e5%8f%96%e7%bd%91%e9%a1%b5%e7%9a%84%e7%bb%93%e6%9e%84">
        #
    </a>
    了解要爬取网页的结构
</div>
</h2>
<p>编写爬虫前先了解被爬取网页的结构信息，以及信息提取方法。这次我需要提取的信息主要是商品列表页面中的物品、价格、评论数这些基本信息，使用浏览器开发者功能查看对应的元素。</p>
<p><img src="http://singleye-public-read.oss-cn-shanghai.aliyuncs.com/singleye.net/static/2017/08/scrapy/scrapy-001.png?x-oss-process=style/png2jpg" alt="page structure"></p>
<p>分析页面发现需要的条目的class都有&rsquo;j-sku-item&rsquo;属性值，知道这个规律后页就可以使用xpath获取到这个条目的具体内容了。</p>
<h2 id="通过xpath获取选取的页面元素" >
<div>
    <a href="#%e9%80%9a%e8%bf%87xpath%e8%8e%b7%e5%8f%96%e9%80%89%e5%8f%96%e7%9a%84%e9%a1%b5%e9%9d%a2%e5%85%83%e7%b4%a0">
        #
    </a>
    通过xpath获取选取的页面元素
</div>
</h2>
<p>每个div的class属性包含&rsquo;j-sku-item&rsquo;的元素可以通过xpath的这条规则来描述&quot;//div[contains(@class, &lsquo;j-sku-item&rsquo;)]&quot;，scrapy的response可以直接支持xpath，那么想要获取这个对应的元素就可以通过这行代码来获取了：</p>
<pre><code>response.xpath(&quot;//div[contains(@class, 'j-sku-item')]&quot;):
</code></pre>
<h2 id="获取价格及评论信息" >
<div>
    <a href="#%e8%8e%b7%e5%8f%96%e4%bb%b7%e6%a0%bc%e5%8f%8a%e8%af%84%e8%ae%ba%e4%bf%a1%e6%81%af">
        #
    </a>
    获取价格及评论信息
</div>
</h2>
<p>通常的静态内容网站数据都可以使用xpath来获取，但在爬京东的网站过程中发现价格及评论数据不是后端与页面一起处理好之后一起发送过来的，所以这两个信息无法使用xpath获取，但仔细分析网络请求可以发现这些信息是通过两个web调用来获取的，我们可以使用python的requests库来获取。</p>
<ul>
<li>获取评论信息的调用：
<img src="http://singleye-public-read.oss-cn-shanghai.aliyuncs.com/singleye.net/static/2017/08/scrapy/scrapy-002.jpeg?x-oss-process=style/png2jpg" alt="comments信息"></li>
</ul>
<p>接下来可以使用curl来尝试获取评论信息的方法，最终发现访问可以通过类似的简化来完成：</p>
<pre><code>https://club.jd.com/comment/productCommentSummaries.action?my=pinglun&amp;referenceIds=959228,1722097,1722101
</code></pre>
<p>主要参数是referenceIds，这里可以指定需要获取的sku的列表。获取的代码如下：</p>
<pre><code>def get_comments(sku_id_list):
    &quot;&quot;&quot;
    url: https://club.jd.com/comment/productCommentSummaries.action?my=pinglun&amp;referenceIds=959228,1722097,1722101&quot;
    &quot;&quot;&quot;
    ids = ','.join(sku_id_list)
    url = &quot;https://club.jd.com/comment/productCommentSummaries.action?my=pinglun&amp;referenceIds=&quot; + ids
    rsp = requests.get(url)
    return rsp.json()
</code></pre>
<ul>
<li>获取价格信息的调用：
<img src="http://singleye-public-read.oss-cn-shanghai.aliyuncs.com/singleye.net/static/2017/08/scrapy/scrapy-003.jpeg?x-oss-process=style/png2jpg" alt="price信息"></li>
</ul>
<p>与获取评论信息类似，最终发现调用的接口可以简化成这个样子：</p>
<pre><code>https://p.3.cn/prices/mgets?skuIds=J_959228%2CJ_1722101%2CJ_2064343
</code></pre>
<p>不同之处是每个sku前面加了一个&quot;J_&ldquo;字段。参照获取评论的方法代码如下：</p>
<pre><code>def get_prices(sku_id_list):
    &quot;&quot;&quot;
    url: https://p.3.cn/prices/mgets?skuIds=J_959228%2CJ_1722101%2CJ_2064343
    &quot;&quot;&quot;
    str_id_list = map(lambda x: &quot;J_&quot;+x, sku_id_list)
    ids = ','.join(str_id_list)
    url = &quot;https://p.3.cn/prices/mgets?skuIds=&quot; + ids
    rsp = requests.get(url)
    return rsp.json()
</code></pre>
<h2 id="分页处理" >
<div>
    <a href="#%e5%88%86%e9%a1%b5%e5%a4%84%e7%90%86">
        #
    </a>
    分页处理
</div>
</h2>
<p>京东的商品条目很多的时候会分页展示，需要爬虫识别分页信息并自动抓取进行上面的处理。</p>
<p>首先先找到如何导向下一页的连接：</p>
<p><img src="http://singleye-public-read.oss-cn-shanghai.aliyuncs.com/singleye.net/static/2017/08/scrapy/scrapy-004.png?x-oss-process=style/png2jpg" alt="分页链接"></p>
<p>获取链接的方式可以通过xpath抓取，把抓取的连接传给scrapy的Request()方法进行新页面的抓取，并指定抓取信息的处理回调即可。代码如下：</p>
<pre><code>next_page = response.xpath(&quot;//a[@class='pn-next']/@href&quot;).get()
if next_page:
    yield scrapy.Request(&quot;https://list.jd.com&quot;+next_page, callback=self.parse)
</code></pre>
<h2 id="抓取项目进行pipeline处理" >
<div>
    <a href="#%e6%8a%93%e5%8f%96%e9%a1%b9%e7%9b%ae%e8%bf%9b%e8%a1%8cpipeline%e5%a4%84%e7%90%86">
        #
    </a>
    抓取项目进行pipeline处理
</div>
</h2>
<p>每一条被爬取好的信息条目会发给pipeline模块进行处理，因此pipeline可以对数据做很多后期的处理工作，包括但不限于：</p>
<ol>
<li>清洗抓起数据</li>
<li>验证抓取的数据</li>
<li>去重</li>
<li>存储数据</li>
</ol>
<p>这次实践主要用pipeline进行数据的存储处理。默认情况下scrapy使用自带的pipeline进行处理，如果需要进行特殊处理则需要对pipeline进行配置。配置文件是crawler/settings.py。</p>
<pre><code># Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'crawler.pipelines.CrawlerPipeline': 300,
}
</code></pre>
<p>pipeline的配置通过ITEM_PIPELINES完成，可以指定多个pipeline，这样一个pipeline处理完之后可以交给后面的pipeline处理。每一个pipeline条目有一个0-1000之间的整数参数，这个参数指定了pipeline的执行顺序。</p>
<p>配置好pipeline之后可以在crawler/pipelines.py中编写代码进行存储处理。默认生成的代码中只包含process_item()方法，但对于需要打开文件或者数据库的场景处理会不太方便，我们可以增加open_spider()和close_spider()方法进行处理，比如</p>
<pre><code>class CrawlerPipeline(object):
    def open_spider(self, spider):
        self.file = open('result.json', 'w')

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(dict(item)).decode(&quot;unicode_escape&quot;).encode('utf-8') + '\n'
        self.file.write(line)
        return item
</code></pre>
<h2 id="编码问题" >
<div>
    <a href="#%e7%bc%96%e7%a0%81%e9%97%ae%e9%a2%98">
        #
    </a>
    编码问题
</div>
</h2>
<p>前面的process_item()方法中有一行处理编码的代码需要解释一下。在用下面这行代码不进行编码处理的情况下如果直接存储json.dumps()的结果时会存储成人类不能直接阅读的内容。</p>
<pre><code>line = json.dumps(dict(item)) + '\n'
</code></pre>
<p>如下存储的其中一个条目，会发现&quot;name&quot;变成了&rdquo;\uXXXX\uXXXX&quot;的字符串。</p>
<pre><code>{&quot;sku&quot;: &quot;10391738071&quot;, &quot;category&quot;: &quot;3128&quot;, &quot;vendor&quot;: &quot;138857&quot;, &quot;name&quot;: &quot;\u98de\u5229\u6d66\uff08PHILIPS\uff09\u51c0\u6c34\u5668 WP4170/00\u7eaf\u6c34\u673a+WP4100/00\u524d\u7f6e\u8fc7\u6ee4\u5668\u5957\u88c5&quot;, &quot;price&quot;: 4699.0,            &quot;comments&quot;: 4}
</code></pre>
<p>原因是因为json.dumps()对于中文字符的处理会进行escape处理，为了存储需要首先进行unescape，就是进行decode(&ldquo;unicode_escape&rdquo;)。处理后的结果变成了utf-8编码，需要注意的是这还不够。如果这行代码写成下面的形式，则会发现些文件的时候无法写入文件。</p>
<pre><code>line = json.dumps(dict(item)).decode(&quot;unicode_escape&quot;) + '\n'
</code></pre>
<p>产生的错误发生在file.write()中：</p>
<pre><code>UnicodeEncodeError: 'ascii' codec can't encode characters in position 71-72: ordinal not in range(128)
</code></pre>
<p>这是因为直接写入文件时write()会认为所有的数据都是ascii码，但中文情况下显然是不成立的。因此需要对数据进行encode()，参照下面的代码写就没有问题了：</p>
<pre><code>line = json.dumps(dict(item)).decode(&quot;unicode_escape&quot;).encode('utf-8') + '\n'
</code></pre>
<p>注，有写地方会用sys.setdefaultencoding(&lsquo;utf-8&rsquo;)来进行处理，但是不是万不得已并不推荐这种直接改变全局环境的做法，有可能会让程序产生意料不到的情况。</p>
<h1 id="数据分析" >
<div>
    <a href="#%e6%95%b0%e6%8d%ae%e5%88%86%e6%9e%90">
        ##
    </a>
    数据分析
</div>
</h1>
<p>这次抓取数据主要是分析在各个价格区间的产品用户使用情况，因此可以通过柱状图来展示。横坐标表示价格，用100元为一个区间进行统计。纵坐标显示在这100元的价格范围内的用户评论数量。</p>
<p>画图使用matplotlib编写，主要的代码逻辑如下：</p>
<pre><code>PRICE_UNIT = 100

axis_price = range(PRICE_LOWER_RANGE, PRICE_UPPER_RANGE, PRICE_UNIT)
axis_comment = [0] * len(axis_price)

for item in get_all_sku(category=category):
    idx = int(item.price/PRICE_UNIT)
    
    # 当超过统计价格的上限区间后将结果合并到最高价格范围中
    if idx &gt;= len(axis_price):
        axis_comment[-1] += item.comment
    else:
        axis_comment[idx] += item.comment

width=0.8*PRICE_UNIT
plt.bar(axis_price, axis_comment, width, color='blue', align='edge')
plt.show()
</code></pre>
<p>绘制结果如下：</p>
<p><img src="http://singleye-public-read.oss-cn-shanghai.aliyuncs.com/singleye.net/static/2017/08/scrapy/scrapy-005.png?x-oss-process=style/png2jpg" alt="绘图"></p>

        </div>

    </article>

    
    

    
        
        
    

    

    
        
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "singleye" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>











    

    

    

        </main>
        
            <footer class="common-footer noselect">
    
    

    <div class="common-footer-bottom">
        

        <div style="display: flex; align-items: center; gap:8px">
            © singleye, 2024
            
        </div>
        <div style="display:flex;align-items: center">
            
            
            
            
            
            
        </div>
        <div>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/Junyi-99/hugo-theme-anubis2">Anubis2</a>.<br>
            

        </div>
    </div>

    <p class="h-card vcard">

    <a href=http://www.singleye.net/ class="p-name u-url url fn" rel="me"></a>

    

    
</p>

</footer>

        
    </div>
</body>
</html>
