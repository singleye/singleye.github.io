<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tag2 on singleye</title>
    <link>/tags/tag2/</link>
    <description>singleye (Tag2)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <managingEditor>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</managingEditor>
    <webMaster>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</webMaster>
    <lastBuildDate>Wed, 22 Nov 2023 12:00:01 +0800</lastBuildDate>
    
    <atom:link href="/tags/tag2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PCL 点云数据过滤处理</title>
      <link>/2023/11/pcl-%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 22 Nov 2023 12:00:01 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2023/11/pcl-%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4%E5%A4%84%E7%90%86/</guid>
      <description>&lt;h1 id=&#34;点云过滤&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%82%b9%e4%ba%91%e8%bf%87%e6%bb%a4&#34;&gt;
        ##
    &lt;/a&gt;
    点云过滤
&lt;/div&gt;
&lt;/h1&gt;</description>
    </item>
    
    <item>
      <title>Wechat_development</title>
      <link>/2023/11/wechat_development/</link>
      <pubDate>Wed, 08 Nov 2023 23:40:06 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2023/11/wechat_development/</guid>
      <description>&lt;h1 id=&#34;订阅号与服务号的区别&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%a2%e9%98%85%e5%8f%b7%e4%b8%8e%e6%9c%8d%e5%8a%a1%e5%8f%b7%e7%9a%84%e5%8c%ba%e5%88%ab&#34;&gt;
        ##
    &lt;/a&gt;
    订阅号与服务号的区别
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Explanation_of_interface_privileges.html&#34;&gt;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Explanation_of_interface_privileges.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;订阅号侧重向用户推送文章，服务号侧重对用户做服务（交易等）但发送文章很少&lt;/p&gt;
&lt;h1 id=&#34;网页开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e9%a1%b5%e5%bc%80%e5%8f%91&#34;&gt;
        ##
    &lt;/a&gt;
    网页开发
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;机制：网页授权
场景：在微信中打开第三方网页，可以用网页授权机制获取用户信息&lt;/p&gt;
&lt;p&gt;scope&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;snsapi_base: 获取进入页面的用户 openid（静默授权）&lt;/li&gt;
&lt;li&gt;snsapi_userinfo: 获取进入页面的用户&lt;strong&gt;基本信息&lt;/strong&gt;，需要用户手动同意，无需用户关注&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;网页获取用户信息&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e9%a1%b5%e8%8e%b7%e5%8f%96%e7%94%a8%e6%88%b7%e4%bf%a1%e6%81%af&#34;&gt;
        #
    &lt;/a&gt;
    网页获取用户信息
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;scope&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#scope&#34;&gt;
        ##
    &lt;/a&gt;
    scope
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;snsapi_base&lt;/li&gt;
&lt;li&gt;snsapi_userinfo&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=APPID&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&#34;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&#34;&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过网页页面获取用户信息的方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;引导用户点击带 redirect_uri 的内容（比如，通过菜单设置: &lt;a href=&#34;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&#34;&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;用户点击后经过腾讯 oauth2 回调到 redirect_uri，并且带有 &amp;lsquo;code&amp;rsquo; 参数&lt;/li&gt;
&lt;li&gt;页面处理 code 信息，例如将 code 作为一个参数带入调用后端的 API 接口&lt;/li&gt;
&lt;li&gt;后端调用腾讯 sns/userinfo 接口获取用户信息
&lt;ol&gt;
&lt;li&gt;首先调用 &lt;a href=&#34;https://api.weixin.qq.com/sns/oauth2/access_token&#34;&gt;https://api.weixin.qq.com/sns/oauth2/access_token&lt;/a&gt; 获取 access token&lt;/li&gt;
&lt;li&gt;再调用 &lt;a href=&#34;https://api.weixin.qq.com/sns/userinfo&#34;&gt;https://api.weixin.qq.com/sns/userinfo&lt;/a&gt; 获取用户信息&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;weixin 与手机号绑定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引导用户进入带oauth2回调页面，获取用户 code&lt;/li&gt;
&lt;li&gt;使用 code 检查用户信息
&lt;ul&gt;
&lt;li&gt;如果已经存在用户，则返回 token 并正常打开 next 页面&lt;/li&gt;
&lt;li&gt;如果不存在，则进入注册信息绑定页面，绑定手机号（短信验证）
&lt;ul&gt;
&lt;li&gt;验证通过后，创建用户并返回 token 并正常打开 next 页面&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;网站实现扫码登陆方法&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e7%ab%99%e5%ae%9e%e7%8e%b0%e6%89%ab%e7%a0%81%e7%99%bb%e9%99%86%e6%96%b9%e6%b3%95&#34;&gt;
        ##
    &lt;/a&gt;
    网站实现扫码登陆方法：
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html&#34;&gt;https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;步骤1：嵌入下面 JS 文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http://res.wx.qq.com/connect/zh_CN/htmledition/js/wxLogin.js
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;步骤2：在需要使用微信登录的地方实例以下JS对象：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; var obj = new WxLogin({
 self_redirect:true,
 id:&amp;#34;login_container&amp;#34;, 
 appid: &amp;#34;&amp;#34;, 
 scope: &amp;#34;&amp;#34;, 
 redirect_uri: &amp;#34;&amp;#34;,
  state: &amp;#34;&amp;#34;,
 style: &amp;#34;&amp;#34;,
 href: &amp;#34;&amp;#34;
 });
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;业务域名&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NLP 资源整理</title>
      <link>/2019/09/nlp-%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</link>
      <pubDate>Thu, 19 Sep 2019 12:18:54 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2019/09/nlp-%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</guid>
      <description>&lt;!--toc--&gt;
&lt;h1 id=&#34;模型&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%a8%a1%e5%9e%8b&#34;&gt;
        ##
    &lt;/a&gt;
    模型
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GloVe: Global Vectors for Word Representation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;https://nlp.stanford.edu/projects/glove/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34;&gt;https://nlp.stanford.edu/pubs/glove.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Annotated Transformer&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://github.com/openai/finetune-transformer-lm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;language understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;Improving Language Understanding with Unsupervised Learning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-2 (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;Better Language Models and Their Implications&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer-XL (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kimiyoung/transformer-xl&#34;&gt;https://github.com/kimiyoung/transformer-xl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1901.02860&#34;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Attentive Language Models Beyond a Fixed-Length Context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLNet (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/zihangdai/xlnet/&#34;&gt;https://github.com/zihangdai/xlnet/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLM (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/XLM/&#34;&gt;https://github.com/facebookresearch/XLM/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07291&#34;&gt;Cross-lingual Language Model Pretraining&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RoBERTa (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/master/examples/roberta&#34;&gt;https://github.com/pytorch/fairseq/tree/master/examples/roberta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DistilBERT (from HuggingFace)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/distillation&#34;&gt;https://github.com/huggingface/transformers/tree/master/examples/distillation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://medium.com/huggingface/distilbert-8cf3380435b5&#34;&gt;Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;项目&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%a1%b9%e7%9b%ae&#34;&gt;
        ##
    &lt;/a&gt;
    项目
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://allennlp.org&#34;&gt;https://allennlp.org&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/didi/delta&#34;&gt;https://github.com/didi/delta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.01853.pdf&#34;&gt;https://arxiv.org/pdf/1908.01853.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;滴滴 Delta&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/software/CRF-NER.html&#34;&gt;https://nlp.stanford.edu/software/CRF-NER.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRF&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Stanford CRF NER&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;https://nlp.stanford.edu/projects/glove/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GloVe: Global Vectors for Word Representation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34;&gt;https://nlp.stanford.edu/pubs/glove.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The Annotated Transformer&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;https://github.com/huggingface/transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transformers&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://huggingface.co/transformers&#34;&gt;https://huggingface.co/transformers&lt;/a&gt; 实现了很多模型（Bert, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, DistilBERT）&amp;lt;\b&amp;gt; &lt;a href=&#34;https://transformer.huggingface.co&#34;&gt;https://transformer.huggingface.co&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://github.com/openai/finetune-transformer-lm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GPT (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;language understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;Improving Language Understanding with Unsupervised Learning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GPT-2 (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;Better Language Models and Their Implications&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kimiyoung/transformer-xl&#34;&gt;https://github.com/kimiyoung/transformer-xl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transformer-XL (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1901.02860&#34;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Attentive Language Models Beyond a Fixed-Length Context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/zihangdai/xlnet/&#34;&gt;https://github.com/zihangdai/xlnet/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;XLNet (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/XLM/&#34;&gt;https://github.com/facebookresearch/XLM/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;XLM (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07291&#34;&gt;Cross-lingual Language Model Pretraining&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/master/examples/roberta&#34;&gt;https://github.com/pytorch/fairseq/tree/master/examples/roberta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;RoBERTa (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/distillation&#34;&gt;https://github.com/huggingface/transformers/tree/master/examples/distillation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;DistilBERT (from HuggingFace)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://medium.com/huggingface/distilbert-8cf3380435b5&#34;&gt;Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hundredblocks/concrete_NLP_tutorial&#34;&gt;https://github.com/hundredblocks/concrete_NLP_tutorial&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;An NLP workshop by Emmanuel Ameisen &lt;a href=&#34;https://twitter.com/EmmanuelAmeisen&#34;&gt;(@EmmanuelAmeisen)&lt;/a&gt;, from Insight AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/BrikerMan/Kashgari&#34;&gt;https://github.com/BrikerMan/Kashgari&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word2Vec, BERT, and GPT2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Kashgari is a Production-ready NLP Transfer learning framework for text-labeling and text-classification, includes Word2Vec, BERT, and GPT2 Language Embedding.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kyzhouhzau/BERT-NER&#34;&gt;https://github.com/kyzhouhzau/BERT-NER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bert&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;基于 CoNLL-2003 数据集的实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/ProHiryu/bert-chinese-ner&#34;&gt;https://github.com/ProHiryu/bert-chinese-ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;基于人民日报数据集的实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/macanv/BERT-BiLSTM-CRF-NER&#34;&gt;https://github.com/macanv/BERT-BiLSTM-CRF-NER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;training, serving&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/marcotcr/lime&#34;&gt;https://github.com/marcotcr/lime&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;用于解释机器学习的分类器。&lt;/p&gt;论文：&lt;a href=&#34;https://arxiv.org/abs/1602.04938&#34;&gt;https://arxiv.org/abs/1602.04938&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;数据集&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;
        ##
    &lt;/a&gt;
    数据集
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus&#34;&gt;https://github.com/SophonPlus/ChineseNlpCorpus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseWordVectors&#34;&gt;https://github.com/SophonPlus/ChineseWordVectors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb&#34;&gt;ChnSentiCorp_htl_all&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/CAIL&#34;&gt;https://github.com/thunlp/CAIL&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chinese AI &amp;amp; Law Challenge &lt;a href=&#34;http://cail.cipsc.org.cn&#34;&gt;http://cail.cipsc.org.cn&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;ner-数据集&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ner-%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;
        #
    &lt;/a&gt;
    NER 数据集
&lt;/div&gt;
&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/ontonotes/conll-formatted-ontonotes-5.0&#34;&gt;https://github.com/ontonotes/conll-formatted-ontonotes-5.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This is a CoNLL formatted version of the OntoNotes 5.0 release.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/juand-r/entity-recognition-datasets#references&#34;&gt;https://github.com/juand-r/entity-recognition-datasets#references&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A collection of corpora for named entity recognition (NER) and entity recognition tasks. These annotated datasets cover a variety of languages, domains and entity types.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;学习资料&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99&#34;&gt;
        ##
    &lt;/a&gt;
    学习资料
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;nlp-roadmap&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#nlp-roadmap&#34;&gt;
        #
    &lt;/a&gt;
    NLP roadmap
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/graykode/nlp-roadmap&#34;&gt;https://github.com/graykode/nlp-roadmap&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;probability--statistics&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#probability--statistics&#34;&gt;
        ##
    &lt;/a&gt;
    Probability &amp;amp; Statistics
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/prob.png&#34; alt=&#34;Probability &amp;amp; Statistics&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;machine-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#machine-learning&#34;&gt;
        ##
    &lt;/a&gt;
    Machine Learning
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/ml.png&#34; alt=&#34;Machine Learning&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;text-mining&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#text-mining&#34;&gt;
        ##
    &lt;/a&gt;
    Text Mining
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/textmining.png&#34; alt=&#34;Text Mining&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#natural-language-processing&#34;&gt;
        ##
    &lt;/a&gt;
    Natural Language Processing
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/nlp.png&#34; alt=&#34;Natural Language Processing&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;标注工具&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%a0%87%e6%b3%a8%e5%b7%a5%e5%85%b7&#34;&gt;
        ##
    &lt;/a&gt;
    标注工具
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;| 工具 | 链接 | 描述 |
|&amp;mdash;+&amp;mdash;+&amp;mdash;|
| Prodigy |https://prodi.gy/docs/|Prodigy (explosion.ai 开发 spacy 的公司)|
| brat |https://github.com/nlplab/brat||
| Knowtator |http://knowtator.sourceforge.net/index.shtml||
| Protégé + Knowtator plugin |https://github.com/UCDenver-ccp/Knowtator-2.0 &lt;a href=&#34;https://protege.stanford.edu/short-courses.php&#34;&gt;https://protege.stanford.edu/short-courses.php&lt;/a&gt;||
||http://deepdive.stanford.edu/labeling||
||https://github.com/SongRb/DeepDiveChineseApps||
||https://github.com/qiangsiwei/DeepDive_Chinese||
||https://github.com/jiesutd/SUTDAnnotator||
||https://github.com/HazyResearch/snorkel||
||https://bitbucket.org/dainkaplan/slate/||
| iepy | &lt;a href=&#34;https://github.com/machinalis/iepy&#34;&gt;https://github.com/machinalis/iepy&lt;/a&gt; | 标注，信息提取 |
| doccano |https://github.com/chakki-works/doccano||
| YEDDA |https://github.com/jiesutd/YEDDA||
| Chinese-Annotator | &lt;a href=&#34;https://github.com/deepwel/Chinese-Annotator&#34;&gt;https://github.com/deepwel/Chinese-Annotator&lt;/a&gt; | online/offline 结合的中文标注工具，想法比较好，目前项目还不完善 |
| HanNLP | &lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;https://github.com/hankcs/HanLP&lt;/a&gt; | NLP 工具箱（中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁），Java语言 |
| poplar |https://github.com/synyi/poplar|国内“森亿”公司开发|&lt;/p&gt;
&lt;h2 id=&#34;brat&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat&#34;&gt;
        #
    &lt;/a&gt;
    brat
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;brat-配置&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat-%e9%85%8d%e7%bd%ae&#34;&gt;
        ##
    &lt;/a&gt;
    brat 配置
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;annotation-配置-annotationconf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#annotation-%e9%85%8d%e7%bd%ae-annotationconf&#34;&gt;
        ###
    &lt;/a&gt;
    Annotation 配置 annotation.conf
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[entities]&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[entities]	 
Person
Location
Organization
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[relations]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数指定格式 &amp;ldquo;ARG:TYPE&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[relations]	 
Family	Arg1:Person, Arg2:Person
Employment	Arg1:Person, Arg2:Organization
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;参数可以有多个，使用 &amp;ldquo;|&amp;rdquo; 分隔&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[relations]	 	 
Located	Arg1:Person,	Arg2:Building|City|Country
Located	Arg1:Building,	Arg2:City|Country
Located	Arg1:City,	Arg2:Country
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[events]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;事件参数格式 &amp;ldquo;ROLE:TYPE&amp;rdquo;, ROLE 可以任意指定。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[events]	 
Marriage	Participant1:Person, Participant2:Person
Bankruptcy	Org:Company
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[attributes]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;属性作用域 &amp;ldquo;ARG:TYPE&amp;rdquo; ，可以用在 relation 和 event 中。&lt;/p&gt;
&lt;p&gt;拥有多个值的属性的值的定义方法是 &amp;ldquo;Value:VAL1|VAL2|VAL3[&amp;hellip;]&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[attributes]	 
Negated	Arg:&amp;lt;EVENT&amp;gt;
Confidence	Arg:&amp;lt;EVENT&amp;gt;, Value:L1|L2|L3
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;brat-标注信息格式&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat-%e6%a0%87%e6%b3%a8%e4%bf%a1%e6%81%af%e6%a0%bc%e5%bc%8f&#34;&gt;
        ##
    &lt;/a&gt;
    brat 标注信息格式
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://brat.nlplab.org/standoff.html&#34;&gt;http://brat.nlplab.org/standoff.html&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;T1	Organization 0 4	Sony
T2	MERGE-ORG 14 27	joint venture
T3	Organization 33 41	Ericsson
E1	MERGE-ORG:T2 Org1:T1 Org2:T3
T4	Country 75 81	Sweden
R1	Origin Arg1:T3 Arg2:T4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;protege--knowtator&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#protege--knowtator&#34;&gt;
        #
    &lt;/a&gt;
    Protege &amp;amp; Knowtator
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;编译 Knowtator 插件：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/UCDenver-ccp/Knowtator-2.0.git
mvn clean install
cp xxx/plugins/knowtator-2.1.5.jar /Applications/Protégé.app/Contents/Java/plugins/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启 Protege&lt;/p&gt;
&lt;h2 id=&#34;iepy&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#iepy&#34;&gt;
        #
    &lt;/a&gt;
    iepy
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;iepy 的 preprcess.py 会失败，需要按照以下方式修改 corenlp.sh 脚本&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Preprocess not running under MacOS&lt;/p&gt;
&lt;p&gt;Problems with the preprocess under MacOS? Apparently a change in the CoreNLP script is needed to be run. You need to change the file corenlp.sh that is located on /Users/&lt;your user&gt;/Library/Application Support/iepy/stanford-corenlp-full-2014-08-27/ and change scriptdir=&lt;code&gt;dirname $0&lt;/code&gt; for scriptdir=&lt;code&gt;dirname &amp;quot;$0&amp;quot;&lt;/code&gt; (ie, add double quotes around $0)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://iepy.readthedocs.io/en/stable/troubleshooting.html#troubleshooting&#34;&gt;https://iepy.readthedocs.io/en/stable/troubleshooting.html#troubleshooting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;自然语言处理工具&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e5%b7%a5%e5%85%b7&#34;&gt;
        ##
    &lt;/a&gt;
    自然语言处理工具
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;自然语言处理 中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;https://github.com/hankcs/HanLP&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;nlp-应用&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#nlp-%e5%ba%94%e7%94%a8&#34;&gt;
        ##
    &lt;/a&gt;
    NLP 应用
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;聊天机器人&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%81%8a%e5%a4%a9%e6%9c%ba%e5%99%a8%e4%ba%ba&#34;&gt;
        #
    &lt;/a&gt;
    聊天机器人
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/3c6f1e32e128&#34;&gt;用 TensorFlow 做个聊天机器人&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;论文&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;
        ##
    &lt;/a&gt;
    论文
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>哈希计算图片相似性</title>
      <link>/2019/06/%E5%93%88%E5%B8%8C%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%89%87%E7%9B%B8%E4%BC%BC%E6%80%A7/</link>
      <pubDate>Mon, 03 Jun 2019 15:19:04 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2019/06/%E5%93%88%E5%B8%8C%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%89%87%E7%9B%B8%E4%BC%BC%E6%80%A7/</guid>
      <description>&lt;!--toc--&gt;
&lt;h1 id=&#34;ahash平均哈希&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ahash%e5%b9%b3%e5%9d%87%e5%93%88%e5%b8%8c&#34;&gt;
        ##
    &lt;/a&gt;
    aHash（平均哈希）
&lt;/div&gt;
&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;将图片缩小到8x8的尺寸&lt;/li&gt;
&lt;li&gt;将缩小后的图片转换成灰度图&lt;/li&gt;
&lt;li&gt;计算8x8图片所有像素灰度值的平均值&lt;/li&gt;
&lt;li&gt;创建一个新的8x8矩阵，矩阵的每个值取值为0或1，计算方法是将原矩阵中对应像素的的灰度值与平均值进行对比，当大于等于平均值时记1，小于平均值时记0&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def ahash(filepath):
    img = cv2.imread(filepath)
    small = cv2.resize(img, (8, 8), interpolation=cv2.INTER_AREA)
    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
    fp = 0
    mean = gray.mean()
    w, h = gray.shape
    for x in range(w):
        for y in range(h):
            if gray[x][y] &amp;gt;= mean:
                fp += ((fp &amp;lt;&amp;lt; 1) + 1)
            else:
                fp += ((fp &amp;lt;&amp;lt; 1) + 0)
    return fp
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;dhash差值哈希&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dhash%e5%b7%ae%e5%80%bc%e5%93%88%e5%b8%8c&#34;&gt;
        ##
    &lt;/a&gt;
    dHash（差值哈希）
&lt;/div&gt;
&lt;/h1&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def dhash(filepath):
    img = cv2.imread(filepath)
    small = cv2.resize(img, (9, 8), interpolation=cv2.INTER_AREA)
    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
    fp = 0
    mean = gray.mean()
    w, h = gray.shape
    for x in range(w-1):
        for y in range(h):
            # if the pixel on the left is brighter mark it as 1
            if gray[x][y] &amp;gt; gray[x+1][y]:
                fp += ((fp &amp;lt;&amp;lt; 1) + 1)
            else:
                fp += ((fp &amp;lt;&amp;lt; 1) + 0)
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;phash感知哈希&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#phash%e6%84%9f%e7%9f%a5%e5%93%88%e5%b8%8c&#34;&gt;
        ##
    &lt;/a&gt;
    pHash（感知哈希）
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;pHash算法主要是使用了离散余弦变换（DCT）进行转换。&lt;/p&gt;
&lt;p&gt;利用感知哈希算法计算图片相似度
计算步骤：&lt;/p&gt;
&lt;p&gt;缩放图片：一般大小为32*32，这样方便DCT计算&lt;/p&gt;
&lt;p&gt;简化色彩，转化为灰度图：可以使用Image的convert(&amp;lsquo;L&amp;rsquo;)方法&lt;/p&gt;
&lt;p&gt;计算DCT（离散余弦变换）:&lt;/p&gt;
&lt;p&gt;获得图像的二维数据矩阵f(x,y)&lt;/p&gt;
&lt;p&gt;求离散余弦变换的系数矩阵[A]&lt;/p&gt;
&lt;p&gt;求系数矩阵对应的转置矩阵[A]T&lt;/p&gt;
&lt;p&gt;根据公式[F(u,v)]=[A][f(x,y)][A]T 计算离散余弦变换
缩小DCT：DCT计算后的矩阵是32&lt;em&gt;32，保留左上角的8&lt;/em&gt;8，这些代表的图片的最低频率&lt;/p&gt;
&lt;p&gt;计算平均值：计算缩小DCT后的所有像素点的平均&lt;/p&gt;
&lt;p&gt;进一步减小DCT：大于平均值记录为1，否则为0&lt;/p&gt;
&lt;p&gt;得到64位信息指纹&lt;/p&gt;
&lt;p&gt;记录两张图片的图像指纹的汉明距离，计算图片相似度&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def phash(filepath, shape=(1, 1)):
    fp = np.zeros(shape, dtype=np.ulonglong)
    try:
        img = cv2.imread(filepath)
        resize = cv2.resize(img, (32, 32))
        gray = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)
    except Exception as e:
        print(&amp;#39;-&amp;#39;*80)
        print(&amp;#39;Exception: image [%s]&amp;#39; % filepath)
        print(e)
        print(&amp;#39;-&amp;#39;*80)
        return fp

    left_upper = cv2.dct(gray.astype(float))[:8, :8]
    mean = left_upper.mean()
    h, w = left_upper.shape
    for x in range(w):
        for y in range(h):
            val = int(fp[y//8, x//8])
            if left_upper[y, x] &amp;gt;= mean:
                fp[y//8, x//8] = ((val &amp;lt;&amp;lt; 1) | 1)
            else:
                fp[y//8, x//8] = ((val &amp;lt;&amp;lt; 1) | 0)
    return fp
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;直方图&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%9b%b4%e6%96%b9%e5%9b%be&#34;&gt;
        ##
    &lt;/a&gt;
    直方图
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nivance/image-similarity&#34;&gt;https://github.com/nivance/image-similarity&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nivance/image-similarity/blob/master/src/main/java/image/similarity/ImageHistogram.java&#34;&gt;https://github.com/nivance/image-similarity/blob/master/src/main/java/image/similarity/ImageHistogram.java&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;直方图算法是对源图像与要筛选的图像进行直方图数据采集，对采集的各自图像直方图进行归一化再使用巴氏系数算法对直方图数据进行计算，最终得出图像相似度值，其值范围在[0, 1]之间0表示极其不同，1表示极其相似（相同）。&lt;/p&gt;
&lt;p&gt;算法步骤大致可以分为两步，根据源图像与候选图像的像素数据，生成各自直方图数据。第二步：使用第一步输出的直方图结果，运用巴氏系数（Bhattacharyya coefficient）算法，计算出相似程度值。&lt;/p&gt;
&lt;p&gt;第一步：直方图计算 直方图分为灰度直方图与RGB直方图，对于灰度图像直方图计算十分简单，只要初始化一个大小为256的直方图数组H，然后根据像素值完成频率分布统计，假设像素值为124，则H[124] += 1, 而对于彩色RGB像素来说直方图表达有两种方式，一种是单一直方图，另外一种是三维直方图，三维直方图比较简单明了，分别对应RGB三种颜色，定义三个直方图HR,HG, HB, 假设某一个像素点P的RGB值为(4, 231,129), 则对于的直方图计算为HR[4] += 1,HG[231] += 1, HB[129] += 1, 如此对每个像素点完成统计以后，RGB彩色直方图数据就生成了。 而RGB像素的单一直方图SH表示稍微复杂点，每个颜色的值范围为0 ~ 255之间的，假设可以分为一定范围等份，当8等份时，每个等份的值范围为32， 16等份时，每个等份值范围为16，当4等份时候，每个等份值的范围为64，假设RGB值为(14, 68, 221), 16等份之后，它对应直方图索引值(index)分别为: (0, 4, 13), 根据计算索引值公式:index = R + G * 16 + B * 16 * 16 对应的直方图index = 0 + 4 * 16 + 13 * 16 * 16， SH[3392] += 1如此遍历所有RGB像素值，完成直方图数据计算。&lt;/p&gt;
&lt;p&gt;第二步：巴氏系数计算，计算公式如下：$\sum_{i=1}^N\sqrt{p(i)p^{&amp;rsquo;}(i)}$ 。其中p, p&amp;rsquo;分别代表源与候选的图像直方图数据，对每个相同i的数据点乘积开平方以后相加得出的结果即为图像相似度值（巴氏系数因子值），范围为0到1之间。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.yorku.ca/~kosta/CompVis_Notes/bhattacharyya.pdf&#34;&gt;Bhattacharyya Coefficient&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;其他思想&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%85%b6%e4%bb%96%e6%80%9d%e6%83%b3&#34;&gt;
        ##
    &lt;/a&gt;
    其他思想
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;大津法&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%a4%a7%e6%b4%a5%e6%b3%95&#34;&gt;
        #
    &lt;/a&gt;
    大津法
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html&#34;&gt;Otsu Thresholding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ruanyifeng.com/blog/2013/03/similar_image_search_part_ii.html&#34;&gt;阮一峰blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1979年，日本学者大津展之证明了，&amp;ldquo;类内差异最小&amp;quot;与&amp;quot;类间差异最大&amp;quot;是同一件事，即对应同一个阈值。他提出一种简单的算法，可以求出这个阈值，这被称为&amp;quot;大津法&amp;rdquo;（Otsu&amp;rsquo;s method）。下面就是他的计算方法。&lt;/p&gt;
&lt;p&gt;假定一张图片共有n个像素，其中灰度值小于阈值的像素为 n1 个，大于等于阈值的像素为 n2 个（ n1 + n2 = n ）。w1 和 w2 表示这两种像素各自的比重。
　　w1 = n1 / n
　　w2 = n2 / n
再假定，所有灰度值小于阈值的像素的平均值和方差分别为 μ1 和 σ1，所有灰度值大于等于阈值的像素的平均值和方差分别为 μ2 和 σ2。于是，可以得到
　　类内差异 = w1(σ1的平方) + w2(σ2的平方)
　　类间差异 = w1w2(μ1-μ2)^2
可以证明，这两个式子是等价的：得到&amp;quot;类内差异&amp;quot;的最小值，等同于得到&amp;quot;类间差异&amp;quot;的最大值。不过，从计算难度看，后者的计算要容易一些。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
