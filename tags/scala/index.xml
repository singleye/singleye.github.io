<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scala on singleye</title>
    <link>/tags/scala/</link>
    <description>singleye (Scala)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <managingEditor>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</managingEditor>
    <webMaster>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</webMaster>
    <lastBuildDate>Thu, 11 Jan 2018 23:10:50 +0800</lastBuildDate>
    
    <atom:link href="/tags/scala/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark开发</title>
      <link>/2018/01/spark%E5%BC%80%E5%8F%91/</link>
      <pubDate>Thu, 11 Jan 2018 23:10:50 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2018/01/spark%E5%BC%80%E5%8F%91/</guid>
      <description>&lt;h2 id=&#34;使用scala进行开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8scala%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91&#34;&gt;
        #
    &lt;/a&gt;
    使用scala进行开发
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;step1-安装sbt&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step1-%e5%ae%89%e8%a3%85sbt&#34;&gt;
        ##
    &lt;/a&gt;
    Step1: 安装sbt
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
$ curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
$ sudo yum install sbt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# cat ~/.sbt/repositories
[repositories]
  # 本地源
  local
  # 阿里源
  aliyun: http://maven.aliyun.com/nexus/content/groups/public/
  typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly
  sonatype-oss-releases
  maven-central
  sonatype-oss-snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step2-创建项目&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step2-%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae&#34;&gt;
        ##
    &lt;/a&gt;
    Step2: 创建项目
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;可以使用Giter8模版创建项目。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ sbt new imarios/frameless.g8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;spark相关的几个模版：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;holdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project).&lt;/li&gt;
&lt;li&gt;imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark)&lt;/li&gt;
&lt;li&gt;nttdata-oss/basic-spark-project.g8 (Spark basic project.)&lt;/li&gt;
&lt;li&gt;spark-jobserver/spark-jobserver.g8 (Template for Spark Jobserver)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step3-编写代码&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step3-%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81&#34;&gt;
        ##
    &lt;/a&gt;
    Step3: 编写代码
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;创建出的项目目录中包含一下主要条目：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ ls
build.sbt  project  src  target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编写的代码放在 &amp;lsquo;src/main/scala/&amp;rsquo; 目录中：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ ls src/main/scala/
SimpleApp.scala

$ cat src/main/scala/SimpleApp.scala
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = &#34;people.txt&#34; // Should be some file on your system
    val spark = SparkSession.builder.appName(&#34;Simple Application&#34;).getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line =&gt; line.contains(&#34;a&#34;)).count()
    val numBs = logData.filter(line =&gt; line.contains(&#34;b&#34;)).count()
    println(s&#34;Lines with a: $numAs, Lines with b: $numBs&#34;)
    spark.stop()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step4-编译打包&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step4-%e7%bc%96%e8%af%91%e6%89%93%e5%8c%85&#34;&gt;
        ##
    &lt;/a&gt;
    Step4: 编译打包
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;程序开发好之后首先需要编译打包：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ sbt package
[info] Loading project definition from /home/spark/scala/frameless/project
[info] Updating {file:/home/spark/scala/frameless/project/}frameless-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Set current project to frameless (in build file:/home/spark/scala/frameless/)
[info] Updating {file:/home/spark/scala/frameless/}root...
[info] Resolving org.sonatype.oss#oss-parent;9 ...
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.11/scala-library-2.11.11.jar ...
[info]  [SUCCESSFUL ] org.scala-lang#scala-library;2.11.11!scala-library.jar (94788ms)

...

[info] downloading https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar ...
[info]  [SUCCESSFUL ] jline#jline;2.14.3!jline.jar (4836ms)
[info] Done updating.
[info] Compiling 1 Scala source to /home/spark/scala/frameless/target/scala-2.11/classes...
[info] &#39;compiler-interface&#39; not yet compiled for Scala 2.11.11. Compiling...
[info]   Compilation completed in 24.981 s
[info] Packaging /home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar ...
[info] Done packaging.
[success] Total time: 2795 s, completed Jan 16, 2018 3:03:12 PM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编译打包成功后的文件输出在 &amp;rsquo;target/scala-2.11/&amp;rsquo; 目录中：&lt;/p&gt;
&lt;p&gt;/home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar&lt;/p&gt;
&lt;h3 id=&#34;step5-部署运行&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step5-%e9%83%a8%e7%bd%b2%e8%bf%90%e8%a1%8c&#34;&gt;
        ##
    &lt;/a&gt;
    Step5: 部署运行
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
# su - spark
$ export SPARK_MAJOR_VERSION=2

$ spark-submit --class SimpleApp --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 target/scala-2.11/frameless_2.11-0.1.jar
SPARK_MAJOR_VERSION is set to 2, using Spark2
Warning: Master yarn-client is deprecated since 2.0. Please use master &#34;yarn&#34; with specified deploy mode instead.
18/01/17 15:20:18 INFO SparkContext: Running Spark version 2.2.0.2.6.3.0-235
18/01/17 15:20:19 INFO SparkContext: Submitted application: Simple Application
18/01/17 15:20:19 INFO SecurityManager: Changing view acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing view acls groups to:
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls groups to:
18/01/17 15:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view
 permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
18/01/17 15:20:20 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 43573.

...

18/01/17 15:21:30 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
18/01/17 15:21:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, bdtest-002, executor 1, partition 0, NODE_LOCAL, 4737 bytes)
18/01/17 15:21:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bdtest-002:44065 (size: 3.7 KB, free: 93.2 MB)
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.4:44312
18/01/17 15:21:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 141 bytes
18/01/17 15:21:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 96 ms on bdtest-002 (executor 1) (1/1)
18/01/17 15:21:30 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool
18/01/17 15:21:30 INFO DAGScheduler: ResultStage 3 (count at SimpleApp.scala:10) finished in 0.096 s
18/01/17 15:21:30 INFO DAGScheduler: Job 1 finished: count at SimpleApp.scala:10, took 0.289165 s
Lines with a: 1, Lines with b: 0
18/01/17 15:21:30 INFO AbstractConnector: Stopped Spark@17b03218{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
18/01/17 15:21:30 INFO SparkUI: Stopped Spark web UI at http://192.168.1.5:4042
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Shutting down all executors
18/01/17 15:21:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
18/01/17 15:21:30 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Stopped
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/01/17 15:21:31 INFO MemoryStore: MemoryStore cleared
18/01/17 15:21:31 INFO BlockManager: BlockManager stopped
18/01/17 15:21:31 INFO BlockManagerMaster: BlockManagerMaster stopped
18/01/17 15:21:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/01/17 15:21:31 INFO SparkContext: Successfully stopped SparkContext
18/01/17 15:21:31 INFO ShutdownHookManager: Shutdown hook called
18/01/17 15:21:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-730219ef-2995-4225-8743-5769fa6269db
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ndash;master: 指定spark driver的运行模式。
&lt;ul&gt;
&lt;li&gt;yarn-cluster: spark driver运行在yarn的application master进程中。如果应用只是进行计算可以使用这种方式运行，如果需要跟前台进行交互则可以考虑yarn-client模式。&lt;/li&gt;
&lt;li&gt;yarn-client: spark driver运行在client进程中，此时application master只负责向yarn申请资源。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使用python进行开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8python%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91&#34;&gt;
        #
    &lt;/a&gt;
    使用python进行开发
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;使用hdp环境提交任务&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8hdp%e7%8e%af%e5%a2%83%e6%8f%90%e4%ba%a4%e4%bb%bb%e5%8a%a1&#34;&gt;
        ##
    &lt;/a&gt;
    使用HDP环境提交任务
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;创建python文件&lt;code&gt;pi.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import sys
from random import random
from operator import add

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&amp;quot;PythonPi&amp;quot;)\.getOrCreate()

partitions = int(sys.argv[1]) if len(sys.argv) &amp;gt; 1 else 2
n = 100000 * partitions

def f(any):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &amp;lt;= 1 else 0

count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
print(&amp;quot;Pi is roughly %f&amp;quot; % (4.0 * count / n))

spark.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行&lt;code&gt;pi.py&lt;/code&gt;，这里指定使用yarn的cluster模式&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPARK_MAJOR_VERSION=2 spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --deploy pi.py 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;在hdp环境运行jupyter-server&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%9c%a8hdp%e7%8e%af%e5%a2%83%e8%bf%90%e8%a1%8cjupyter-server&#34;&gt;
        ##
    &lt;/a&gt;
    在HDP环境运行jupyter server
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;使用yarn的cluster模式运行，在运行期间会始终在yarn中占用资源，如果需要长期运行，使用&lt;code&gt;--master local&lt;/code&gt;比较好&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;XDG_RUNTIME_DIR=&amp;quot;&amp;quot; SPARK_MAJOR_VERSION=2 PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=&#39;notebook --no-browser --port 8888 --ip 121.43.171.231&#39; pyspark --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;为jupyter server设置密码等操作参考&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/stable/public_server.html&#34;&gt;jupyter文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;运行后访问服务器8888端口即可访问jupyter服务&lt;/p&gt;
&lt;h3 id=&#34;非hdp环境开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%9d%9ehdp%e7%8e%af%e5%a2%83%e5%bc%80%e5%8f%91&#34;&gt;
        ##
    &lt;/a&gt;
    非HDP环境开发
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;确保本机有一份spark二进制文件，比如&lt;code&gt;spark-2.2.1-bin-hadoop2.7&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;安装pyspark&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install pyspark
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;设置SPARK_HOME（可选）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export SPARK_HOME=/home/xxx/spark-2.2.1-bin-hadoop2.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过代码提交任务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
from pyspark import SparkConf, SparkContext

# if `SPARK_HOME` is undefined yet
if &#39;SPARK_HOME&#39; not in os.environ:
    os.environ[&#39;SPARK_HOME&#39;] = &#39;/home/xxx/spark-2.2.1-bin-hadoop2.7&#39;

conf = SparkConf().setAppName(&#39;Demo&#39;).setMaster(&#39;yarn&#39;).set(&#39;spark.yarn.deploy.mode&#39;, &#39;cluster&#39;)
sc = SparkContext(conf=conf)

# Do something with sc...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或者使用&lt;code&gt;SparkSession&lt;/code&gt; API&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from spark.sql import SparkSession
spark = (SparkSession.builder
          .master(&amp;quot;yarn&amp;quot;)
          .appName(&amp;quot;Demo&amp;quot;)
          .config(&amp;quot;spark.yarn.deploy.mode&amp;quot;, &amp;quot;cluster&amp;quot;)
          .getOrCreate())

# Do something with spark...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;oozie自动化&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#oozie%e8%87%aa%e5%8a%a8%e5%8c%96&#34;&gt;
        ##
    &lt;/a&gt;
    oozie自动化
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h1 id=&#34;development&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#development&#34;&gt;
        ##
    &lt;/a&gt;
    Development
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;Spark 2.0之前的开发接口主要是RDD（Resilient Distributed Dataset），从2.0之后原有&lt;a href=&#34;https://spark.apache.org/docs/latest/rdd-programming-guide.html&#34;&gt;RDD接口&lt;/a&gt;仍然支持，但RDD被Dataset取代，如果使用Dataset编程的话需要使用&lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark SQL&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;api&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#api&#34;&gt;
        #
    &lt;/a&gt;
    API
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/1.3.1/api/scala/index.html&#34;&gt;http://spark.apache.org/docs/1.3.1/api/scala/index.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;sparksession-vs-sparkcontext&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparksession-vs-sparkcontext&#34;&gt;
        #
    &lt;/a&gt;
    SparkSession vs SparkContext
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Spark 2.0之前有3个主要的连接对象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SparkContext: 建立与Spark执行环境相关的连接，用于创建RDD&lt;/li&gt;
&lt;li&gt;SqlContext：利用SparkContext背后的SparkSQL建立连接&lt;/li&gt;
&lt;li&gt;HiveContext：创建访问hive的接口&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从Spark 2.0开始，Datasets/Dataframes成为主要的数据访问接口，SparkSession(org.apache.spark.sql.SparkSession)成为主要的访问Spark执行环境的接口。&lt;/p&gt;
&lt;h3 id=&#34;sparkconf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparkconf&#34;&gt;
        ##
    &lt;/a&gt;
    SparkConf
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Spark 2.0之前需要先创建SparkConf，在创建SparkContext。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//set up the spark configuration and create contexts
val sparkConf = new SparkConf().setAppName(&#34;SparkSessionZipsExample&#34;).setMaster(&#34;local&#34;)
// your handle to SparkContext to access other context like SQLContext
val sc = new SparkContext(sparkConf).set(&#34;spark.some.config.option&#34;, &#34;some-value&#34;)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark 2.0之后可以不用显示的创建SparkConf对象就可以创建SparkSession对象。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
// Create a SparkSession. No need to create SparkContext
// You automatically get it as part of the SparkSession
val warehouseLocation = &#34;file:${system:user.dir}/spark-warehouse&#34;
val spark = SparkSession
   .builder()
   .appName(&#34;SparkSessionZipsExample&#34;)
   .config(&#34;spark.sql.warehouse.dir&#34;, warehouseLocation)
   .enableHiveSupport()
   .getOrCreate()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在SparkSession创建完后，Spark的运行时配置属性还可以通过SparkSession.conf()被修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//set new runtime options
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, 6)
spark.conf.set(&#34;spark.executor.memory&#34;, &#34;2g&#34;)
//get all settings
val configMap:Map[String, String] = spark.conf.getAll()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.javachen.com/2015/06/07/spark-configuration.html&#34;&gt;Spark conf参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sparksql&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparksql&#34;&gt;
        ##
    &lt;/a&gt;
    SparkSQL
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;2.0之前需要通过创建SqlContext来使用SparkSQL。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
val sc = new SparkContext(sparkConf).set(&#34;spark.some.config.option&#34;, &#34;some-value&#34;)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.0后通过SparkSession可以很方便的访问SparkSession.sql()来使用SparkSQL。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
// Now create an SQL table and issue SQL queries against it without
// using the sqlContext but through the SparkSession object.
// Creates a temporary view of the DataFrame
zipsDF.createOrReplaceTempView(&#34;zips_table&#34;)
zipsDF.cache()
val resultsDF = spark.sql(&#34;SELECT city, pop, state, zip FROM zips_table&#34;)
resultsDF.show(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;访问catalog-metadata&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%bf%e9%97%aecatalog-metadata&#34;&gt;
        ##
    &lt;/a&gt;
    访问Catalog Metadata
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;SparkSession暴露了访问metastore的接口SparkSession.catalog，这个接口可以用来访问catalog metadata信息，比如（Hcatalog）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//fetch metadata data from the catalog
spark.catalog.listDatabases.show(false)
spark.catalog.listTables.show(false)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
scala&gt; spark.catalog.listDatabases.show(false)
+-------+---------------------+------------------------------------------+
|name   |description          |locationUri                               |
+-------+---------------------+------------------------------------------+
|default|Default Hive database|hdfs://bdtest-001:8020/apps/hive/warehouse|
+-------+---------------------+------------------------------------------+

scala&gt; spark.catalog.listTables.show(false)
+-------------------------+--------+----------------------------------------+---------+-----------+
|name                     |database|description                             |tableType|isTemporary|
+-------------------------+--------+----------------------------------------+---------+-----------+
|activity_donate_record   |default |Imported by sqoop on 2018/01/08 19:03:33|MANAGED  |false      |
|activity_periods_config  |default |Imported by sqoop on 2018/01/08 19:04:20|MANAGED  |false      |
|activity_result          |default |Imported by sqoop on 2018/01/08 19:05:00|MANAGED  |false      |
|activity_school_info     |default |Imported by sqoop on 2018/01/08 19:05:46|MANAGED  |false      |
|activity_season_game_data|default |Imported by sqoop on 2018/01/08 19:06:23|MANAGED  |false      |
|activity_season_game_log |default |Imported by sqoop on 2018/01/08 19:07:00|MANAGED  |false      |
|ana_tag                  |default |Imported by sqoop on 2018/01/08 19:07:35|MANAGED  |false      |
|ana_user_tag             |default |Imported by sqoop on 2018/01/08 19:08:18|MANAGED  |false      |
|api_invoking_log         |default |Imported by sqoop on 2018/01/08 19:08:58|MANAGED  |false      |
|app_function             |default |Imported by sqoop on 2018/01/08 19:09:34|MANAGED  |false      |
|bank_card                |default |Imported by sqoop on 2018/01/08 19:10:09|MANAGED  |false      |
|bank_card_bin            |default |Imported by sqoop on 2018/01/08 19:10:46|MANAGED  |false      |
|command_invocation       |default |Imported by sqoop on 2018/01/08 19:11:26|MANAGED  |false      |
|coupon_grant_log         |default |Imported by sqoop on 2018/01/08 19:12:05|MANAGED  |false      |
|coupon_user              |default |Imported by sqoop on 2018/01/08 19:12:43|MANAGED  |false      |
|device_command           |default |Imported by sqoop on 2018/01/08 19:13:18|MANAGED  |false      |
|device_preorder          |default |Imported by sqoop on 2018/01/08 19:13:54|MANAGED  |false      |
|dro_dropdetails          |default |null                                    |MANAGED  |false      |
|dro_dropinfo             |default |Imported by sqoop on 2018/01/08 19:15:09|MANAGED  |false      |
|dro_exchange_record      |default |Imported by sqoop on 2018/01/08 19:15:43|MANAGED  |false      |
+-------------------------+--------+----------------------------------------+---------+-----------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dataframe-和-datasets&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataframe-%e5%92%8c-datasets&#34;&gt;
        #
    &lt;/a&gt;
    DataFrame 和 Datasets
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png&#34; alt=&#34;DataFrame &amp;amp; Dataset&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataset&#34;&gt;
        ##
    &lt;/a&gt;
    Dataset：
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;强类型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持functional和relational操作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;操作分类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;转换（transformation）：用于产生新的Datasets
&lt;ul&gt;
&lt;li&gt;例如：map，filter，select，aggregate（groupBy）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;操作（actions）：触发计算并返回结果
&lt;ul&gt;
&lt;li&gt;例如：count，show，把数据写回文件系统&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lazy：计算只有到action触发的时候才进行。Dataset内部可以理解为存储了如何进行计算的计划。当action执行的时候Spark query optimizer生成并优化计算方案后将其执行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder&#34;&gt;Encoder&lt;/a&gt;：把JVM中的类型T跟Spark SQL的数据表现形式进行转换的机制。（marshal/unmarshal?）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;两种创建方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用SparkSession的read()方法&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;
  val people = spark.read.parquet(&#34;...&#34;).as[Person]  // Scala
  Dataset&lt;Person&gt; people = spark.read().parquet(&#34;...&#34;).as(Encoders.bean(Person.class)); // Java
  &lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对现有Dataset做转换（transformation）&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;
  val names = people.map(_.name)  // in Scala; names is a Dataset[String]
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dataset&lt;String&gt; names = people.map((Person p) -&amp;gt; p.name, Encoders.STRING));
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dataset操作也可以是untyped，通过：&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&#34;&gt;Dataset&lt;/a&gt;, &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column&#34;&gt;Column&lt;/a&gt;, &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$&#34;&gt;DataFrame的function&lt;/a&gt;等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列操作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择一列（抛弃其他列）：select(&amp;ldquo;column&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;增加一列：withColumn(&amp;ldquo;newColumn&amp;rdquo;, Column)&lt;/li&gt;
&lt;li&gt;修改一列: withColumn(&amp;ldquo;column&amp;rdquo;, Column)&lt;/li&gt;
&lt;li&gt;删除一列: drop(&amp;ldquo;column&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;类型转换：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列高级操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UDF:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataframe&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataframe&#34;&gt;
        ##
    &lt;/a&gt;
    DataFrame：
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset的无类型view (untyped view)，对应Dataset的Row&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$&#34;&gt;DataFrame操作Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataFrame是有名列（named columns）组成的数据集合，类似关系数据库中的表或者python/R中的pandas。&lt;/p&gt;
&lt;p&gt;可以由多种数据源来构造DataFrame，比如Hive中的table，Spark的RDD（Resilient Distributed Datasets）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Datasets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SparkSession有很多种方法创建DataFrame和Datasets。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;DataFrame&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;创建API&lt;/td&gt;
&lt;td&gt;SparkSession.createDataFrame&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;其他&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Datasets&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;创建API&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;其他&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow&#34;&gt;
        ##
    &lt;/a&gt;
    &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row&#34;&gt;Row&lt;/a&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;创建Row&lt;/li&gt;
&lt;li&gt;访问
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;generic访问，使用ordinal序列访问&lt;/p&gt;
&lt;p&gt;row(0)：访问列第一个成员&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原始类型访问&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn&#34;&gt;
        ##
    &lt;/a&gt;
    &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column&#34;&gt;Column&lt;/a&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DataSet API:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&#34;&gt;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;处理时间&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%a4%84%e7%90%86%e6%97%b6%e9%97%b4&#34;&gt;
        ##
    &lt;/a&gt;
    处理时间
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;-ver--15&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#-ver--15&#34;&gt;
        ###
    &lt;/a&gt;
    * ver &amp;lt;= 1.5
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;使用hiveContext来查询&lt;/p&gt;
&lt;h4 id=&#34;--15--ver--22&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#--15--ver--22&#34;&gt;
        ###
    &lt;/a&gt;
    *  1.5 &amp;lt;= ver &amp;lt; 2.2
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Spark 1.5引入了unix_timestamp()，所以在1.5之后到2.2的spark版本中可以这样操作时间信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
import org.apache.spark.sql.functions.{unix_timestamp, to_date}

val df = Seq((1L, &#34;01-APR-2015&#34;)).toDF(&#34;id&#34;, &#34;ts&#34;)

df.select(to_date(unix_timestamp(
  $&#34;ts&#34;, &#34;dd-MMM-yyyy&#34;
).cast(&#34;timestamp&#34;)).alias(&#34;timestamp&#34;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;-ver--22&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#-ver--22&#34;&gt;
        ###
    &lt;/a&gt;
    * ver &amp;gt;= 2.2
&lt;/div&gt;
&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;
import org.apache.spark.sql.functions.{to_date, to_timestamp}

df.select(to_date($&#34;ts&#34;, &#34;dd-MMM-yyyy&#34;).alias(&#34;date&#34;))

df.select(to_timestamp($&#34;ts&#34;, &#34;dd-MM-yyyy HH:mm:ss&#34;).alias(&#34;timestamp&#34;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;import-sparkimplicits_&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#import-sparkimplicits_&#34;&gt;
        ##
    &lt;/a&gt;
    import spark.implicits._
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;引入toDF()方法和“$”操作符&lt;/p&gt;
&lt;h2 id=&#34;编程陷阱&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bc%96%e7%a8%8b%e9%99%b7%e9%98%b1&#34;&gt;
        #
    &lt;/a&gt;
    编程陷阱
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%99%b7%e9%98%b11dataframe%e6%9c%89%e5%85%83%e7%b4%a0%e4%bd%86%e9%80%9a%e8%bf%87%e5%be%aa%e7%8e%af%e6%8a%8a%e5%80%bc%e6%94%be%e5%85%a5listbuffer%e5%90%8elistbuffer%e5%8d%b4%e4%b8%ba%e7%a9%ba%e5%88%97%e8%a1%a8&#34;&gt;
        ##
    &lt;/a&gt;
    陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
   val valueDF = allData.select(&#34;value&#34;).distinct()
    valueDF.show()
    //valueDF.foreach(r =&gt; valueList.append(r.getAs[Int](&#34;value&#34;)))

    for (row &lt;- valueDF) {
      println(&#34;value:&#34; + row.getAs[Int](&#34;value&#34;))
      valueList.append(row.getAs[Int](&#34;value&#34;))
    }
    println(&#34;Values:&#34; + valueList)
    valueList.foreach(println))

// 程序运行输出

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

Values:ListBuffer()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看出两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题1: 虽然可以知道valueDF不为空，可是&amp;quot;println(&amp;hellip;)&amp;ldquo;却没有任何输出内容&lt;/li&gt;
&lt;li&gt;问题2: 结果valueList为空&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;原因&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%8e%9f%e5%9b%a0&#34;&gt;
        ###
    &lt;/a&gt;
    原因：
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Spark任务执行是通过Driver/Executor来完成的，Driver控制任务执行，Executor负责任务执行，foreach/for会被Driver分配给Executor来执行（分布式执行），每个Executor创建一个自己的ListBuffer拷贝，因此当任务结束后信息就丢失了。&lt;/p&gt;
&lt;p&gt;参考Stackoverflow的讨论：&lt;a href=&#34;https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop&#34;&gt;https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;修复方法使用collect&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bf%ae%e5%a4%8d%e6%96%b9%e6%b3%95%e4%bd%bf%e7%94%a8collect&#34;&gt;
        ###
    &lt;/a&gt;
    修复方法：使用collect()
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;collect()定义：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
def collect(): Array[T]
Returns an array that contains all rows in this Dataset.
Running collect requires moving all the data into the application&#39;s driver
process, and doing so on a very large dataset can crash the driver process with
OutOfMemoryError.
For Java API, use collectAsList.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修复代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
val valueDF = allData.select(&#34;value&#34;).distinct()
valueDF.show()
//valueDF.foreach(r =&gt; valueList.append(r.getAs[Int](&#34;value&#34;)))

for (row &lt;- valueDF.collect()) {
  println(&#34;value:&#34; + row.getAs[Int](&#34;value&#34;))
  valueList.append(row.getAs[Int](&#34;value&#34;))
}
println(&#34;getAllChannels:&#34; + valueList)


// 程序运行输出如下

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

value:1
value:3
value:5
value:4
value:10
value:2
value:0
getAllChannels:ListBuffer(1, 3, 5, 4, 10, 2, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        ##
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://community.hortonworks.com/articles/84071/apache-ambari-workflow-manager-view-for-apache-ooz-2.html&#34;&gt;Ambari workflow manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.scala-lang.org/tour/basics.html&#34;&gt;Scala basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/foundweekends/giter8/wiki/giter8-templates&#34;&gt;Giter8项目模版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_spark-component-guide/content/ch_oozie-spark-action.html&#34;&gt;Automating Spark job with Oozie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://community.hortonworks.com/articles/104949/using-virtualenv-with-pyspark-1.html&#34;&gt;Using VirtualEnv with PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
