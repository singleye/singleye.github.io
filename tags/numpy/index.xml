<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Numpy on 好奇心是探索未知世界的钥匙</title>
    <link>https://www.singleye.net/tags/numpy/</link>
    <description>Recent content in Numpy on 好奇心是探索未知世界的钥匙</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 02 Oct 2017 02:11:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.singleye.net/tags/numpy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>softmax输出层公式推导及代码实验</title>
      <link>https://www.singleye.net/2017/10/softmax%E8%BE%93%E5%87%BA%E5%B1%82%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E9%AA%8C/</link>
      <pubDate>Mon, 02 Oct 2017 02:11:00 +0000</pubDate>
      
      <guid>https://www.singleye.net/2017/10/softmax%E8%BE%93%E5%87%BA%E5%B1%82%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E9%AA%8C/</guid>
      <description>sigmoid激活函数在神经网络中有着强大的通用性，但也存在这一些问题，比如：
 在w/b参数还没有训练成熟时，训练预测偏差较大，此时的训练速度会较慢。这个问题的解决方法有两种：  使用交叉熵代价函数: $ C = -{1\over n} \sum_{i=1}^n [y_i \ln a_i + (1-y_i) \ln (1-a_i)] $ 使用softmax和log-likelyhood代价函数作为输出层  sigmoid的输出结果是伯努利分布 $ P(y_1|X), P(y_2|X), &amp;hellip; P(y_n|X) $，说明每一个输出项之间是相互独立的，这在预测一种输出结果的情形时不太符合人们的直观感受。这个问题也可以用softmax输出层解决，因为softmax的输出是多项分布：$ P(y_1, y_2, &amp;hellip; y_n | X) $，其中y1, &amp;hellip; yn之间相互关联，且总和为1。  这样看起来softmax是个很有效的方法，下面就对这个方法进行一些研究。
softmax定义：  $$ softmax(z_j) = {e^{z_j} \over {\sum_{i=1}^m e^{z_i} }} , j=1, ... m $$  将softmax层应用在网络输出层时，每一个神经元的softmax激活输出可以理解为该神经元对应结果的预测概率，这里有几个基本事实：
 每个神经元的输出为正数，并且输出数值介于0-1之间。 所有神经元的输出总和为1。 某一项输入（Z值）增大时，其对应的输出概率增大；同时其他输出概率同时减小（总和总是1）。该结论可以从$ \frac {\partial a_i} {\partial {z_i}} $（总为正数）以及$ \frac {\partial a_i} {\partial {z_j}}$（总为负数）推算出来，这两个数字也说明了softmax的输入／输出单调性。 softmax的每个激活输出值之间相互关联，表现出了输出非局部性特征。直观的理解就是因为所有激活输出的总和总是为1，那么其中一个激活输出的值发生变动的时候其他的激活输出也必将变化。这一点也是跟sigmoid激活函数的很不同的一点，也说明了$ \frac {\partial a_i} {\partial {z_j}}$值存在的意义。  下图展示了softmax层工作的基本原理。</description>
    </item>
    
    <item>
      <title>神经网络实践：自动驾驶</title>
      <link>https://www.singleye.net/2017/08/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E8%B7%B5%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/</link>
      <pubDate>Thu, 17 Aug 2017 20:01:00 +0000</pubDate>
      
      <guid>https://www.singleye.net/2017/08/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E8%B7%B5%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/</guid>
      <description>最近学习了下神经网络，于是写了一个开车的小游戏，然后训练了一个6层神经网络自己驾驶练练手。
代码实现主要用了pygame和numpy，网络有7个输入分别对应小车前面的7个距离探头数据，2个输出进行转向输出。
 您还没有安装flash播放器,请点击 这里 安装</description>
    </item>
    
    <item>
      <title>图像卷积实践</title>
      <link>https://www.singleye.net/2017/07/%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Sun, 30 Jul 2017 18:30:50 +0800</pubDate>
      
      <guid>https://www.singleye.net/2017/07/%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E5%AE%9E%E8%B7%B5/</guid>
      <description>最近对图像识别技术很感兴趣，了解到在这个领域中CNN的应用可以比较有效的解决问题，这里对卷积（convolution）相关的知识进行一下记录说明。
图像卷积是什么？ 将一张图片看作一张像素的矩阵的话，卷积就是把另一个矩阵（卷积核）在这张图片上移动，在移动的过程中取图片上对应大小的矩阵与卷积核进行运算，每次矩阵运算得出的结果保存成一个新的像素，这个过程就是图像的卷积运算。
卷积的过程可以用下面的示意图展示：
为什么做卷积？ 一张原始图像包含了大量的噪音信息，这些噪音信息会干扰后续的运算过程。如果将一张图像看作一个输入信号的话，如果找到一种过滤器将噪音信息过滤掉就可以提高后续运算的准确度。卷积就是这么一个过滤器，这个过滤器的正式称呼是卷积核。
那么这个过滤器可以做些什么呢？其实常见的图像处理软件早已经在使用卷积进行图片处理了，比如图像锐化、模糊、浮雕效果等等&amp;hellip;
下面收集了一些常用的过滤器，对这张图片处理后可以看一下效果。
 图像边界检测   $$ \left[ \begin{matrix} -1 &amp; -1 &amp; -1 \\ -1 &amp; 8 &amp; -1 \\ -1 &amp; -1 &amp; -1 \end{matrix} \right] $$   图像模糊   $$ \left[ \begin{matrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{matrix} \right] $$   图像锐化   $$ \left[ \begin{matrix} -1 &amp; -1 &amp; -1 \\ -1 &amp; 9 &amp; -1 \\ -1 &amp; -1 &amp; -1 \end{matrix} \right] $$   浮雕   $$ \left[ \begin{matrix} -1 &amp; -1 &amp; 0 \\ -1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{matrix} \right] $$  用numpy进行卷积计算 以上的图片使用下面的算法生成，主要使用了numpy的array进行的计算。通过该算法生成的图片效果还不够理想：</description>
    </item>
    
    <item>
      <title>numpy学习笔记[1]</title>
      <link>https://www.singleye.net/2017/03/numpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</link>
      <pubDate>Thu, 16 Mar 2017 14:45:31 +0800</pubDate>
      
      <guid>https://www.singleye.net/2017/03/numpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</guid>
      <description>numpy数据结构 基本数据    数据类型 描述     bool 用一个字节存储的布尔类型（True或False）   inti 由所在平台决定其大小的整数（一般为int32或int64）   int8 一个字节大小，-128 至 127   int16 整数，-32768 至 32767   int32 整数，-2 ** 31 至 2 ** 32 -1   int64 整数，-2 ** 63 至 2 ** 63 - 1   uint8 无符号整数，0 至 255   uint16 无符号整数，0 至 65535   uint32 无符号整数，0 至 2 ** 32 - 1   uint64 无符号整数，0 至 2 ** 64 - 1   float16 半精度浮点数：16位，正负号1位，指数5位，精度10位   float32 单精度浮点数：32位，正负号1位，指数8位，精度23位   float64或float 双精度浮点数：64位，正负号1位，指数11位，精度52位   complex64 复数，分别用两个32位浮点数表示实部和虚部   complex128或complex 复数，分别用两个64位浮点数表示实部和虚部    array ‘array’表示元素数据大小固定的同质（相同数据类型）多维度数据。</description>
    </item>
    
  </channel>
</rss>