<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bp on 好奇心是探索未知世界的钥匙</title>
    <link>https://www.singleye.net/tags/bp/</link>
    <description>Recent content in Bp on 好奇心是探索未知世界的钥匙</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 25 Sep 2017 15:44:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.singleye.net/tags/bp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>神经网络之反向传播算法</title>
      <link>https://www.singleye.net/2017/09/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 25 Sep 2017 15:44:00 +0000</pubDate>
      
      <guid>https://www.singleye.net/2017/09/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</guid>
      <description>之前使用神经网络算法的时候并没有认真总结关键的算法，虽然可以用但总觉得不爽，于是这两天对神经网络算法中的反向传播（Back Propagation）进行了推导。即理解了算法的数学本质，也对神经网络算法的工程特性有了深刻体会，工程算法真的是以解决问题为驱动的，追求的是解决问题的实用性。
神经网络 网络拓扑 神经元 神经元是神经网络的基本构成，上图的每一个圆圈代表了一个神经元。每一个神经元有一个输入和一个输出，神经元的作用是对输入值进行计算。下图是一个神经元的简单示意图：
 该神经元的输出：$ a=f(z)=sigmoid(z) $  神经元输入 在一个复杂点的神经网络中，一个神经元接收来自多个前级神经元的激活输出，并进行加权相加后产生该神经元的输入值，这个过程示意图如下：
定义第 $ l^{th} $ 层第 $ j^{th} $ 个神经元的输入：
神经元加权 神经元的加权结构可以看下面的示意图：
注，第一层神经元的输入就是采样数据，不需要计算z值，这层采样数据直接通过权重计算输入到第二层的神经元。
反向传播算法 代价函数 定义代价函数：$ cost = {1 \over 2} \sum (y^{(i)} - a^{(i)})^2 $
神经元错误量 $ \delta_j^{(l)} $ 每个神经元的输入记为&amp;rsquo;z&amp;rsquo;，经过激活函数&amp;rsquo;f(z)&amp;lsquo;生成激活值&amp;rsquo;a&amp;rsquo;，通常情况下激活函数使用sigmoid()。那么假设对于每个神经元的输入&amp;rsquo;z&amp;rsquo;做一点微小的改变记为 $ \Delta z $，由这个改变引起的代价变化记为这个神经元的错误量 $ \delta_j^{(l)} $，从这个定义可以看出来这是一个代价函数相对于神经元的输入&amp;rsquo;z&amp;rsquo;的偏导数。
定义 $ \delta_j^{(l)} $ 为 $ l^{th} $ 层中的第 $ j^{th} $ 个神经元的错误量，记作：$ \delta_j^{(l)} =\frac{\partial C}{\partial z_j^{(l)}} $
经过数学推导可以得出结论：</description>
    </item>
    
  </channel>
</rss>