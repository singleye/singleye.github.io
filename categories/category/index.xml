<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Category on singleye</title>
    <link>/categories/category/</link>
    <description>singleye (Category)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <managingEditor>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</managingEditor>
    <webMaster>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</webMaster>
    <lastBuildDate>Wed, 22 Nov 2023 12:00:01 +0800</lastBuildDate>
    
    <atom:link href="/categories/category/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PCL 点云数据过滤处理</title>
      <link>/2023/11/pcl-%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 22 Nov 2023 12:00:01 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2023/11/pcl-%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4%E5%A4%84%E7%90%86/</guid>
      <description>&lt;h1 id=&#34;点云过滤&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%82%b9%e4%ba%91%e8%bf%87%e6%bb%a4&#34;&gt;
        ##
    &lt;/a&gt;
    点云过滤
&lt;/div&gt;
&lt;/h1&gt;</description>
    </item>
    
    <item>
      <title>Wechat_development</title>
      <link>/2023/11/wechat_development/</link>
      <pubDate>Wed, 08 Nov 2023 23:40:06 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2023/11/wechat_development/</guid>
      <description>&lt;h1 id=&#34;订阅号与服务号的区别&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%a2%e9%98%85%e5%8f%b7%e4%b8%8e%e6%9c%8d%e5%8a%a1%e5%8f%b7%e7%9a%84%e5%8c%ba%e5%88%ab&#34;&gt;
        ##
    &lt;/a&gt;
    订阅号与服务号的区别
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Explanation_of_interface_privileges.html&#34;&gt;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Explanation_of_interface_privileges.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;订阅号侧重向用户推送文章，服务号侧重对用户做服务（交易等）但发送文章很少&lt;/p&gt;
&lt;h1 id=&#34;网页开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e9%a1%b5%e5%bc%80%e5%8f%91&#34;&gt;
        ##
    &lt;/a&gt;
    网页开发
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;机制：网页授权
场景：在微信中打开第三方网页，可以用网页授权机制获取用户信息&lt;/p&gt;
&lt;p&gt;scope&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;snsapi_base: 获取进入页面的用户 openid（静默授权）&lt;/li&gt;
&lt;li&gt;snsapi_userinfo: 获取进入页面的用户&lt;strong&gt;基本信息&lt;/strong&gt;，需要用户手动同意，无需用户关注&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;网页获取用户信息&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e9%a1%b5%e8%8e%b7%e5%8f%96%e7%94%a8%e6%88%b7%e4%bf%a1%e6%81%af&#34;&gt;
        #
    &lt;/a&gt;
    网页获取用户信息
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;scope&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#scope&#34;&gt;
        ##
    &lt;/a&gt;
    scope
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;snsapi_base&lt;/li&gt;
&lt;li&gt;snsapi_userinfo&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=APPID&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&#34;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&#34;&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过网页页面获取用户信息的方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;引导用户点击带 redirect_uri 的内容（比如，通过菜单设置: &lt;a href=&#34;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&#34;&gt;https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83&amp;amp;redirect_uri=REDIRECT_URI&amp;amp;response_type=code&amp;amp;scope=SCOPE&amp;amp;state=STATE#wechat_redirect&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;用户点击后经过腾讯 oauth2 回调到 redirect_uri，并且带有 &amp;lsquo;code&amp;rsquo; 参数&lt;/li&gt;
&lt;li&gt;页面处理 code 信息，例如将 code 作为一个参数带入调用后端的 API 接口&lt;/li&gt;
&lt;li&gt;后端调用腾讯 sns/userinfo 接口获取用户信息
&lt;ol&gt;
&lt;li&gt;首先调用 &lt;a href=&#34;https://api.weixin.qq.com/sns/oauth2/access_token&#34;&gt;https://api.weixin.qq.com/sns/oauth2/access_token&lt;/a&gt; 获取 access token&lt;/li&gt;
&lt;li&gt;再调用 &lt;a href=&#34;https://api.weixin.qq.com/sns/userinfo&#34;&gt;https://api.weixin.qq.com/sns/userinfo&lt;/a&gt; 获取用户信息&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;weixin 与手机号绑定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引导用户进入带oauth2回调页面，获取用户 code&lt;/li&gt;
&lt;li&gt;使用 code 检查用户信息
&lt;ul&gt;
&lt;li&gt;如果已经存在用户，则返回 token 并正常打开 next 页面&lt;/li&gt;
&lt;li&gt;如果不存在，则进入注册信息绑定页面，绑定手机号（短信验证）
&lt;ul&gt;
&lt;li&gt;验证通过后，创建用户并返回 token 并正常打开 next 页面&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;网站实现扫码登陆方法&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bd%91%e7%ab%99%e5%ae%9e%e7%8e%b0%e6%89%ab%e7%a0%81%e7%99%bb%e9%99%86%e6%96%b9%e6%b3%95&#34;&gt;
        ##
    &lt;/a&gt;
    网站实现扫码登陆方法：
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html&#34;&gt;https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;步骤1：嵌入下面 JS 文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http://res.wx.qq.com/connect/zh_CN/htmledition/js/wxLogin.js
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;步骤2：在需要使用微信登录的地方实例以下JS对象：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; var obj = new WxLogin({
 self_redirect:true,
 id:&amp;#34;login_container&amp;#34;, 
 appid: &amp;#34;&amp;#34;, 
 scope: &amp;#34;&amp;#34;, 
 redirect_uri: &amp;#34;&amp;#34;,
  state: &amp;#34;&amp;#34;,
 style: &amp;#34;&amp;#34;,
 href: &amp;#34;&amp;#34;
 });
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;业务域名&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NLP 资源整理</title>
      <link>/2019/09/nlp-%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</link>
      <pubDate>Thu, 19 Sep 2019 12:18:54 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2019/09/nlp-%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</guid>
      <description>&lt;!--toc--&gt;
&lt;h1 id=&#34;模型&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%a8%a1%e5%9e%8b&#34;&gt;
        ##
    &lt;/a&gt;
    模型
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GloVe: Global Vectors for Word Representation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;https://nlp.stanford.edu/projects/glove/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34;&gt;https://nlp.stanford.edu/pubs/glove.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Annotated Transformer&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://github.com/openai/finetune-transformer-lm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;language understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;Improving Language Understanding with Unsupervised Learning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-2 (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;Better Language Models and Their Implications&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer-XL (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kimiyoung/transformer-xl&#34;&gt;https://github.com/kimiyoung/transformer-xl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1901.02860&#34;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Attentive Language Models Beyond a Fixed-Length Context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLNet (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/zihangdai/xlnet/&#34;&gt;https://github.com/zihangdai/xlnet/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLM (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/XLM/&#34;&gt;https://github.com/facebookresearch/XLM/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07291&#34;&gt;Cross-lingual Language Model Pretraining&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RoBERTa (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/master/examples/roberta&#34;&gt;https://github.com/pytorch/fairseq/tree/master/examples/roberta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DistilBERT (from HuggingFace)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/distillation&#34;&gt;https://github.com/huggingface/transformers/tree/master/examples/distillation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://medium.com/huggingface/distilbert-8cf3380435b5&#34;&gt;Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;项目&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%a1%b9%e7%9b%ae&#34;&gt;
        ##
    &lt;/a&gt;
    项目
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://allennlp.org&#34;&gt;https://allennlp.org&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/didi/delta&#34;&gt;https://github.com/didi/delta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.01853.pdf&#34;&gt;https://arxiv.org/pdf/1908.01853.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;滴滴 Delta&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/software/CRF-NER.html&#34;&gt;https://nlp.stanford.edu/software/CRF-NER.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRF&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Stanford CRF NER&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;论文&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;https://nlp.stanford.edu/projects/glove/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GloVe: Global Vectors for Word Representation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34;&gt;https://nlp.stanford.edu/pubs/glove.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The Annotated Transformer&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;https://github.com/huggingface/transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transformers&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://huggingface.co/transformers&#34;&gt;https://huggingface.co/transformers&lt;/a&gt; 实现了很多模型（Bert, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, DistilBERT）&amp;lt;\b&amp;gt; &lt;a href=&#34;https://transformer.huggingface.co&#34;&gt;https://transformer.huggingface.co&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://github.com/openai/finetune-transformer-lm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GPT (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;language understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;Improving Language Understanding with Unsupervised Learning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GPT-2 (from OpenAI)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;Better Language Models and Their Implications&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kimiyoung/transformer-xl&#34;&gt;https://github.com/kimiyoung/transformer-xl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transformer-XL (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1901.02860&#34;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Attentive Language Models Beyond a Fixed-Length Context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/zihangdai/xlnet/&#34;&gt;https://github.com/zihangdai/xlnet/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;XLNet (from Google/CMU)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/XLM/&#34;&gt;https://github.com/facebookresearch/XLM/&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;XLM (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07291&#34;&gt;Cross-lingual Language Model Pretraining&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/master/examples/roberta&#34;&gt;https://github.com/pytorch/fairseq/tree/master/examples/roberta&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;RoBERTa (from Facebook)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/distillation&#34;&gt;https://github.com/huggingface/transformers/tree/master/examples/distillation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;DistilBERT (from HuggingFace)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://medium.com/huggingface/distilbert-8cf3380435b5&#34;&gt;Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hundredblocks/concrete_NLP_tutorial&#34;&gt;https://github.com/hundredblocks/concrete_NLP_tutorial&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;An NLP workshop by Emmanuel Ameisen &lt;a href=&#34;https://twitter.com/EmmanuelAmeisen&#34;&gt;(@EmmanuelAmeisen)&lt;/a&gt;, from Insight AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/BrikerMan/Kashgari&#34;&gt;https://github.com/BrikerMan/Kashgari&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word2Vec, BERT, and GPT2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Kashgari is a Production-ready NLP Transfer learning framework for text-labeling and text-classification, includes Word2Vec, BERT, and GPT2 Language Embedding.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kyzhouhzau/BERT-NER&#34;&gt;https://github.com/kyzhouhzau/BERT-NER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bert&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;基于 CoNLL-2003 数据集的实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/ProHiryu/bert-chinese-ner&#34;&gt;https://github.com/ProHiryu/bert-chinese-ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;基于人民日报数据集的实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/macanv/BERT-BiLSTM-CRF-NER&#34;&gt;https://github.com/macanv/BERT-BiLSTM-CRF-NER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;bert&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;training, serving&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/marcotcr/lime&#34;&gt;https://github.com/marcotcr/lime&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;用于解释机器学习的分类器。&lt;/p&gt;论文：&lt;a href=&#34;https://arxiv.org/abs/1602.04938&#34;&gt;https://arxiv.org/abs/1602.04938&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;数据集&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;
        ##
    &lt;/a&gt;
    数据集
&lt;/div&gt;
&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus&#34;&gt;https://github.com/SophonPlus/ChineseNlpCorpus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseWordVectors&#34;&gt;https://github.com/SophonPlus/ChineseWordVectors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb&#34;&gt;ChnSentiCorp_htl_all&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/CAIL&#34;&gt;https://github.com/thunlp/CAIL&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chinese AI &amp;amp; Law Challenge &lt;a href=&#34;http://cail.cipsc.org.cn&#34;&gt;http://cail.cipsc.org.cn&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;ner-数据集&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ner-%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;
        #
    &lt;/a&gt;
    NER 数据集
&lt;/div&gt;
&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/ontonotes/conll-formatted-ontonotes-5.0&#34;&gt;https://github.com/ontonotes/conll-formatted-ontonotes-5.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This is a CoNLL formatted version of the OntoNotes 5.0 release.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/juand-r/entity-recognition-datasets#references&#34;&gt;https://github.com/juand-r/entity-recognition-datasets#references&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A collection of corpora for named entity recognition (NER) and entity recognition tasks. These annotated datasets cover a variety of languages, domains and entity types.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;学习资料&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99&#34;&gt;
        ##
    &lt;/a&gt;
    学习资料
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;nlp-roadmap&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#nlp-roadmap&#34;&gt;
        #
    &lt;/a&gt;
    NLP roadmap
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/graykode/nlp-roadmap&#34;&gt;https://github.com/graykode/nlp-roadmap&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;probability--statistics&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#probability--statistics&#34;&gt;
        ##
    &lt;/a&gt;
    Probability &amp;amp; Statistics
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/prob.png&#34; alt=&#34;Probability &amp;amp; Statistics&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;machine-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#machine-learning&#34;&gt;
        ##
    &lt;/a&gt;
    Machine Learning
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/ml.png&#34; alt=&#34;Machine Learning&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;text-mining&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#text-mining&#34;&gt;
        ##
    &lt;/a&gt;
    Text Mining
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/textmining.png&#34; alt=&#34;Text Mining&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#natural-language-processing&#34;&gt;
        ##
    &lt;/a&gt;
    Natural Language Processing
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/graykode/nlp-roadmap/raw/master/img/nlp.png&#34; alt=&#34;Natural Language Processing&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;标注工具&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e6%a0%87%e6%b3%a8%e5%b7%a5%e5%85%b7&#34;&gt;
        ##
    &lt;/a&gt;
    标注工具
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;| 工具 | 链接 | 描述 |
|&amp;mdash;+&amp;mdash;+&amp;mdash;|
| Prodigy |https://prodi.gy/docs/|Prodigy (explosion.ai 开发 spacy 的公司)|
| brat |https://github.com/nlplab/brat||
| Knowtator |http://knowtator.sourceforge.net/index.shtml||
| Protégé + Knowtator plugin |https://github.com/UCDenver-ccp/Knowtator-2.0 &lt;a href=&#34;https://protege.stanford.edu/short-courses.php&#34;&gt;https://protege.stanford.edu/short-courses.php&lt;/a&gt;||
||http://deepdive.stanford.edu/labeling||
||https://github.com/SongRb/DeepDiveChineseApps||
||https://github.com/qiangsiwei/DeepDive_Chinese||
||https://github.com/jiesutd/SUTDAnnotator||
||https://github.com/HazyResearch/snorkel||
||https://bitbucket.org/dainkaplan/slate/||
| iepy | &lt;a href=&#34;https://github.com/machinalis/iepy&#34;&gt;https://github.com/machinalis/iepy&lt;/a&gt; | 标注，信息提取 |
| doccano |https://github.com/chakki-works/doccano||
| YEDDA |https://github.com/jiesutd/YEDDA||
| Chinese-Annotator | &lt;a href=&#34;https://github.com/deepwel/Chinese-Annotator&#34;&gt;https://github.com/deepwel/Chinese-Annotator&lt;/a&gt; | online/offline 结合的中文标注工具，想法比较好，目前项目还不完善 |
| HanNLP | &lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;https://github.com/hankcs/HanLP&lt;/a&gt; | NLP 工具箱（中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁），Java语言 |
| poplar |https://github.com/synyi/poplar|国内“森亿”公司开发|&lt;/p&gt;
&lt;h2 id=&#34;brat&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat&#34;&gt;
        #
    &lt;/a&gt;
    brat
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;brat-配置&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat-%e9%85%8d%e7%bd%ae&#34;&gt;
        ##
    &lt;/a&gt;
    brat 配置
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;annotation-配置-annotationconf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#annotation-%e9%85%8d%e7%bd%ae-annotationconf&#34;&gt;
        ###
    &lt;/a&gt;
    Annotation 配置 annotation.conf
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[entities]&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[entities]	 
Person
Location
Organization
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[relations]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数指定格式 &amp;ldquo;ARG:TYPE&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[relations]	 
Family	Arg1:Person, Arg2:Person
Employment	Arg1:Person, Arg2:Organization
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;参数可以有多个，使用 &amp;ldquo;|&amp;rdquo; 分隔&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[relations]	 	 
Located	Arg1:Person,	Arg2:Building|City|Country
Located	Arg1:Building,	Arg2:City|Country
Located	Arg1:City,	Arg2:Country
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[events]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;事件参数格式 &amp;ldquo;ROLE:TYPE&amp;rdquo;, ROLE 可以任意指定。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[events]	 
Marriage	Participant1:Person, Participant2:Person
Bankruptcy	Org:Company
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;[attributes]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;属性作用域 &amp;ldquo;ARG:TYPE&amp;rdquo; ，可以用在 relation 和 event 中。&lt;/p&gt;
&lt;p&gt;拥有多个值的属性的值的定义方法是 &amp;ldquo;Value:VAL1|VAL2|VAL3[&amp;hellip;]&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[attributes]	 
Negated	Arg:&amp;lt;EVENT&amp;gt;
Confidence	Arg:&amp;lt;EVENT&amp;gt;, Value:L1|L2|L3
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;brat-标注信息格式&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#brat-%e6%a0%87%e6%b3%a8%e4%bf%a1%e6%81%af%e6%a0%bc%e5%bc%8f&#34;&gt;
        ##
    &lt;/a&gt;
    brat 标注信息格式
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://brat.nlplab.org/standoff.html&#34;&gt;http://brat.nlplab.org/standoff.html&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;T1	Organization 0 4	Sony
T2	MERGE-ORG 14 27	joint venture
T3	Organization 33 41	Ericsson
E1	MERGE-ORG:T2 Org1:T1 Org2:T3
T4	Country 75 81	Sweden
R1	Origin Arg1:T3 Arg2:T4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;protege--knowtator&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#protege--knowtator&#34;&gt;
        #
    &lt;/a&gt;
    Protege &amp;amp; Knowtator
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;编译 Knowtator 插件：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/UCDenver-ccp/Knowtator-2.0.git
mvn clean install
cp xxx/plugins/knowtator-2.1.5.jar /Applications/Protégé.app/Contents/Java/plugins/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启 Protege&lt;/p&gt;
&lt;h2 id=&#34;iepy&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#iepy&#34;&gt;
        #
    &lt;/a&gt;
    iepy
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;iepy 的 preprcess.py 会失败，需要按照以下方式修改 corenlp.sh 脚本&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Preprocess not running under MacOS&lt;/p&gt;
&lt;p&gt;Problems with the preprocess under MacOS? Apparently a change in the CoreNLP script is needed to be run. You need to change the file corenlp.sh that is located on /Users/&lt;your user&gt;/Library/Application Support/iepy/stanford-corenlp-full-2014-08-27/ and change scriptdir=&lt;code&gt;dirname $0&lt;/code&gt; for scriptdir=&lt;code&gt;dirname &amp;quot;$0&amp;quot;&lt;/code&gt; (ie, add double quotes around $0)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://iepy.readthedocs.io/en/stable/troubleshooting.html#troubleshooting&#34;&gt;https://iepy.readthedocs.io/en/stable/troubleshooting.html#troubleshooting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;自然语言处理工具&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e5%b7%a5%e5%85%b7&#34;&gt;
        ##
    &lt;/a&gt;
    自然语言处理工具
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;自然语言处理 中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;https://github.com/hankcs/HanLP&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;nlp-应用&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#nlp-%e5%ba%94%e7%94%a8&#34;&gt;
        ##
    &lt;/a&gt;
    NLP 应用
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;聊天机器人&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%81%8a%e5%a4%a9%e6%9c%ba%e5%99%a8%e4%ba%ba&#34;&gt;
        #
    &lt;/a&gt;
    聊天机器人
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/3c6f1e32e128&#34;&gt;用 TensorFlow 做个聊天机器人&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;论文&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;
        ##
    &lt;/a&gt;
    论文
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
