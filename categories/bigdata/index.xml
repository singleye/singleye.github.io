<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BigData on singleye</title>
    <link>/categories/bigdata/</link>
    <description>singleye (BigData)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <managingEditor>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</managingEditor>
    <webMaster>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</webMaster>
    <lastBuildDate>Thu, 11 Jan 2018 23:10:50 +0800</lastBuildDate>
    
    <atom:link href="/categories/bigdata/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark开发</title>
      <link>/2018/01/spark%E5%BC%80%E5%8F%91/</link>
      <pubDate>Thu, 11 Jan 2018 23:10:50 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2018/01/spark%E5%BC%80%E5%8F%91/</guid>
      <description>&lt;h2 id=&#34;使用scala进行开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8scala%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91&#34;&gt;
        #
    &lt;/a&gt;
    使用scala进行开发
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;step1-安装sbt&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step1-%e5%ae%89%e8%a3%85sbt&#34;&gt;
        ##
    &lt;/a&gt;
    Step1: 安装sbt
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
$ curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
$ sudo yum install sbt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# cat ~/.sbt/repositories
[repositories]
  # 本地源
  local
  # 阿里源
  aliyun: http://maven.aliyun.com/nexus/content/groups/public/
  typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly
  sonatype-oss-releases
  maven-central
  sonatype-oss-snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step2-创建项目&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step2-%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae&#34;&gt;
        ##
    &lt;/a&gt;
    Step2: 创建项目
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;可以使用Giter8模版创建项目。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ sbt new imarios/frameless.g8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;spark相关的几个模版：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;holdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project).&lt;/li&gt;
&lt;li&gt;imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark)&lt;/li&gt;
&lt;li&gt;nttdata-oss/basic-spark-project.g8 (Spark basic project.)&lt;/li&gt;
&lt;li&gt;spark-jobserver/spark-jobserver.g8 (Template for Spark Jobserver)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step3-编写代码&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step3-%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81&#34;&gt;
        ##
    &lt;/a&gt;
    Step3: 编写代码
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;创建出的项目目录中包含一下主要条目：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ ls
build.sbt  project  src  target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编写的代码放在 &amp;lsquo;src/main/scala/&amp;rsquo; 目录中：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ ls src/main/scala/
SimpleApp.scala

$ cat src/main/scala/SimpleApp.scala
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = &#34;people.txt&#34; // Should be some file on your system
    val spark = SparkSession.builder.appName(&#34;Simple Application&#34;).getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line =&gt; line.contains(&#34;a&#34;)).count()
    val numBs = logData.filter(line =&gt; line.contains(&#34;b&#34;)).count()
    println(s&#34;Lines with a: $numAs, Lines with b: $numBs&#34;)
    spark.stop()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step4-编译打包&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step4-%e7%bc%96%e8%af%91%e6%89%93%e5%8c%85&#34;&gt;
        ##
    &lt;/a&gt;
    Step4: 编译打包
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;程序开发好之后首先需要编译打包：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ sbt package
[info] Loading project definition from /home/spark/scala/frameless/project
[info] Updating {file:/home/spark/scala/frameless/project/}frameless-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Set current project to frameless (in build file:/home/spark/scala/frameless/)
[info] Updating {file:/home/spark/scala/frameless/}root...
[info] Resolving org.sonatype.oss#oss-parent;9 ...
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.11/scala-library-2.11.11.jar ...
[info]  [SUCCESSFUL ] org.scala-lang#scala-library;2.11.11!scala-library.jar (94788ms)

...

[info] downloading https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar ...
[info]  [SUCCESSFUL ] jline#jline;2.14.3!jline.jar (4836ms)
[info] Done updating.
[info] Compiling 1 Scala source to /home/spark/scala/frameless/target/scala-2.11/classes...
[info] &#39;compiler-interface&#39; not yet compiled for Scala 2.11.11. Compiling...
[info]   Compilation completed in 24.981 s
[info] Packaging /home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar ...
[info] Done packaging.
[success] Total time: 2795 s, completed Jan 16, 2018 3:03:12 PM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编译打包成功后的文件输出在 &amp;rsquo;target/scala-2.11/&amp;rsquo; 目录中：&lt;/p&gt;
&lt;p&gt;/home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar&lt;/p&gt;
&lt;h3 id=&#34;step5-部署运行&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#step5-%e9%83%a8%e7%bd%b2%e8%bf%90%e8%a1%8c&#34;&gt;
        ##
    &lt;/a&gt;
    Step5: 部署运行
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
# su - spark
$ export SPARK_MAJOR_VERSION=2

$ spark-submit --class SimpleApp --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 target/scala-2.11/frameless_2.11-0.1.jar
SPARK_MAJOR_VERSION is set to 2, using Spark2
Warning: Master yarn-client is deprecated since 2.0. Please use master &#34;yarn&#34; with specified deploy mode instead.
18/01/17 15:20:18 INFO SparkContext: Running Spark version 2.2.0.2.6.3.0-235
18/01/17 15:20:19 INFO SparkContext: Submitted application: Simple Application
18/01/17 15:20:19 INFO SecurityManager: Changing view acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls to: spark
18/01/17 15:20:19 INFO SecurityManager: Changing view acls groups to:
18/01/17 15:20:19 INFO SecurityManager: Changing modify acls groups to:
18/01/17 15:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view
 permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
18/01/17 15:20:20 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 43573.

...

18/01/17 15:21:30 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
18/01/17 15:21:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, bdtest-002, executor 1, partition 0, NODE_LOCAL, 4737 bytes)
18/01/17 15:21:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bdtest-002:44065 (size: 3.7 KB, free: 93.2 MB)
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.4:44312
18/01/17 15:21:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 141 bytes
18/01/17 15:21:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 96 ms on bdtest-002 (executor 1) (1/1)
18/01/17 15:21:30 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool
18/01/17 15:21:30 INFO DAGScheduler: ResultStage 3 (count at SimpleApp.scala:10) finished in 0.096 s
18/01/17 15:21:30 INFO DAGScheduler: Job 1 finished: count at SimpleApp.scala:10, took 0.289165 s
Lines with a: 1, Lines with b: 0
18/01/17 15:21:30 INFO AbstractConnector: Stopped Spark@17b03218{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
18/01/17 15:21:30 INFO SparkUI: Stopped Spark web UI at http://192.168.1.5:4042
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Shutting down all executors
18/01/17 15:21:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
18/01/17 15:21:30 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
18/01/17 15:21:30 INFO YarnClientSchedulerBackend: Stopped
18/01/17 15:21:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/01/17 15:21:31 INFO MemoryStore: MemoryStore cleared
18/01/17 15:21:31 INFO BlockManager: BlockManager stopped
18/01/17 15:21:31 INFO BlockManagerMaster: BlockManagerMaster stopped
18/01/17 15:21:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/01/17 15:21:31 INFO SparkContext: Successfully stopped SparkContext
18/01/17 15:21:31 INFO ShutdownHookManager: Shutdown hook called
18/01/17 15:21:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-730219ef-2995-4225-8743-5769fa6269db
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ndash;master: 指定spark driver的运行模式。
&lt;ul&gt;
&lt;li&gt;yarn-cluster: spark driver运行在yarn的application master进程中。如果应用只是进行计算可以使用这种方式运行，如果需要跟前台进行交互则可以考虑yarn-client模式。&lt;/li&gt;
&lt;li&gt;yarn-client: spark driver运行在client进程中，此时application master只负责向yarn申请资源。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使用python进行开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8python%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91&#34;&gt;
        #
    &lt;/a&gt;
    使用python进行开发
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;使用hdp环境提交任务&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8hdp%e7%8e%af%e5%a2%83%e6%8f%90%e4%ba%a4%e4%bb%bb%e5%8a%a1&#34;&gt;
        ##
    &lt;/a&gt;
    使用HDP环境提交任务
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;创建python文件&lt;code&gt;pi.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import sys
from random import random
from operator import add

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&amp;quot;PythonPi&amp;quot;)\.getOrCreate()

partitions = int(sys.argv[1]) if len(sys.argv) &amp;gt; 1 else 2
n = 100000 * partitions

def f(any):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &amp;lt;= 1 else 0

count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
print(&amp;quot;Pi is roughly %f&amp;quot; % (4.0 * count / n))

spark.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行&lt;code&gt;pi.py&lt;/code&gt;，这里指定使用yarn的cluster模式&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPARK_MAJOR_VERSION=2 spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --deploy pi.py 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;在hdp环境运行jupyter-server&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%9c%a8hdp%e7%8e%af%e5%a2%83%e8%bf%90%e8%a1%8cjupyter-server&#34;&gt;
        ##
    &lt;/a&gt;
    在HDP环境运行jupyter server
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;使用yarn的cluster模式运行，在运行期间会始终在yarn中占用资源，如果需要长期运行，使用&lt;code&gt;--master local&lt;/code&gt;比较好&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;XDG_RUNTIME_DIR=&amp;quot;&amp;quot; SPARK_MAJOR_VERSION=2 PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=&#39;notebook --no-browser --port 8888 --ip 121.43.171.231&#39; pyspark --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;为jupyter server设置密码等操作参考&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/stable/public_server.html&#34;&gt;jupyter文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;运行后访问服务器8888端口即可访问jupyter服务&lt;/p&gt;
&lt;h3 id=&#34;非hdp环境开发&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%9d%9ehdp%e7%8e%af%e5%a2%83%e5%bc%80%e5%8f%91&#34;&gt;
        ##
    &lt;/a&gt;
    非HDP环境开发
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;确保本机有一份spark二进制文件，比如&lt;code&gt;spark-2.2.1-bin-hadoop2.7&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;安装pyspark&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install pyspark
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;设置SPARK_HOME（可选）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export SPARK_HOME=/home/xxx/spark-2.2.1-bin-hadoop2.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过代码提交任务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
from pyspark import SparkConf, SparkContext

# if `SPARK_HOME` is undefined yet
if &#39;SPARK_HOME&#39; not in os.environ:
    os.environ[&#39;SPARK_HOME&#39;] = &#39;/home/xxx/spark-2.2.1-bin-hadoop2.7&#39;

conf = SparkConf().setAppName(&#39;Demo&#39;).setMaster(&#39;yarn&#39;).set(&#39;spark.yarn.deploy.mode&#39;, &#39;cluster&#39;)
sc = SparkContext(conf=conf)

# Do something with sc...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或者使用&lt;code&gt;SparkSession&lt;/code&gt; API&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from spark.sql import SparkSession
spark = (SparkSession.builder
          .master(&amp;quot;yarn&amp;quot;)
          .appName(&amp;quot;Demo&amp;quot;)
          .config(&amp;quot;spark.yarn.deploy.mode&amp;quot;, &amp;quot;cluster&amp;quot;)
          .getOrCreate())

# Do something with spark...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;oozie自动化&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#oozie%e8%87%aa%e5%8a%a8%e5%8c%96&#34;&gt;
        ##
    &lt;/a&gt;
    oozie自动化
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h1 id=&#34;development&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#development&#34;&gt;
        ##
    &lt;/a&gt;
    Development
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;Spark 2.0之前的开发接口主要是RDD（Resilient Distributed Dataset），从2.0之后原有&lt;a href=&#34;https://spark.apache.org/docs/latest/rdd-programming-guide.html&#34;&gt;RDD接口&lt;/a&gt;仍然支持，但RDD被Dataset取代，如果使用Dataset编程的话需要使用&lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark SQL&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;api&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#api&#34;&gt;
        #
    &lt;/a&gt;
    API
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/1.3.1/api/scala/index.html&#34;&gt;http://spark.apache.org/docs/1.3.1/api/scala/index.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;sparksession-vs-sparkcontext&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparksession-vs-sparkcontext&#34;&gt;
        #
    &lt;/a&gt;
    SparkSession vs SparkContext
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Spark 2.0之前有3个主要的连接对象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SparkContext: 建立与Spark执行环境相关的连接，用于创建RDD&lt;/li&gt;
&lt;li&gt;SqlContext：利用SparkContext背后的SparkSQL建立连接&lt;/li&gt;
&lt;li&gt;HiveContext：创建访问hive的接口&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从Spark 2.0开始，Datasets/Dataframes成为主要的数据访问接口，SparkSession(org.apache.spark.sql.SparkSession)成为主要的访问Spark执行环境的接口。&lt;/p&gt;
&lt;h3 id=&#34;sparkconf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparkconf&#34;&gt;
        ##
    &lt;/a&gt;
    SparkConf
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Spark 2.0之前需要先创建SparkConf，在创建SparkContext。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//set up the spark configuration and create contexts
val sparkConf = new SparkConf().setAppName(&#34;SparkSessionZipsExample&#34;).setMaster(&#34;local&#34;)
// your handle to SparkContext to access other context like SQLContext
val sc = new SparkContext(sparkConf).set(&#34;spark.some.config.option&#34;, &#34;some-value&#34;)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark 2.0之后可以不用显示的创建SparkConf对象就可以创建SparkSession对象。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
// Create a SparkSession. No need to create SparkContext
// You automatically get it as part of the SparkSession
val warehouseLocation = &#34;file:${system:user.dir}/spark-warehouse&#34;
val spark = SparkSession
   .builder()
   .appName(&#34;SparkSessionZipsExample&#34;)
   .config(&#34;spark.sql.warehouse.dir&#34;, warehouseLocation)
   .enableHiveSupport()
   .getOrCreate()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在SparkSession创建完后，Spark的运行时配置属性还可以通过SparkSession.conf()被修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//set new runtime options
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, 6)
spark.conf.set(&#34;spark.executor.memory&#34;, &#34;2g&#34;)
//get all settings
val configMap:Map[String, String] = spark.conf.getAll()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.javachen.com/2015/06/07/spark-configuration.html&#34;&gt;Spark conf参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sparksql&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sparksql&#34;&gt;
        ##
    &lt;/a&gt;
    SparkSQL
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;2.0之前需要通过创建SqlContext来使用SparkSQL。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
val sc = new SparkContext(sparkConf).set(&#34;spark.some.config.option&#34;, &#34;some-value&#34;)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.0后通过SparkSession可以很方便的访问SparkSession.sql()来使用SparkSQL。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
// Now create an SQL table and issue SQL queries against it without
// using the sqlContext but through the SparkSession object.
// Creates a temporary view of the DataFrame
zipsDF.createOrReplaceTempView(&#34;zips_table&#34;)
zipsDF.cache()
val resultsDF = spark.sql(&#34;SELECT city, pop, state, zip FROM zips_table&#34;)
resultsDF.show(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;访问catalog-metadata&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e8%ae%bf%e9%97%aecatalog-metadata&#34;&gt;
        ##
    &lt;/a&gt;
    访问Catalog Metadata
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;SparkSession暴露了访问metastore的接口SparkSession.catalog，这个接口可以用来访问catalog metadata信息，比如（Hcatalog）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//fetch metadata data from the catalog
spark.catalog.listDatabases.show(false)
spark.catalog.listTables.show(false)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
scala&gt; spark.catalog.listDatabases.show(false)
+-------+---------------------+------------------------------------------+
|name   |description          |locationUri                               |
+-------+---------------------+------------------------------------------+
|default|Default Hive database|hdfs://bdtest-001:8020/apps/hive/warehouse|
+-------+---------------------+------------------------------------------+

scala&gt; spark.catalog.listTables.show(false)
+-------------------------+--------+----------------------------------------+---------+-----------+
|name                     |database|description                             |tableType|isTemporary|
+-------------------------+--------+----------------------------------------+---------+-----------+
|activity_donate_record   |default |Imported by sqoop on 2018/01/08 19:03:33|MANAGED  |false      |
|activity_periods_config  |default |Imported by sqoop on 2018/01/08 19:04:20|MANAGED  |false      |
|activity_result          |default |Imported by sqoop on 2018/01/08 19:05:00|MANAGED  |false      |
|activity_school_info     |default |Imported by sqoop on 2018/01/08 19:05:46|MANAGED  |false      |
|activity_season_game_data|default |Imported by sqoop on 2018/01/08 19:06:23|MANAGED  |false      |
|activity_season_game_log |default |Imported by sqoop on 2018/01/08 19:07:00|MANAGED  |false      |
|ana_tag                  |default |Imported by sqoop on 2018/01/08 19:07:35|MANAGED  |false      |
|ana_user_tag             |default |Imported by sqoop on 2018/01/08 19:08:18|MANAGED  |false      |
|api_invoking_log         |default |Imported by sqoop on 2018/01/08 19:08:58|MANAGED  |false      |
|app_function             |default |Imported by sqoop on 2018/01/08 19:09:34|MANAGED  |false      |
|bank_card                |default |Imported by sqoop on 2018/01/08 19:10:09|MANAGED  |false      |
|bank_card_bin            |default |Imported by sqoop on 2018/01/08 19:10:46|MANAGED  |false      |
|command_invocation       |default |Imported by sqoop on 2018/01/08 19:11:26|MANAGED  |false      |
|coupon_grant_log         |default |Imported by sqoop on 2018/01/08 19:12:05|MANAGED  |false      |
|coupon_user              |default |Imported by sqoop on 2018/01/08 19:12:43|MANAGED  |false      |
|device_command           |default |Imported by sqoop on 2018/01/08 19:13:18|MANAGED  |false      |
|device_preorder          |default |Imported by sqoop on 2018/01/08 19:13:54|MANAGED  |false      |
|dro_dropdetails          |default |null                                    |MANAGED  |false      |
|dro_dropinfo             |default |Imported by sqoop on 2018/01/08 19:15:09|MANAGED  |false      |
|dro_exchange_record      |default |Imported by sqoop on 2018/01/08 19:15:43|MANAGED  |false      |
+-------------------------+--------+----------------------------------------+---------+-----------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dataframe-和-datasets&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataframe-%e5%92%8c-datasets&#34;&gt;
        #
    &lt;/a&gt;
    DataFrame 和 Datasets
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png&#34; alt=&#34;DataFrame &amp;amp; Dataset&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataset&#34;&gt;
        ##
    &lt;/a&gt;
    Dataset：
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;强类型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持functional和relational操作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;操作分类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;转换（transformation）：用于产生新的Datasets
&lt;ul&gt;
&lt;li&gt;例如：map，filter，select，aggregate（groupBy）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;操作（actions）：触发计算并返回结果
&lt;ul&gt;
&lt;li&gt;例如：count，show，把数据写回文件系统&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lazy：计算只有到action触发的时候才进行。Dataset内部可以理解为存储了如何进行计算的计划。当action执行的时候Spark query optimizer生成并优化计算方案后将其执行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder&#34;&gt;Encoder&lt;/a&gt;：把JVM中的类型T跟Spark SQL的数据表现形式进行转换的机制。（marshal/unmarshal?）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;两种创建方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用SparkSession的read()方法&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;
  val people = spark.read.parquet(&#34;...&#34;).as[Person]  // Scala
  Dataset&lt;Person&gt; people = spark.read().parquet(&#34;...&#34;).as(Encoders.bean(Person.class)); // Java
  &lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对现有Dataset做转换（transformation）&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;
  val names = people.map(_.name)  // in Scala; names is a Dataset[String]
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dataset&lt;String&gt; names = people.map((Person p) -&amp;gt; p.name, Encoders.STRING));
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dataset操作也可以是untyped，通过：&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&#34;&gt;Dataset&lt;/a&gt;, &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column&#34;&gt;Column&lt;/a&gt;, &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$&#34;&gt;DataFrame的function&lt;/a&gt;等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列操作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择一列（抛弃其他列）：select(&amp;ldquo;column&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;增加一列：withColumn(&amp;ldquo;newColumn&amp;rdquo;, Column)&lt;/li&gt;
&lt;li&gt;修改一列: withColumn(&amp;ldquo;column&amp;rdquo;, Column)&lt;/li&gt;
&lt;li&gt;删除一列: drop(&amp;ldquo;column&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;类型转换：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列高级操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UDF:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataframe&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dataframe&#34;&gt;
        ##
    &lt;/a&gt;
    DataFrame：
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset的无类型view (untyped view)，对应Dataset的Row&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$&#34;&gt;DataFrame操作Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataFrame是有名列（named columns）组成的数据集合，类似关系数据库中的表或者python/R中的pandas。&lt;/p&gt;
&lt;p&gt;可以由多种数据源来构造DataFrame，比如Hive中的table，Spark的RDD（Resilient Distributed Datasets）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Datasets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SparkSession有很多种方法创建DataFrame和Datasets。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;DataFrame&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;创建API&lt;/td&gt;
&lt;td&gt;SparkSession.createDataFrame&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;其他&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Datasets&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;创建API&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;其他&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow&#34;&gt;
        ##
    &lt;/a&gt;
    &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row&#34;&gt;Row&lt;/a&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;创建Row&lt;/li&gt;
&lt;li&gt;访问
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;generic访问，使用ordinal序列访问&lt;/p&gt;
&lt;p&gt;row(0)：访问列第一个成员&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原始类型访问&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn&#34;&gt;
        ##
    &lt;/a&gt;
    &lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column&#34;&gt;Column&lt;/a&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DataSet API:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&#34;&gt;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;处理时间&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%a4%84%e7%90%86%e6%97%b6%e9%97%b4&#34;&gt;
        ##
    &lt;/a&gt;
    处理时间
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;-ver--15&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#-ver--15&#34;&gt;
        ###
    &lt;/a&gt;
    * ver &amp;lt;= 1.5
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;使用hiveContext来查询&lt;/p&gt;
&lt;h4 id=&#34;--15--ver--22&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#--15--ver--22&#34;&gt;
        ###
    &lt;/a&gt;
    *  1.5 &amp;lt;= ver &amp;lt; 2.2
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Spark 1.5引入了unix_timestamp()，所以在1.5之后到2.2的spark版本中可以这样操作时间信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
import org.apache.spark.sql.functions.{unix_timestamp, to_date}

val df = Seq((1L, &#34;01-APR-2015&#34;)).toDF(&#34;id&#34;, &#34;ts&#34;)

df.select(to_date(unix_timestamp(
  $&#34;ts&#34;, &#34;dd-MMM-yyyy&#34;
).cast(&#34;timestamp&#34;)).alias(&#34;timestamp&#34;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;-ver--22&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#-ver--22&#34;&gt;
        ###
    &lt;/a&gt;
    * ver &amp;gt;= 2.2
&lt;/div&gt;
&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;
import org.apache.spark.sql.functions.{to_date, to_timestamp}

df.select(to_date($&#34;ts&#34;, &#34;dd-MMM-yyyy&#34;).alias(&#34;date&#34;))

df.select(to_timestamp($&#34;ts&#34;, &#34;dd-MM-yyyy HH:mm:ss&#34;).alias(&#34;timestamp&#34;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;import-sparkimplicits_&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#import-sparkimplicits_&#34;&gt;
        ##
    &lt;/a&gt;
    import spark.implicits._
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;引入toDF()方法和“$”操作符&lt;/p&gt;
&lt;h2 id=&#34;编程陷阱&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e7%bc%96%e7%a8%8b%e9%99%b7%e9%98%b1&#34;&gt;
        #
    &lt;/a&gt;
    编程陷阱
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%99%b7%e9%98%b11dataframe%e6%9c%89%e5%85%83%e7%b4%a0%e4%bd%86%e9%80%9a%e8%bf%87%e5%be%aa%e7%8e%af%e6%8a%8a%e5%80%bc%e6%94%be%e5%85%a5listbuffer%e5%90%8elistbuffer%e5%8d%b4%e4%b8%ba%e7%a9%ba%e5%88%97%e8%a1%a8&#34;&gt;
        ##
    &lt;/a&gt;
    陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表
&lt;/div&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
   val valueDF = allData.select(&#34;value&#34;).distinct()
    valueDF.show()
    //valueDF.foreach(r =&gt; valueList.append(r.getAs[Int](&#34;value&#34;)))

    for (row &lt;- valueDF) {
      println(&#34;value:&#34; + row.getAs[Int](&#34;value&#34;))
      valueList.append(row.getAs[Int](&#34;value&#34;))
    }
    println(&#34;Values:&#34; + valueList)
    valueList.foreach(println))

// 程序运行输出

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

Values:ListBuffer()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看出两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题1: 虽然可以知道valueDF不为空，可是&amp;quot;println(&amp;hellip;)&amp;ldquo;却没有任何输出内容&lt;/li&gt;
&lt;li&gt;问题2: 结果valueList为空&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;原因&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%8e%9f%e5%9b%a0&#34;&gt;
        ###
    &lt;/a&gt;
    原因：
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Spark任务执行是通过Driver/Executor来完成的，Driver控制任务执行，Executor负责任务执行，foreach/for会被Driver分配给Executor来执行（分布式执行），每个Executor创建一个自己的ListBuffer拷贝，因此当任务结束后信息就丢失了。&lt;/p&gt;
&lt;p&gt;参考Stackoverflow的讨论：&lt;a href=&#34;https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop&#34;&gt;https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;修复方法使用collect&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e4%bf%ae%e5%a4%8d%e6%96%b9%e6%b3%95%e4%bd%bf%e7%94%a8collect&#34;&gt;
        ###
    &lt;/a&gt;
    修复方法：使用collect()
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;collect()定义：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
def collect(): Array[T]
Returns an array that contains all rows in this Dataset.
Running collect requires moving all the data into the application&#39;s driver
process, and doing so on a very large dataset can crash the driver process with
OutOfMemoryError.
For Java API, use collectAsList.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修复代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
val valueDF = allData.select(&#34;value&#34;).distinct()
valueDF.show()
//valueDF.foreach(r =&gt; valueList.append(r.getAs[Int](&#34;value&#34;)))

for (row &lt;- valueDF.collect()) {
  println(&#34;value:&#34; + row.getAs[Int](&#34;value&#34;))
  valueList.append(row.getAs[Int](&#34;value&#34;))
}
println(&#34;getAllChannels:&#34; + valueList)


// 程序运行输出如下

+-------+
|value  |
+-------+
|      1|
|      3|
|      5|
|      4|
|     10|
|      2|
|      0|
+-------+

value:1
value:3
value:5
value:4
value:10
value:2
value:0
getAllChannels:ListBuffer(1, 3, 5, 4, 10, 2, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        ##
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://community.hortonworks.com/articles/84071/apache-ambari-workflow-manager-view-for-apache-ooz-2.html&#34;&gt;Ambari workflow manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.scala-lang.org/tour/basics.html&#34;&gt;Scala basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/foundweekends/giter8/wiki/giter8-templates&#34;&gt;Giter8项目模版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_spark-component-guide/content/ch_oozie-spark-action.html&#34;&gt;Automating Spark job with Oozie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://community.hortonworks.com/articles/104949/using-virtualenv-with-pyspark-1.html&#34;&gt;Using VirtualEnv with PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hive Intro</title>
      <link>/2018/01/hive-intro/</link>
      <pubDate>Thu, 11 Jan 2018 22:25:07 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2018/01/hive-intro/</guid>
      <description>&lt;h1 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        ##
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h1&gt;
&lt;h1 id=&#34;-hive-documenthttpscwikiapacheorgconfluencedisplayhivehome&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#-hive-documenthttpscwikiapacheorgconfluencedisplayhivehome&#34;&gt;
        ##
    &lt;/a&gt;
    * &lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/Home&#34;&gt;Hive document&lt;/a&gt;
&lt;/div&gt;
&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Hcatalog简介</title>
      <link>/2018/01/hcatalog%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Thu, 11 Jan 2018 19:51:16 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2018/01/hcatalog%E7%AE%80%E4%BB%8B/</guid>
      <description>&lt;p&gt;HCatalog是Hadoop生态链中的一个有趣的组件。HCatalog构建于Hive的&lt;a href=&#34;http://hadooptutorial.info/hive-metastore-configuration/&#34;&gt;metastore&lt;/a&gt;之上并结合了Hive的DDL，通过服务的形式开放给Hadoop生态链中的其他组件，这样即可用一种统一的形式将Hive数据仓库中的数据的metadata开放给需要的服务，这样的话需要的服务就可以通过HCatalog来了解到所使用的数据的内容以及格式等等元信息。&lt;/p&gt;
&lt;p&gt;下图展示了HCatalog在Hadoop生态系统中的定位：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.stack.imgur.com/eAn8r.png&#34; alt=&#34;Architecture-001&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看出HCatalog内置可以支持多种数据格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ORC&lt;/li&gt;
&lt;li&gt;RC&lt;/li&gt;
&lt;li&gt;Text&lt;/li&gt;
&lt;li&gt;SequenceFile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外用户还可以自定义格式，不过需要编写InputFormat, OutputFormat, SerDe(Serializer/Deserializer):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.tutorialspoint.com/hcatalog/images/architecture.jpg&#34; alt=&#34;Architecture-002&#34;&gt;&lt;/p&gt;
&lt;p&gt;HCatalog提供了&amp;rsquo;hcat&#39;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ hcat
usage: hcat { -e &#34;&lt;query&gt;&#34; | -f &#34;&lt;filepath&gt;&#34; } [ -g &#34;&lt;group&gt;&#34; ] [ -p &#34;&lt;perms&gt;&#34; ] [ -D&#34;&lt;name&gt;=&lt;value&gt;&#34; ]
 -D &lt;property=value&gt;   use hadoop value for given property
 -e &lt;exec&gt;             hcat command given from command line
 -f &lt;file&gt;             hcat commands in file
 -g &lt;group&gt;            group for the db/table specified in CREATE statement
 -h,--help             Print help information
 -p &lt;perms&gt;            permissions for the db/table specified in CREATE statement
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;参数**-e**提供了使用Hive &amp;lsquo;DDL&amp;rsquo;命令的接口&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;DDL命令&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CREATE TABLE&lt;/td&gt;
&lt;td&gt;建表操作，注意如果建表时使用了“CLUSTERED BY”那么这个表不能被Pig和MapReduce使用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALTER TABLE&lt;/td&gt;
&lt;td&gt;修改表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SHOW TABLES&lt;/td&gt;
&lt;td&gt;查询表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DROP TABLE&lt;/td&gt;
&lt;td&gt;删除表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CREATE/ALTER/DROP VIEW&lt;/td&gt;
&lt;td&gt;管理view&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SHOW PARTITIONS&lt;/td&gt;
&lt;td&gt;查询分区表的分区信息&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Create/Drop Index&lt;/td&gt;
&lt;td&gt;管理index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DESCRIBE&lt;/td&gt;
&lt;td&gt;查询表结构&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;例子：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$ hcat -e  &#34;show tables;&#34;
OK
activity
device
...
Time taken: 3.255 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
# hcat -e &#34;describe device;&#34;
OK
id               bigint
uuid             string
time             string
type             int
address          string
Time taken: 3.824 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;APIs&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;API&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;HCatReader&lt;/td&gt;
&lt;td&gt;从hdfs中读取数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HCatWriter&lt;/td&gt;
&lt;td&gt;向hdfs中写入数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DataTransferFactory&lt;/td&gt;
&lt;td&gt;创建HCatReader/HCatWriter实例&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HCatInputFormat&lt;/td&gt;
&lt;td&gt;利用MapReduce job从表结构由HCatalog管理的表中读取并处理数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HCatOutputFormat&lt;/td&gt;
&lt;td&gt;利用MapReduce job处理数据并向表结构由HCatalog管理的表中写入数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HCatLoader&lt;/td&gt;
&lt;td&gt;Pig script用来读取表结构由HCatalog管理的数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HCatStorer&lt;/td&gt;
&lt;td&gt;Pig script用来写入表结构由HCatalog管理的数据&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;参考信息&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%8f%82%e8%80%83%e4%bf%a1%e6%81%af&#34;&gt;
        ##
    &lt;/a&gt;
    参考信息
&lt;/div&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/hcatalog/hcatalog_quick_guide.htm&#34;&gt;HCataLog quick guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://hadooptutorial.info/hive-metastore-configuration/&#34;&gt;Hive metastore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Yarn Config</title>
      <link>/2017/12/yarn-config/</link>
      <pubDate>Fri, 29 Dec 2017 11:44:51 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2017/12/yarn-config/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/yarn-config/ResourceManagement.jpg&#34; alt=&#34;Yarn config&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Resource manager&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Node manager&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Application master&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Container&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;container所需资源，minimum/maximum memory/vcores&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scheduler&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FIFO&lt;/li&gt;
&lt;li&gt;CapacityScheduler&lt;/li&gt;
&lt;li&gt;FairScheduler&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Queue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Application &amp;mdash; scheduler &amp;mdash;&amp;gt; Queue&lt;/p&gt;
&lt;p&gt;yarn-site.xml中配置。&lt;/p&gt;
&lt;p&gt;下文引用自&lt;a href=&#34;http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/&#34;&gt;http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;node-manager&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#node-manager&#34;&gt;
        #
    &lt;/a&gt;
    Node manager
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Node manager architecture&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2012/09/Node-Manager-Diagram-Small.png&#34; alt=&#34;Node manager&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;resourcemanager相关配置参数&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#resourcemanager%e7%9b%b8%e5%85%b3%e9%85%8d%e7%bd%ae%e5%8f%82%e6%95%b0&#34;&gt;
        #
    &lt;/a&gt;
    ResourceManager相关配置参数
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;（1） yarn.resourcemanager.address&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：ResourceManager 对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等。&lt;/p&gt;
&lt;p&gt;默认值：${yarn.resourcemanager.hostname}:8032&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（2） yarn.resourcemanager.scheduler.address&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：ResourceManager 对ApplicationMaster暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等。&lt;/p&gt;
&lt;p&gt;默认值：${yarn.resourcemanager.hostname}:8030&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（3） yarn.resourcemanager.resource-tracker.address&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：ResourceManager 对NodeManager暴露的地址.。NodeManager通过该地址向RM汇报心跳，领取任务等。
默认值：${yarn.resourcemanager.hostname}:8031&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（4） yarn.resourcemanager.admin.address&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：ResourceManager 对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等。&lt;/p&gt;
&lt;p&gt;默认值：${yarn.resourcemanager.hostname}:8033&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（5） yarn.resourcemanager.webapp.address&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息。&lt;/p&gt;
&lt;p&gt;默认值：${yarn.resourcemanager.hostname}:8088&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（6） yarn.resourcemanager.scheduler.class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：启用的资源调度器主类。目前可用的有FIFO、Capacity Scheduler和Fair Scheduler。&lt;/p&gt;
&lt;p&gt;默认值：
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（7） yarn.resourcemanager.resource-tracker.client.thread-count&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：处理来自NodeManager的RPC请求的Handler数目。&lt;/p&gt;
&lt;p&gt;默认值：50&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（8） yarn.resourcemanager.scheduler.client.thread-count&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：处理来自ApplicationMaster的RPC请求的Handler数目。&lt;/p&gt;
&lt;p&gt;默认值：50&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（9） yarn.scheduler.minimum-allocation-mb/ yarn.scheduler.maximum-allocation-mb&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。&lt;/p&gt;
&lt;p&gt;默认值：1024/8192&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（10） yarn.scheduler.minimum-allocation-vcores / yarn.scheduler.maximum-allocation-vcores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：单个可申请的最小/最大虚拟CPU个数。比如设置为1和4，则运行MapRedce作业时，每个Task最少可申请1个虚拟CPU，最多可申请4个虚拟CPU。什么是虚拟CPU，可阅读我的这篇文章：“YARN 资源调度器剖析”。&lt;/p&gt;
&lt;p&gt;默认值：1/32&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（11） yarn.resourcemanager.nodes.include-path /yarn.resourcemanager.nodes.exclude-path&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager黑白名单。如果发现若干个NodeManager存在问题，比如故障率很高，任务运行失败率高，则可以将之加入黑名单中。注意，这两个配置参数可以动态生效。（调用一个refresh命令即可）&lt;/p&gt;
&lt;p&gt;默认值：“”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（12） yarn.resourcemanager.nodemanagers.heartbeat-interval-ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager心跳间隔&lt;/p&gt;
&lt;p&gt;默认值：1000（毫秒）&lt;/p&gt;
&lt;h2 id=&#34;nodemanager相关配置参数&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#nodemanager%e7%9b%b8%e5%85%b3%e9%85%8d%e7%bd%ae%e5%8f%82%e6%95%b0&#34;&gt;
        #
    &lt;/a&gt;
    NodeManager相关配置参数
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;（1） yarn.nodemanager.resource.memory-mb&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager总的可用物理内存。注意，该参数是不可修改的，一旦设置，整个运行过程中不可动态修改。另外，该参数的默认值是8192MB，即使你的机器内存不够8192MB，YARN也会按照这些内存来使用（傻不傻？），因此，这个值通过一定要配置。不过，Apache已经正在尝试将该参数做成可动态修改的。&lt;/p&gt;
&lt;p&gt;默认值：8192&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（2） yarn.nodemanager.vmem-pmem-ratio&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：每使用1MB物理内存，最多可用的虚拟内存数。&lt;/p&gt;
&lt;p&gt;默认值：2.1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（3） yarn.nodemanager.resource.cpu-vcores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager总的可用虚拟CPU个数。&lt;/p&gt;
&lt;p&gt;默认值：8&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（4） yarn.nodemanager.local-dirs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：中间结果存放位置，类似于1.0中的mapred.local.dir。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。&lt;/p&gt;
&lt;p&gt;默认值：${hadoop.tmp.dir}/nm-local-dir&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（5） yarn.nodemanager.log-dirs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：日志存放地址（可配置多个目录）。&lt;/p&gt;
&lt;p&gt;默认值：${yarn.log.dir}/userlogs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（6） yarn.nodemanager.log.retain-seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager上日志最多存放时间（不启用日志聚集功能时有效）。&lt;/p&gt;
&lt;p&gt;默认值：10800（3小时）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（7） yarn.nodemanager.aux-services&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参数解释：NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序&lt;/p&gt;
&lt;p&gt;默认值：“”&lt;/p&gt;
&lt;p&gt;yarn-site.xml
yarn.scheduler.minimum-allocation-mb:341
yarn.scheduler.increment-allocation-mb:
yarn.scheduler.maximum-allocation-mb:1023&lt;/p&gt;
&lt;p&gt;节点分配给NodeManager的可用内存
yarn.nodemanager.resource.memory-mb:1023&lt;/p&gt;
&lt;p&gt;yarn.scheduler.minimum-allocation-mb &amp;lt; yarn.nodemanager.resource.memory-mb
yarn.scheduler.maximum-allocation-mb:1023 &amp;lt; yarn.nodemanager.resource.memory-mb&lt;/p&gt;
&lt;h2 id=&#34;mapred-sitexml&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#mapred-sitexml&#34;&gt;
        #
    &lt;/a&gt;
    mapred-site.xml
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;yarn.app.mapreduce.am.resource.mb： 341
mapreduce.map.memory.mb：341
mapreduce.reduce.memory.mb：682
mapreduce.task.io.sort.mb：190&lt;/p&gt;
&lt;p&gt;mapreduce.map.java.opts: -Xmx272m
mapreduce.reduce.java.opts: -Xmx545m&lt;/p&gt;
&lt;p&gt;Resource manager: bdtest-002&lt;/p&gt;
&lt;p&gt;Application manager:&lt;/p&gt;
&lt;p&gt;Node manager:&lt;/p&gt;
&lt;h1 id=&#34;yarn操作&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#yarn%e6%93%8d%e4%bd%9c&#34;&gt;
        ##
    &lt;/a&gt;
    Yarn操作
&lt;/div&gt;
&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;查看application log&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  $ yarn logs -applicationId applicationID
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;控制application&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  yarn application -list
  yarn application -status
  yarn application -kill
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yarn参考文章&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#yarn%e5%8f%82%e8%80%83%e6%96%87%e7%ab%a0&#34;&gt;
        #
    &lt;/a&gt;
    Yarn参考文章
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/suifeng3051/article/details/49508261&#34;&gt;Yarn scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml&#34;&gt;Yarn configure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sqoop介绍及使用</title>
      <link>/2017/12/sqoop%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 25 Dec 2017 18:11:29 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2017/12/sqoop%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid>
      <description>&lt;h1 id=&#34;1-hdp中使用sqoop进行工作流处理准备工作&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#1-hdp%e4%b8%ad%e4%bd%bf%e7%94%a8sqoop%e8%bf%9b%e8%a1%8c%e5%b7%a5%e4%bd%9c%e6%b5%81%e5%a4%84%e7%90%86%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c&#34;&gt;
        ##
    &lt;/a&gt;
    1. HDP中使用sqoop进行工作流处理准备工作
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;11-准备hdfs中的用户目录&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#11-%e5%87%86%e5%a4%87hdfs%e4%b8%ad%e7%9a%84%e7%94%a8%e6%88%b7%e7%9b%ae%e5%bd%95&#34;&gt;
        #
    &lt;/a&gt;
    1.1 准备hdfs中的用户目录
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在进行hdfs操作前需要切换用户到&amp;rsquo;hdfs&#39;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  # su - hdfs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为操作用户创建hdfs环境&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此处为ambari的admin用户准备环境：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	$ hdfs dfs -mkdir /user/admin
	$ hdfs dfs -chown admin:hdfs /user/admin
	$ hdfs dfs -ls /user
	$ hdfs dfs -chmod -R 770 /user/admin
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;12-配置ozzie&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#12-%e9%85%8d%e7%bd%aeozzie&#34;&gt;
        #
    &lt;/a&gt;
    1.2 配置ozzie
&lt;/div&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设置ozzie进程在hdfs中的proxy user&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  配置项：hadoop.proxyuser.oozie.groups
  值：$USER_GROUPS_THAT_ALLOW_IMPERSONATION

  配置项: hadoop.proxyuser.oozie.hosts
  值：$OOZIE_SERVER_HOSTNAME
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;PS. 代理机制&lt;/strong&gt; ：&lt;a href=&#34;http://dongxicheng.org/mapreduce-nextgen/hadoop-secure-impersonation/&#34;&gt;http://dongxicheng.org/mapreduce-nextgen/hadoop-secure-impersonation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://dongxicheng.org/wp-content/uploads/2014/03/Hadoop%E7%94%A8%E6%88%B7%E4%BC%AA%E8%A3%85%E6%9C%BA%E5%88%B6.jpg&#34; alt=&#34;proxy user mechanism&#34;&gt;&lt;/p&gt;
&lt;p&gt;主备namenode和resoucemanager（hadoop 2.0）上的core-site.xml中增加以下配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	&amp;lt;property&amp;gt;
	   &amp;lt;name&amp;gt;hadoop.proxyuser.oozie.groups&amp;lt;/name&amp;gt;
	   &amp;lt;value&amp;gt;group1,group2&amp;lt;value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property&amp;gt;
	    &amp;lt;name&amp;gt;hadoop.proxyuser.oozie.hosts&amp;lt;/name&amp;gt;
	    &amp;lt;value&amp;gt;host1,host2&amp;lt;value&amp;gt;
	&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里，假设用户user1属于group1（注意，这里的user1和group1都是linux用户和用户组，需要在namenode和jobtracker上进行添加），此外，为了限制客户端随意部署，超级用户代理功能只支持host1和host2两个节点。经过以上配置后，在host1和host2上的客户端上，属于group1和group2的用户可以sudo成oozie用户，执行作业流。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拷贝mysql-connector及配置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;切换到oozie用户后执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	# 复制mysql-connector
	$ hdfs dfs -put /$PATH/mysql-connector-java-5.1.37.jar /user/oozie/share/lib/lib_$TIMESTAMP/sqoop
	
	# 复制配置文件
	$ hdfs dfs -put /etc/hive/conf/hive-site.xml /user/oozie/share/lib/lib_20171226172323/hive
	$ hdfs dfs -put /etc/hive2/conf/hive-site.xml /user/oozie/share/lib/lib_20171226172323/hive2

	# 通知Ozzie使用新的sharelib
	$ oozie admin -sharelibupdate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.3/bk_workflow-management/content/copy_files.html&#34;&gt;HDP doc&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;sqoop基本操作&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sqoop%e5%9f%ba%e6%9c%ac%e6%93%8d%e4%bd%9c&#34;&gt;
        ##
    &lt;/a&gt;
    sqoop基本操作
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html#_importing_data_into_hive&#34;&gt;https://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html#_importing_data_into_hive&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;列举数据库(list-databases)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; # sqoop list-databases --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P
 SLF4J: Class path contains multiple SLF4J bindings.
 SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/accumulo/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
 17/12/25 18:27:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.3.0-235
 Enter password:
 17/12/25 18:27:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
 information_schema
 iot_test_12
 mysql
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;简单查询(eval)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; # sqoop eval --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --query &amp;quot;select date(add_time), count(*) from device group by date(add_time)&amp;quot;
 SLF4J: Class path contains multiple SLF4J bindings.
 SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/accumulo/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
 17/12/25 18:32:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.3.0-235
 Enter password:
 17/12/25 18:32:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
 -------------------------------------
 | date(add_time) | count(*)             |
 -------------------------------------
 | 2017-06-16 | 3                    |
 | 2017-06-23 | 1                    |
 | 2017-06-28 | 1                    |
 | 2017-07-06 | 1                    |
 | 2017-07-15 | 1                    |
 | 2017-07-24 | 6                    |
 | 2017-07-25 | 2                    |
 | 2017-07-26 | 11                   |
 | 2017-07-27 | 56                   |
 | 2017-07-28 | 2                    |
 | 2017-07-29 | 28                   |
 | 2017-07-31 | 373                  |
 | 2017-08-01 | 96                   |
 | 2017-08-02 | 3                    |
 | 2017-08-03 | 2                    |
 | 2017-08-09 | 2                    |
 | 2017-08-15 | 1                    |
 | 2017-08-19 | 1                    |
 | 2017-09-02 | 4                    |
 | 2017-09-07 | 4                    |
 | 2017-09-08 | 1                    |
 | 2017-09-19 | 3                    |
 | 2017-10-10 | 1                    |
 | 2017-11-02 | 2                    |
 | 2017-11-03 | 1                    |
 | 2017-11-17 | 1                    |
 | 2017-11-29 | 1                    |
 | 2017-12-04 | 2                    |
 | 2017-12-05 | 1                    |
 | 2017-12-07 | 1                    |
 | 2017-12-15 | 1                    |
 -------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导入hive&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;将一张表导入hive：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意，需要使用hive用户：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# su - hive
$ sqoop-import --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --table device --hive-import -m 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;导入后的数据可以从hdfs中看到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ hdfs dfs -ls /apps/hive/warehouse/device
Found 1 items
-rwxrwxrwx   3 hive hadoop      62177 2017-12-27 18:11 /apps/hive/warehouse/device/part-m-00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;配置hive-site.xml的warehouse：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.warehouse.subdir.inherit.perms&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;将所有表导入hive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  # su - hive
  $ sqoop import-all-tables --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --warehouse-dir /apps/hive/warehouse/IOT --hive-import --create-hive-table -m 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指定目标目录（&amp;ndash;warehouse-dir /apps/hive/warehouse/IOT）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;从hive导出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $ sqoop export --connect jdbc:mysql://localhost/db --username root --table employee --export-dir /emp/emp_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Job管理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; sqoop job --list

 sqoop job --show &#39;jobname&#39;

 sqoop job --exec &#39;jobname&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;issue-1&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#issue-1&#34;&gt;
        ##
    &lt;/a&gt;
    Issue 1:
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Application is added to the scheduler and is not yet activated. Queue&amp;rsquo;s AM resource limit exceeded. Details :
AM Partition = &amp;lt;DEFAULT_PARTITION&amp;gt;;
AM Resource Request = &amp;lt;memory:512, vCores:1&amp;gt;;
Queue Resource Limit for AM = &amp;lt;memory:512, vCores:1&amp;gt;;
User AM Resource Limit of the queue = &amp;lt;memory:512, vCores:1&amp;gt;;
Queue AM Resource Usage = &amp;lt;memory:1023, vCores:1&amp;gt;;&lt;/p&gt;
&lt;h1 id=&#34;创建工作流wfm&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%88%9b%e5%bb%ba%e5%b7%a5%e4%bd%9c%e6%b5%81wfm&#34;&gt;
        ##
    &lt;/a&gt;
    创建工作流(WFM)
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://community.hortonworks.com/articles/84394/apache-ambari-workflow-manager-view-for-apache-ooz-3.html&#34;&gt;https://community.hortonworks.com/articles/84394/apache-ambari-workflow-manager-view-for-apache-ooz-3.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1创建动作用sqoop获取数据&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#1%e5%88%9b%e5%bb%ba%e5%8a%a8%e4%bd%9c%e7%94%a8sqoop%e8%8e%b7%e5%8f%96%e6%95%b0%e6%8d%ae&#34;&gt;
        #
    &lt;/a&gt;
    1.创建动作用sqoop获取数据
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;​Create the Sqoop Action to Extract Data&lt;/p&gt;
&lt;p&gt;Command:
import &amp;ndash;connect jdbc:mysql://192.168.1.1/iot_test_12 &amp;ndash;username iot &amp;ndash;password-file /user/admin/iot.passwd &amp;ndash;table iottest &amp;ndash;split-by rowkey &amp;ndash;hive-import -m 1&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/sqoop/wfm-sqoop-command.png&#34; alt=&#34;cmd&#34;&gt;&lt;/p&gt;
&lt;p&gt;Advanced -&amp;gt; File:&lt;/p&gt;
&lt;p&gt;/user/oozie/share/lib/lib_20171226172323/hive/hive-site.xml
&lt;img src=&#34;https://www.singleye.net/media/2017/12/sqoop/wfm-sqoop-adv.png&#34; alt=&#34;cmd&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#2&#34;&gt;
        #
    &lt;/a&gt;
    2.
&lt;/div&gt;
&lt;/h2&gt;
&lt;h1 id=&#34;ambari-restart&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ambari-restart&#34;&gt;
        ##
    &lt;/a&gt;
    Ambari restart
&lt;/div&gt;
&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Shut down all services using Ambari.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shutdown ambari-agents on all nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shutdown ambari-server.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reboot all nodes as required .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart ambari-server, agents and services in that order.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;hadoop&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#hadoop&#34;&gt;
        ##
    &lt;/a&gt;
    hadoop
&lt;/div&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;停止Hadoop任务：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  hadoop job -kill job-id
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;misc&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#misc&#34;&gt;
        ##
    &lt;/a&gt;
    misc
&lt;/div&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;apache-tezhttpstezapacheorg&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#apache-tezhttpstezapacheorg&#34;&gt;
        #
    &lt;/a&gt;
    &lt;a href=&#34;https://tez.apache.org&#34;&gt;Apache Tez&lt;/a&gt;
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;The Apache TEZ® project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is currently built atop Apache Hadoop YARN.&lt;/p&gt;
&lt;p&gt;Tez &amp;mdash; (manage DAG task)  &amp;mdash;&amp;gt; Yarn&lt;/p&gt;
&lt;p&gt;Empowering end users by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expressive dataflow definition APIs&lt;/li&gt;
&lt;li&gt;Flexible Input-Processor-Output runtime model&lt;/li&gt;
&lt;li&gt;Data type agnostic&lt;/li&gt;
&lt;li&gt;Simplifying deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Execution Performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance gains over Map Reduce&lt;/li&gt;
&lt;li&gt;Optimal resource management&lt;/li&gt;
&lt;li&gt;Plan reconfiguration at runtime&lt;/li&gt;
&lt;li&gt;Dynamic physical data flow decisions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tez.apache.org/images/PigHiveQueryOnMR.png&#34; alt=&#34;Pig/Hive - MR&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tez.apache.org/images/PigHiveQueryOnTez.png&#34; alt=&#34;Pig/Hive - TEZ&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HDP中提交的Hive查询任务通过Tez调度执行&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Horwonworks HDP 2.6安装过程</title>
      <link>/2017/12/horwonworks-hdp-2.6%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Sat, 23 Dec 2017 15:38:37 +0800</pubDate>
      <author>**Email:** [singleye512@gmail.com](mailto:singleye512@gmail.com) (singleye)</author>
      <guid>/2017/12/horwonworks-hdp-2.6%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B/</guid>
      <description>&lt;h1 id=&#34;安装ambari-server&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e5%ae%89%e8%a3%85ambari-server&#34;&gt;
        ##
    &lt;/a&gt;
    安装ambari-server
&lt;/div&gt;
&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;# yum install ambari-server
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;配置ambari-cluster&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%85%8d%e7%bd%aeambari-cluster&#34;&gt;
        ##
    &lt;/a&gt;
    配置ambari cluster
&lt;/div&gt;
&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;# ambari-server setup
# ambari-server start
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;配置cluster&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#%e9%85%8d%e7%bd%aecluster&#34;&gt;
        ##
    &lt;/a&gt;
    配置cluster
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;步骤1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将clsuter中的所有node设置成ssh免密码登录了的方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤2:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;访问 http://&amp;lt;ambari-server&amp;gt;:8080&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤3:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设置cluster名称&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-001.png&#34; alt=&#34;001&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤4:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;配置安装源，可以使用私有源：&lt;/p&gt;
&lt;p&gt;HDP私有源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://hdp-repo.iwaterdata.com:7300/HDP/centos7/2.x/updates/2.6.3.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;HDP-UTILS私有源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://hdp-repo.iwaterdata.com:7300/HDP-UTILS-1.1.0.21/repos/centos7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-002.png&#34; alt=&#34;002&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤5:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;安装程序检查配置节点&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-003.png&#34; alt=&#34;003&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤6:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;选择安装软件列表&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-004.png&#34; alt=&#34;004&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤7:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设置各软件的配置信息&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-005.png&#34; alt=&#34;005&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤8:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;安装完成后程序会尝试启动各服务，有可能启动会失败（比如内存不足无法启动所有程序），只要确定程序正确安装可以继续&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.singleye.net/media/2017/12/hdp/hdp-setup-006.png&#34; alt=&#34;006&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;misc&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#misc&#34;&gt;
        ##
    &lt;/a&gt;
    misc
&lt;/div&gt;
&lt;/h1&gt;
&lt;p&gt;设置JDBC driver(Customize service中 oozie需要)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
