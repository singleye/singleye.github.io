<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>在 Apple silicon (M3 Max) 上对 Llama2 进行微调 - My New Hugo Site</title>

<meta name="description" content="">


    <meta name="keywords" content="llama2,AppleSilicon,M3Max,HuggingFace">




<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://localhost:1313/favicon.png">








    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
    

    
        <link rel="stylesheet" href="/css/style.03df79c682b91915c7cd261ecd1a6ec4d0fe668c98fa46310d0fbade319b11bd.css" integrity="sha256-A995xoK5GRXHzSYezRpuxND&#43;ZoyY&#43;kYxDQ&#43;63jGbEb0=">
    





    





    
    
    

    
        <link rel="stylesheet" href="/css/style.9c1888ebff42c0224ce04dac10cb2c401f1b77f54f78e8d87d73c3bed781c263.css" integrity="sha256-nBiI6/9CwCJM4E2sEMssQB8bd/VPeOjYfXPDvteBwmM=">
    





    





    
    
    

    
        <link rel="stylesheet" href="/css/style.acd606c0fce58853afe0248d37bb41acbbcdd8b1aca2412b6c0fa760da0137f3.css" integrity="sha256-rNYGwPzliFOv4CSNN7tBrLvN2LGsokErbA&#43;nYNoBN/M=">
    












    

    





    
    
    

    
        <script src="/js/script.672e2309c296e07c18bcd08b28d797a56222ff941d65f308fba3158c44885b14.js" type="text/javascript" charset="utf-8" integrity="sha256-Zy4jCcKW4HwYvNCLKNeXpWIi/5QdZfMI&#43;6MVjESIWxQ="></script>
    



















    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header">
            
                <div class="header-top">
    <div class="header-top-left">
        <h1 class="site-title noselect">
    <a href="/">My New Hugo Site</a>
</h1>

        







    
        <div class="theme-switcher">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>


</span>

        </div>
    

    <script>
        const STORAGE_KEY = 'user-color-scheme'
        const defaultTheme = "light"

        let currentTheme
        let switchButton
        let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

        function switchTheme(e) {
            currentTheme = (currentTheme === 'dark') ? 'light' : 'dark';
            if (localStorage) localStorage.setItem(STORAGE_KEY, currentTheme);
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        const autoChangeScheme = e => {
            currentTheme = e.matches ? 'dark' : 'light'
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        document.addEventListener('DOMContentLoaded', function () {
            switchButton = document.querySelector('.theme-switcher')
            currentTheme = detectCurrentScheme()

            if (currentTheme === 'auto') {
                autoChangeScheme(autoDefinedScheme);
                autoDefinedScheme.addListener(autoChangeScheme);
            } else {
                document.documentElement.setAttribute('data-theme', currentTheme)
            }

            if (switchButton) {
                switchButton.addEventListener('click', switchTheme, false)
            }

            showContent();
        })

        function detectCurrentScheme() {
            if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
                return localStorage.getItem(STORAGE_KEY)
            }
            if (defaultTheme) {
                return defaultTheme
            }
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function showContent() {
            document.body.style.visibility = 'visible';
            document.body.style.opacity = 1;
        }

        function changeGiscusTheme (theme) {
            function sendMessage(message) {
              const iframe = document.querySelector('iframe.giscus-frame');
              if (!iframe) return;
              iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
            }

            sendMessage({
              setConfig: {
                theme: theme
              }
            });
        }
    </script>


        <ul class="social-icons noselect">







    <li>
            <a href="/index.xml" title="RSS" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 1 0 2 0a1 1 0 1 0 -2 0" /><path d="M4 4a16 16 0 0 1 16 16" /><path d="M4 11a9 9 0 0 1 9 9" /></svg>


</span>

            </a>
        </li>
    

</ul>

    </div>
    <div class="header-top-right">

    </div>
</div>


    <nav class="noselect">
        
        
        <a class="" href="http://localhost:1313/" title="">Home</a>
        
        <a class="" href="http://localhost:1313/posts/" title="">Posts</a>
        
        <a class="" href="http://localhost:1313/project/" title="">Project</a>
        
        <a class="" href="http://localhost:1313/about/" title="">About</a>
        
        <a class="" href="http://localhost:1313/journey/" title="">Journey</a>
        
        <a class="" href="http://localhost:1313/contact/" title="">Contact</a>
        
    </nav>



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>





            
        </header>
        <main id="main" tabindex="-1">
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">在 Apple silicon (M3 Max) 上对 Llama2 进行微调</h1>
                

            </header>
            



<div class="post-info noselect">
    
        <div class="post-date dt-published">
            <time datetime="2024-04-26">2024-04-26</time>
            
        </div>
    

    <a class="post-hidden-url u-url" href="/post/machinelearning/llama2_mps_fine_tuning/">/post/machinelearning/llama2_mps_fine_tuning/</a>
    <a href="http://localhost:1313/" class="p-name p-author post-hidden-author h-card" rel="me"></a>


    <div class="post-taxonomies">
        
            <ul class="post-categories">
                
                    
                    <li><a href="/categories/ai/">AI</a></li>
                
                    
                    <li><a href="/categories/nlp/">NLP</a></li>
                
                    
                    <li><a href="/categories/llm/">LLM</a></li>
                
                    
                    <li><a href="/categories/llama2/">Llama2</a></li>
                
            </ul>
        
        
            <ul class="post-tags">
                
                    
                    <li><a href="/tags/llama2/">#Llama2</a></li>
                
                    
                    <li><a href="/tags/applesilicon/">#AppleSilicon</a></li>
                
                    
                    <li><a href="/tags/m3max/">#M3Max</a></li>
                
                    
                    <li><a href="/tags/huggingface/">#HuggingFace</a></li>
                
            </ul>
        
        
    </div>
</div>

        </div>
        

  
  




  
  



<script>
  var toc = document.querySelector(".toc");
  if (toc) {
    toc.addEventListener("click", function () {
      if (event.target.tagName !== "A") {
        event.preventDefault();
        if (this.open) {
          this.open = false;
          this.classList.remove("expanded");
        } else {
          this.open = true;
          this.classList.add("expanded");
        }
      }
    });
  }
</script>

        <div class="content e-content">
            <!-- raw HTML omitted -->
<p>参考 <a href="https://www.datacamp.com/tutorial/fine-tuning-llama-2">https://www.datacamp.com/tutorial/fine-tuning-llama-2</a> 进行Llama2 微调训练时发现使用稳重代码无法在 Apple M3 Max 上运行起来，经过一番实验后得以顺利运行，下面把过程记录下来。</p>
<p>相关代码： <a href="https://github.com/singleye/Llama2-finetune">https://github.com/singleye/Llama2-finetune</a></p>
<h1 id="1-准备" >
<div>
    <a href="#1-%e5%87%86%e5%a4%87">
        ##
    </a>
    1. 准备
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>pip install accelerate peft bitsandbytes transformers<span style="color:#f92672">==</span>4.38.1 trl
</span></span></code></pre></div><p>注意：</p>
<ul>
<li>transformers 不能使用 4.38.2 版本，否则在 M3 上会碰到下面的错误</li>
</ul>
<pre tabindex="0"><code>RuntimeError: User specified an unsupported autocast device_type &#39;mps&#39;
</code></pre><ul>
<li>bitsandbytes 无法在 M3 上使用</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> (
</span></span><span style="display:flex;"><span>    AutoModelForCausalLM,
</span></span><span style="display:flex;"><span>    AutoTokenizer,
</span></span><span style="display:flex;"><span>    BitsAndBytesConfig,
</span></span><span style="display:flex;"><span>    TrainingArguments,
</span></span><span style="display:flex;"><span>    pipeline,
</span></span><span style="display:flex;"><span>    logging,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> peft <span style="color:#f92672">import</span> LoraConfig
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> trl <span style="color:#f92672">import</span> SFTTrainer
</span></span></code></pre></div><h1 id="2-模型配置" >
<div>
    <a href="#2-%e6%a8%a1%e5%9e%8b%e9%85%8d%e7%bd%ae">
        ##
    </a>
    2. 模型配置
</div>
</h1>
<p>由于国内直接从 HuggingFace 网站下载模型速度太慢，可以使用镜像站进行下载。</p>
<p>设置环境变量 HF_ENDPOINT：</p>
<pre tabindex="0"><code>export HF_ENDPOINT=https://hf-mirror.com
</code></pre><p>下载模型：</p>
<pre tabindex="0"><code>huggingface-cli download --resume-download NousResearch/Llama-2-7b-chat-hf --local-dir Llama-2-7b-chat-hf
</code></pre><p>下载数据集：</p>
<pre tabindex="0"><code>huggingface-cli download --repo-type dataset --resume-download mlabonne/guanaco-llama2-1k --local-dir guanaco-llama2-1k
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>base_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;～/Llama2-finetuning&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model from local directory</span>
</span></span><span style="display:flex;"><span>base_model <span style="color:#f92672">=</span> base_dir <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/Llama-2-7b-chat-hf&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Dataset from local directory</span>
</span></span><span style="display:flex;"><span>guanaco_dataset <span style="color:#f92672">=</span> base_dir <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/guanaco-llama2-1k&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fine-tuned model</span>
</span></span><span style="display:flex;"><span>new_model <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;llama-2-7b-chat-guanaco&#34;</span>
</span></span></code></pre></div><h1 id="3-加载数据集" >
<div>
    <a href="#3-%e5%8a%a0%e8%bd%bd%e6%95%b0%e6%8d%ae%e9%9b%86">
        ##
    </a>
    3. 加载数据集
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> load_dataset(guanaco_dataset, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>)
</span></span></code></pre></div><h1 id="4-qlora-4-bit-量化配置-m3-跳过" >
<div>
    <a href="#4-qlora-4-bit-%e9%87%8f%e5%8c%96%e9%85%8d%e7%bd%ae-m3-%e8%b7%b3%e8%bf%87">
        ##
    </a>
    4. QLoRA 4-bit 量化配置 (M3 跳过)
</div>
</h1>
<p>Paper: <a href="https://arxiv.org/abs/2305.14314">&ldquo;QLoRA: Efficient Finetuning of Quantized LLMs&rdquo;</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>compute_dtype <span style="color:#f92672">=</span> getattr(torch, <span style="color:#e6db74">&#34;float16&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quant_config <span style="color:#f92672">=</span> BitsAndBytesConfig(
</span></span><span style="display:flex;"><span>    load_in_4bit<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_quant_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nf4&#34;</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_compute_dtype<span style="color:#f92672">=</span>compute_dtype,
</span></span><span style="display:flex;"><span>    bnb_4bit_use_double_quant<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h1 id="5-加载模型" >
<div>
    <a href="#5-%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b">
        ##
    </a>
    5. 加载模型
</div>
</h1>
<p>注意，由于 BitsAndBytesConfig 无法在 Apple Silicon (M3) 上使用，所以需要进行平台判断并做相应处理。由于无法使用量化方法进行处理，所以在 Apple Silicon (M3) 上需要使用更多的内存进行微调训练，在这个例子中大约使用了 75 GB 的内存。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>compute_dtype <span style="color:#f92672">=</span> getattr(torch, <span style="color:#e6db74">&#34;float16&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quant_config <span style="color:#f92672">=</span> BitsAndBytesConfig(
</span></span><span style="display:flex;"><span>    load_in_4bit<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_quant_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nf4&#34;</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_compute_dtype<span style="color:#f92672">=</span>compute_dtype,
</span></span><span style="display:flex;"><span>    bnb_4bit_use_double_quant<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Using &#39;mps&#39; (Apple Silicon)&#34;</span>)
</span></span><span style="display:flex;"><span>    active_device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;mps&#39;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        pretrained_model_name_or_path<span style="color:#f92672">=</span>base_model,
</span></span><span style="display:flex;"><span>        trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        low_cpu_mem_usage<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span>active_device
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">elif</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Using GPU&#34;</span>)
</span></span><span style="display:flex;"><span>    active_device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        base_model,
</span></span><span style="display:flex;"><span>        quantization_config<span style="color:#f92672">=</span>quant_config,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span>active_device
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Using CPU&#34;</span>)
</span></span><span style="display:flex;"><span>    active_device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        base_model,
</span></span><span style="display:flex;"><span>        quantization_config<span style="color:#f92672">=</span>quant_config,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span>active_device
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>use_cache <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>pretraining_tp <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span></code></pre></div><h1 id="6-加载模型的-tokenizer" >
<div>
    <a href="#6-%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b%e7%9a%84-tokenizer">
        ##
    </a>
    6. 加载模型的 tokenizer
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(base_model, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>pad_token <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>eos_token
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>padding_side <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;right&#34;</span>
</span></span></code></pre></div><h1 id="7-配置-peft-参数" >
<div>
    <a href="#7-%e9%85%8d%e7%bd%ae-peft-%e5%8f%82%e6%95%b0">
        ##
    </a>
    7. 配置 PEFT 参数
</div>
</h1>
<ul>
<li><a href="https://huggingface.co/docs/peft/conceptual_guides/lora">Parameter-Efficient Fine-Tuning (PEFT) </a></li>
<li><a href="https://arxiv.org/abs/2305.14314">&ldquo;QLoRA&rdquo;</a>
<img src="https://images.datacamp.com/image/upload/v1697713094/image7_3e12912d0d.png" alt="QLoRA"></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>peft_params <span style="color:#f92672">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>    lora_dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    r<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>,
</span></span><span style="display:flex;"><span>    bias<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;none&#34;</span>,
</span></span><span style="display:flex;"><span>    task_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CAUSAL_LM&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h1 id="8-配置训练参数" >
<div>
    <a href="#8-%e9%85%8d%e7%bd%ae%e8%ae%ad%e7%bb%83%e5%8f%82%e6%95%b0">
        ##
    </a>
    8. 配置训练参数
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>training_params <span style="color:#f92672">=</span> TrainingArguments(
</span></span><span style="display:flex;"><span>	output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./results&#34;</span>,
</span></span><span style="display:flex;"><span>	num_train_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>	per_device_train_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>	gradient_accumulation_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>	gradient_checkpointing <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>	learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-4</span>,
</span></span><span style="display:flex;"><span>	weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>,
</span></span><span style="display:flex;"><span>	lr_scheduler_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;constant&#34;</span>,
</span></span><span style="display:flex;"><span>	warmup_ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">0.03</span>,
</span></span><span style="display:flex;"><span>	max_grad_norm<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,
</span></span><span style="display:flex;"><span>	max_steps<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>	save_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>,
</span></span><span style="display:flex;"><span>	logging_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>,
</span></span><span style="display:flex;"><span>	logging_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./logs&#34;</span>, 
</span></span><span style="display:flex;"><span>	group_by_length<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>	fp16<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>	report_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tensorboard&#34;</span>,
</span></span><span style="display:flex;"><span>	adam_beta2<span style="color:#f92672">=</span><span style="color:#ae81ff">0.999</span>,
</span></span><span style="display:flex;"><span>	do_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h1 id="9-模型微调训练" >
<div>
    <a href="#9-%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83%e8%ae%ad%e7%bb%83">
        ##
    </a>
    9. 模型微调训练
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> SFTTrainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>model,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#f92672">=</span>dataset_train,
</span></span><span style="display:flex;"><span>    peft_config<span style="color:#f92672">=</span>peft_params,
</span></span><span style="display:flex;"><span>    dataset_text_field<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text&#34;</span>,
</span></span><span style="display:flex;"><span>    max_seq_length<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    tokenizer<span style="color:#f92672">=</span>tokenizer,
</span></span><span style="display:flex;"><span>    args<span style="color:#f92672">=</span>training_params,
</span></span><span style="display:flex;"><span>    packing<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span></code></pre></div><h1 id="10-保存训练好的模型" >
<div>
    <a href="#10-%e4%bf%9d%e5%ad%98%e8%ae%ad%e7%bb%83%e5%a5%bd%e7%9a%84%e6%a8%a1%e5%9e%8b">
        ##
    </a>
    10. 保存训练好的模型
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>save_pretrained(new_model)
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>save_pretrained(new_model)
</span></span></code></pre></div><h1 id="11-使用模型" >
<div>
    <a href="#11-%e4%bd%bf%e7%94%a8%e6%a8%a1%e5%9e%8b">
        ##
    </a>
    11. 使用模型
</div>
</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>logging<span style="color:#f92672">.</span>set_verbosity(logging<span style="color:#f92672">.</span>CRITICAL)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Who is Leonardo Da Vinci?&#34;</span>
</span></span><span style="display:flex;"><span>pipe <span style="color:#f92672">=</span> pipeline(task<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text-generation&#34;</span>, model<span style="color:#f92672">=</span>model, tokenizer<span style="color:#f92672">=</span>tokenizer, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> pipe(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;&lt;s&gt;[INST] </span><span style="color:#e6db74">{</span>prompt<span style="color:#e6db74">}</span><span style="color:#e6db74"> [/INST]&#34;</span>)
</span></span><span style="display:flex;"><span>print(result[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;generated_text&#39;</span>])
</span></span></code></pre></div>
        </div>

    </article>

    
    

    
        
        
            <h3 class="read-next-title noselect">Read next</h3>
            <ul class="read-next-posts noselect">
                
                <li><a href="/post/machinelearning/tmux_ai_helper/">tmux AI 助手</a></li>
                
            </ul>
        
    

    

    
        









    

    

    

        </main>
        
            <footer class="common-footer noselect">
    
    

    <div class="common-footer-bottom">
        

        <div style="display: flex; align-items: center; gap:8px">
            © 2024
            
        </div>
        <div style="display:flex;align-items: center">
            
            
            
            
            
            
        </div>
        <div>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/Junyi-99/hugo-theme-anubis2">Anubis2</a>.<br>
            

        </div>
    </div>

    <p class="h-card vcard">

    <a href=http://localhost:1313/ class="p-name u-url url fn" rel="me"></a>

    

    
</p>

</footer>

        
    </div>
</body>
</html>
