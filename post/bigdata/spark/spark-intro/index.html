<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Spark开发 - My New Hugo Site</title>

<meta name="description" content="# 使用scala进行开发 ## Step1: 安装sbt 配置源：
## Step2: 创建项目 可以使用Giter8模版创建项目。
spark相关的几个模版：
holdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project). imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark) nttdata-oss/basic-spark-project.">


    <meta name="keywords" content="spark,scala,python">




<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://localhost:1313/favicon.png">








    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
    

    
        <link rel="stylesheet" href="/css/style.03df79c682b91915c7cd261ecd1a6ec4d0fe668c98fa46310d0fbade319b11bd.css" integrity="sha256-A995xoK5GRXHzSYezRpuxND&#43;ZoyY&#43;kYxDQ&#43;63jGbEb0=">
    





    





    
    
    

    
        <link rel="stylesheet" href="/css/style.9c1888ebff42c0224ce04dac10cb2c401f1b77f54f78e8d87d73c3bed781c263.css" integrity="sha256-nBiI6/9CwCJM4E2sEMssQB8bd/VPeOjYfXPDvteBwmM=">
    





    





    
    
    

    
        <link rel="stylesheet" href="/css/style.acd606c0fce58853afe0248d37bb41acbbcdd8b1aca2412b6c0fa760da0137f3.css" integrity="sha256-rNYGwPzliFOv4CSNN7tBrLvN2LGsokErbA&#43;nYNoBN/M=">
    












    

    





    
    
    

    
        <script src="/js/script.672e2309c296e07c18bcd08b28d797a56222ff941d65f308fba3158c44885b14.js" type="text/javascript" charset="utf-8" integrity="sha256-Zy4jCcKW4HwYvNCLKNeXpWIi/5QdZfMI&#43;6MVjESIWxQ="></script>
    



















    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header">
            
                <div class="header-top">
    <div class="header-top-left">
        <h1 class="site-title noselect">
    <a href="/">My New Hugo Site</a>
</h1>

        







    
        <div class="theme-switcher">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>


</span>

        </div>
    

    <script>
        const STORAGE_KEY = 'user-color-scheme'
        const defaultTheme = "light"

        let currentTheme
        let switchButton
        let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

        function switchTheme(e) {
            currentTheme = (currentTheme === 'dark') ? 'light' : 'dark';
            if (localStorage) localStorage.setItem(STORAGE_KEY, currentTheme);
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        const autoChangeScheme = e => {
            currentTheme = e.matches ? 'dark' : 'light'
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        document.addEventListener('DOMContentLoaded', function () {
            switchButton = document.querySelector('.theme-switcher')
            currentTheme = detectCurrentScheme()

            if (currentTheme === 'auto') {
                autoChangeScheme(autoDefinedScheme);
                autoDefinedScheme.addListener(autoChangeScheme);
            } else {
                document.documentElement.setAttribute('data-theme', currentTheme)
            }

            if (switchButton) {
                switchButton.addEventListener('click', switchTheme, false)
            }

            showContent();
        })

        function detectCurrentScheme() {
            if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
                return localStorage.getItem(STORAGE_KEY)
            }
            if (defaultTheme) {
                return defaultTheme
            }
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function showContent() {
            document.body.style.visibility = 'visible';
            document.body.style.opacity = 1;
        }

        function changeGiscusTheme (theme) {
            function sendMessage(message) {
              const iframe = document.querySelector('iframe.giscus-frame');
              if (!iframe) return;
              iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
            }

            sendMessage({
              setConfig: {
                theme: theme
              }
            });
        }
    </script>


        <ul class="social-icons noselect">







    <li>
            <a href="/index.xml" title="RSS" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 1 0 2 0a1 1 0 1 0 -2 0" /><path d="M4 4a16 16 0 0 1 16 16" /><path d="M4 11a9 9 0 0 1 9 9" /></svg>


</span>

            </a>
        </li>
    

</ul>

    </div>
    <div class="header-top-right">

    </div>
</div>


    <nav class="noselect">
        
        
        <a class="" href="http://localhost:1313/" title="">Home</a>
        
        <a class="" href="http://localhost:1313/posts/" title="">Posts</a>
        
        <a class="" href="http://localhost:1313/project/" title="">Project</a>
        
        <a class="" href="http://localhost:1313/about/" title="">About</a>
        
        <a class="" href="http://localhost:1313/journey/" title="">Journey</a>
        
        <a class="" href="http://localhost:1313/contact/" title="">Contact</a>
        
    </nav>



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>





            
        </header>
        <main id="main" tabindex="-1">
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">Spark开发</h1>
                

            </header>
            



<div class="post-info noselect">
    
        <div class="post-date dt-published">
            <time datetime="2018-01-11">2018-01-11</time>
            
        </div>
    

    <a class="post-hidden-url u-url" href="/post/bigdata/spark/spark-intro/">/post/bigdata/spark/spark-intro/</a>
    <a href="http://localhost:1313/" class="p-name p-author post-hidden-author h-card" rel="me"></a>


    <div class="post-taxonomies">
        
            <ul class="post-categories">
                
                    
                    <li><a href="/categories/bigdata/">BigData</a></li>
                
                    
                    <li><a href="/categories/spark/">Spark</a></li>
                
            </ul>
        
        
            <ul class="post-tags">
                
                    
                    <li><a href="/tags/spark/">#Spark</a></li>
                
                    
                    <li><a href="/tags/scala/">#Scala</a></li>
                
                    
                    <li><a href="/tags/python/">#Python</a></li>
                
            </ul>
        
        
    </div>
</div>

        </div>
        

  
  




  
  
  
  <details class="toc noselect">
    <summary>Table of Contents</summary>
    <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#使用scala进行开发">使用scala进行开发</a>
      <ul>
        <li><a href="#step1-安装sbt">Step1: 安装sbt</a></li>
        <li><a href="#step2-创建项目">Step2: 创建项目</a></li>
        <li><a href="#step3-编写代码">Step3: 编写代码</a></li>
        <li><a href="#step4-编译打包">Step4: 编译打包</a></li>
        <li><a href="#step5-部署运行">Step5: 部署运行</a></li>
      </ul>
    </li>
    <li><a href="#使用python进行开发">使用python进行开发</a>
      <ul>
        <li><a href="#使用hdp环境提交任务">使用HDP环境提交任务</a></li>
        <li><a href="#在hdp环境运行jupyter-server">在HDP环境运行jupyter server</a></li>
        <li><a href="#非hdp环境开发">非HDP环境开发</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#api">API</a></li>
    <li><a href="#sparksession-vs-sparkcontext">SparkSession vs SparkContext</a>
      <ul>
        <li><a href="#sparkconf">SparkConf</a></li>
        <li><a href="#sparksql">SparkSQL</a></li>
        <li><a href="#访问catalog-metadata">访问Catalog Metadata</a></li>
      </ul>
    </li>
    <li><a href="#dataframe-和-datasets">DataFrame 和 Datasets</a>
      <ul>
        <li><a href="#dataset">Dataset：</a></li>
        <li><a href="#dataframe">DataFrame：</a></li>
        <li><a href="#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow"><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row">Row</a></a></li>
        <li><a href="#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn"><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a></a></li>
        <li><a href="#处理时间">处理时间</a></li>
        <li><a href="#import-sparkimplicits_">import spark.implicits._</a></li>
      </ul>
    </li>
    <li><a href="#编程陷阱">编程陷阱</a>
      <ul>
        <li><a href="#陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表">陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
  </details>
  



<script>
  var toc = document.querySelector(".toc");
  if (toc) {
    toc.addEventListener("click", function () {
      if (event.target.tagName !== "A") {
        event.preventDefault();
        if (this.open) {
          this.open = false;
          this.classList.remove("expanded");
        } else {
          this.open = true;
          this.classList.add("expanded");
        }
      }
    });
  }
</script>

        <div class="content e-content">
            <h2 id="使用scala进行开发" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8scala%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91">
        #
    </a>
    使用scala进行开发
</div>
</h2>
<h3 id="step1-安装sbt" >
<div>
    <a href="#step1-%e5%ae%89%e8%a3%85sbt">
        ##
    </a>
    Step1: 安装sbt
</div>
</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>配置源：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="step2-创建项目" >
<div>
    <a href="#step2-%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae">
        ##
    </a>
    Step2: 创建项目
</div>
</h3>
<p>可以使用Giter8模版创建项目。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>spark相关的几个模版：</p>
<ul>
<li>holdenk/sparkProjectTemplate.g8 (Template for Scala Apache Spark project).</li>
<li>imarios/frameless.g8 (A simple frameless template to start with more expressive types for Spark)</li>
<li>nttdata-oss/basic-spark-project.g8 (Spark basic project.)</li>
<li>spark-jobserver/spark-jobserver.g8 (Template for Spark Jobserver)</li>
</ul>
<h3 id="step3-编写代码" >
<div>
    <a href="#step3-%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81">
        ##
    </a>
    Step3: 编写代码
</div>
</h3>
<p>创建出的项目目录中包含一下主要条目：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>编写的代码放在 &lsquo;src/main/scala/&rsquo; 目录中：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="step4-编译打包" >
<div>
    <a href="#step4-%e7%bc%96%e8%af%91%e6%89%93%e5%8c%85">
        ##
    </a>
    Step4: 编译打包
</div>
</h3>
<p>程序开发好之后首先需要编译打包：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>编译打包成功后的文件输出在 &rsquo;target/scala-2.11/&rsquo; 目录中：</p>
<p>/home/spark/scala/frameless/target/scala-2.11/frameless_2.11-0.1.jar</p>
<h3 id="step5-部署运行" >
<div>
    <a href="#step5-%e9%83%a8%e7%bd%b2%e8%bf%90%e8%a1%8c">
        ##
    </a>
    Step5: 部署运行
</div>
</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>&ndash;master: 指定spark driver的运行模式。
<ul>
<li>yarn-cluster: spark driver运行在yarn的application master进程中。如果应用只是进行计算可以使用这种方式运行，如果需要跟前台进行交互则可以考虑yarn-client模式。</li>
<li>yarn-client: spark driver运行在client进程中，此时application master只负责向yarn申请资源。</li>
</ul>
</li>
</ul>
<h2 id="使用python进行开发" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8python%e8%bf%9b%e8%a1%8c%e5%bc%80%e5%8f%91">
        #
    </a>
    使用python进行开发
</div>
</h2>
<h3 id="使用hdp环境提交任务" >
<div>
    <a href="#%e4%bd%bf%e7%94%a8hdp%e7%8e%af%e5%a2%83%e6%8f%90%e4%ba%a4%e4%bb%bb%e5%8a%a1">
        ##
    </a>
    使用HDP环境提交任务
</div>
</h3>
<p>创建python文件<code>pi.py</code></p>
<pre><code>import sys
from random import random
from operator import add

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;PythonPi&quot;)\.getOrCreate()

partitions = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 2
n = 100000 * partitions

def f(any):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0

count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
print(&quot;Pi is roughly %f&quot; % (4.0 * count / n))

spark.stop()
</code></pre>
<p>运行<code>pi.py</code>，这里指定使用yarn的cluster模式</p>
<pre><code>SPARK_MAJOR_VERSION=2 spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --deploy pi.py 10
</code></pre>
<h3 id="在hdp环境运行jupyter-server" >
<div>
    <a href="#%e5%9c%a8hdp%e7%8e%af%e5%a2%83%e8%bf%90%e8%a1%8cjupyter-server">
        ##
    </a>
    在HDP环境运行jupyter server
</div>
</h3>
<p>使用yarn的cluster模式运行，在运行期间会始终在yarn中占用资源，如果需要长期运行，使用<code>--master local</code>比较好</p>
<pre><code>XDG_RUNTIME_DIR=&quot;&quot; SPARK_MAJOR_VERSION=2 PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 121.43.171.231' pyspark --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m
</code></pre>
<p>为jupyter server设置密码等操作参考<a href="https://jupyter-notebook.readthedocs.io/en/stable/public_server.html">jupyter文档</a></p>
<p>运行后访问服务器8888端口即可访问jupyter服务</p>
<h3 id="非hdp环境开发" >
<div>
    <a href="#%e9%9d%9ehdp%e7%8e%af%e5%a2%83%e5%bc%80%e5%8f%91">
        ##
    </a>
    非HDP环境开发
</div>
</h3>
<p>确保本机有一份spark二进制文件，比如<code>spark-2.2.1-bin-hadoop2.7</code></p>
<p>安装pyspark</p>
<pre><code>pip install pyspark
</code></pre>
<p>设置SPARK_HOME（可选）</p>
<pre><code>export SPARK_HOME=/home/xxx/spark-2.2.1-bin-hadoop2.7
</code></pre>
<p>通过代码提交任务</p>
<pre><code>import os
from pyspark import SparkConf, SparkContext

# if `SPARK_HOME` is undefined yet
if 'SPARK_HOME' not in os.environ:
    os.environ['SPARK_HOME'] = '/home/xxx/spark-2.2.1-bin-hadoop2.7'

conf = SparkConf().setAppName('Demo').setMaster('yarn').set('spark.yarn.deploy.mode', 'cluster')
sc = SparkContext(conf=conf)

# Do something with sc...
</code></pre>
<p>或者使用<code>SparkSession</code> API</p>
<pre><code>from spark.sql import SparkSession
spark = (SparkSession.builder
          .master(&quot;yarn&quot;)
          .appName(&quot;Demo&quot;)
          .config(&quot;spark.yarn.deploy.mode&quot;, &quot;cluster&quot;)
          .getOrCreate())

# Do something with spark...
</code></pre>
<h1 id="oozie自动化" >
<div>
    <a href="#oozie%e8%87%aa%e5%8a%a8%e5%8c%96">
        ##
    </a>
    oozie自动化
</div>
</h1>
<p>TODO</p>
<h1 id="development" >
<div>
    <a href="#development">
        ##
    </a>
    Development
</div>
</h1>
<p>Spark 2.0之前的开发接口主要是RDD（Resilient Distributed Dataset），从2.0之后原有<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD接口</a>仍然支持，但RDD被Dataset取代，如果使用Dataset编程的话需要使用<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>。</p>
<h2 id="api" >
<div>
    <a href="#api">
        #
    </a>
    API
</div>
</h2>
<p><a href="http://spark.apache.org/docs/1.3.1/api/scala/index.html">http://spark.apache.org/docs/1.3.1/api/scala/index.html</a></p>
<h2 id="sparksession-vs-sparkcontext" >
<div>
    <a href="#sparksession-vs-sparkcontext">
        #
    </a>
    SparkSession vs SparkContext
</div>
</h2>
<p>Spark 2.0之前有3个主要的连接对象：</p>
<ul>
<li>SparkContext: 建立与Spark执行环境相关的连接，用于创建RDD</li>
<li>SqlContext：利用SparkContext背后的SparkSQL建立连接</li>
<li>HiveContext：创建访问hive的接口</li>
</ul>
<p>从Spark 2.0开始，Datasets/Dataframes成为主要的数据访问接口，SparkSession(org.apache.spark.sql.SparkSession)成为主要的访问Spark执行环境的接口。</p>
<h3 id="sparkconf" >
<div>
    <a href="#sparkconf">
        ##
    </a>
    SparkConf
</div>
</h3>
<p>Spark 2.0之前需要先创建SparkConf，在创建SparkContext。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Spark 2.0之后可以不用显示的创建SparkConf对象就可以创建SparkSession对象。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>在SparkSession创建完后，Spark的运行时配置属性还可以通过SparkSession.conf()被修改：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li><a href="http://blog.javachen.com/2015/06/07/spark-configuration.html">Spark conf参考</a></li>
</ul>
<h3 id="sparksql" >
<div>
    <a href="#sparksql">
        ##
    </a>
    SparkSQL
</div>
</h3>
<p>2.0之前需要通过创建SqlContext来使用SparkSQL。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>2.0后通过SparkSession可以很方便的访问SparkSession.sql()来使用SparkSQL。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="访问catalog-metadata" >
<div>
    <a href="#%e8%ae%bf%e9%97%aecatalog-metadata">
        ##
    </a>
    访问Catalog Metadata
</div>
</h3>
<p>SparkSession暴露了访问metastore的接口SparkSession.catalog，这个接口可以用来访问catalog metadata信息，比如（Hcatalog）</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="dataframe-和-datasets" >
<div>
    <a href="#dataframe-%e5%92%8c-datasets">
        #
    </a>
    DataFrame 和 Datasets
</div>
</h2>
<p><img src="https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png" alt="DataFrame &amp; Dataset"></p>
<h3 id="dataset" >
<div>
    <a href="#dataset">
        ##
    </a>
    Dataset：
</div>
</h3>
<ul>
<li>
<p>强类型</p>
</li>
<li>
<p>支持functional和relational操作</p>
</li>
<li>
<p>操作分类</p>
<ul>
<li>转换（transformation）：用于产生新的Datasets
<ul>
<li>例如：map，filter，select，aggregate（groupBy）</li>
</ul>
</li>
<li>操作（actions）：触发计算并返回结果
<ul>
<li>例如：count，show，把数据写回文件系统</li>
</ul>
</li>
</ul>
</li>
<li>
<p>lazy：计算只有到action触发的时候才进行。Dataset内部可以理解为存储了如何进行计算的计划。当action执行的时候Spark query optimizer生成并优化计算方案后将其执行。</p>
</li>
<li>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder">Encoder</a>：把JVM中的类型T跟Spark SQL的数据表现形式进行转换的机制。（marshal/unmarshal?）</p>
</li>
<li>
<p>两种创建方法：</p>
<ul>
<li>
<p>使用SparkSession的read()方法</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>对现有Dataset做转换（transformation）</p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ul>
<p>Dataset<!-- raw HTML omitted --> names = people.map((Person p) -&gt; p.name, Encoders.STRING));
<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<ul>
<li>
<p>Dataset操作也可以是untyped，通过：<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a>, <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a>, <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame的function</a>等。</p>
</li>
<li>
<p>列操作</p>
<ul>
<li>选择一列（抛弃其他列）：select(&ldquo;column&rdquo;)</li>
<li>增加一列：withColumn(&ldquo;newColumn&rdquo;, Column)</li>
<li>修改一列: withColumn(&ldquo;column&rdquo;, Column)</li>
<li>删除一列: drop(&ldquo;column&rdquo;)</li>
<li>类型转换：</li>
</ul>
</li>
<li>
<p>列高级操作：</p>
<ul>
<li>UDF:</li>
</ul>
</li>
</ul>
<h3 id="dataframe" >
<div>
    <a href="#dataframe">
        ##
    </a>
    DataFrame：
</div>
</h3>
<ul>
<li>Dataset的无类型view (untyped view)，对应Dataset的Row</li>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame操作Functions</a></li>
</ul>
<p>DataFrame是有名列（named columns）组成的数据集合，类似关系数据库中的表或者python/R中的pandas。</p>
<p>可以由多种数据源来构造DataFrame，比如Hive中的table，Spark的RDD（Resilient Distributed Datasets）。</p>
<p><strong>TODO: Datasets</strong></p>
<p>SparkSession有很多种方法创建DataFrame和Datasets。</p>
<table>
<thead>
<tr>
<th></th>
<th>DataFrame</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建API</td>
<td>SparkSession.createDataFrame</td>
<td></td>
</tr>
<tr>
<td>访问</td>
<td></td>
<td></td>
</tr>
<tr>
<td>其他</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th>Datasets</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建API</td>
<td></td>
<td></td>
</tr>
<tr>
<td>访问</td>
<td></td>
<td></td>
</tr>
<tr>
<td>其他</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow" >
<div>
    <a href="#rowhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlrow">
        ##
    </a>
    <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row">Row</a>
</div>
</h3>
<ul>
<li>创建Row</li>
<li>访问
<ul>
<li>
<p>generic访问，使用ordinal序列访问</p>
<p>row(0)：访问列第一个成员</p>
</li>
<li>
<p>原始类型访问</p>
</li>
</ul>
</li>
</ul>
<h3 id="columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn" >
<div>
    <a href="#columnhttpsparkapacheorgdocslatestapiscalaindexhtmlorgapachesparksqlcolumn">
        ##
    </a>
    <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column">Column</a>
</div>
</h3>
<ul>
<li>DataSet API:</li>
</ul>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset</a></p>
<h3 id="处理时间" >
<div>
    <a href="#%e5%a4%84%e7%90%86%e6%97%b6%e9%97%b4">
        ##
    </a>
    处理时间
</div>
</h3>
<h4 id="-ver--15" >
<div>
    <a href="#-ver--15">
        ###
    </a>
    * ver &lt;= 1.5
</div>
</h4>
<p>使用hiveContext来查询</p>
<h4 id="--15--ver--22" >
<div>
    <a href="#--15--ver--22">
        ###
    </a>
    *  1.5 &lt;= ver &lt; 2.2
</div>
</h4>
<p>Spark 1.5引入了unix_timestamp()，所以在1.5之后到2.2的spark版本中可以这样操作时间信息</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h4 id="-ver--22" >
<div>
    <a href="#-ver--22">
        ###
    </a>
    * ver &gt;= 2.2
</div>
</h4>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="import-sparkimplicits_" >
<div>
    <a href="#import-sparkimplicits_">
        ##
    </a>
    import spark.implicits._
</div>
</h3>
<p>引入toDF()方法和“$”操作符</p>
<h2 id="编程陷阱" >
<div>
    <a href="#%e7%bc%96%e7%a8%8b%e9%99%b7%e9%98%b1">
        #
    </a>
    编程陷阱
</div>
</h2>
<h3 id="陷阱1dataframe有元素但通过循环把值放入listbuffer后listbuffer却为空列表" >
<div>
    <a href="#%e9%99%b7%e9%98%b11dataframe%e6%9c%89%e5%85%83%e7%b4%a0%e4%bd%86%e9%80%9a%e8%bf%87%e5%be%aa%e7%8e%af%e6%8a%8a%e5%80%bc%e6%94%be%e5%85%a5listbuffer%e5%90%8elistbuffer%e5%8d%b4%e4%b8%ba%e7%a9%ba%e5%88%97%e8%a1%a8">
        ##
    </a>
    陷阱1：dataframe有元素，但通过循环把值放入ListBuffer()后ListBuffer()却为空列表
</div>
</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>可以看出两个问题：</p>
<ul>
<li>问题1: 虽然可以知道valueDF不为空，可是&quot;println(&hellip;)&ldquo;却没有任何输出内容</li>
<li>问题2: 结果valueList为空</li>
</ul>
<h4 id="原因" >
<div>
    <a href="#%e5%8e%9f%e5%9b%a0">
        ###
    </a>
    原因：
</div>
</h4>
<p>Spark任务执行是通过Driver/Executor来完成的，Driver控制任务执行，Executor负责任务执行，foreach/for会被Driver分配给Executor来执行（分布式执行），每个Executor创建一个自己的ListBuffer拷贝，因此当任务结束后信息就丢失了。</p>
<p>参考Stackoverflow的讨论：<a href="https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop">https://stackoverflow.com/questions/36203299/scala-listbuffer-emptying-itself-after-every-add-in-loop</a></p>
<h4 id="修复方法使用collect" >
<div>
    <a href="#%e4%bf%ae%e5%a4%8d%e6%96%b9%e6%b3%95%e4%bd%bf%e7%94%a8collect">
        ###
    </a>
    修复方法：使用collect()
</div>
</h4>
<p>collect()定义：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>修复代码如下：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h1 id="reference" >
<div>
    <a href="#reference">
        ##
    </a>
    Reference
</div>
</h1>
<ul>
<li><a href="https://community.hortonworks.com/articles/84071/apache-ambari-workflow-manager-view-for-apache-ooz-2.html">Ambari workflow manager</a></li>
<li><a href="http://docs.scala-lang.org/tour/basics.html">Scala basics</a></li>
<li><a href="https://github.com/foundweekends/giter8/wiki/giter8-templates">Giter8项目模版</a></li>
<li><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_spark-component-guide/content/ch_oozie-spark-action.html">Automating Spark job with Oozie</a></li>
<li><a href="https://community.hortonworks.com/articles/104949/using-virtualenv-with-pyspark-1.html">Using VirtualEnv with PySpark</a></li>
</ul>

        </div>

    </article>

    
    

    
        
        
            <h3 class="read-next-title noselect">Read next</h3>
            <ul class="read-next-posts noselect">
                
                <li><a href="/post/machinelearning/neuralnetwork/softmax/">softmax输出层公式推导及代码实验</a></li>
                
                <li><a href="/post/programming/python/python_main_global/">python中__main__的作用域及变量使用</a></li>
                
            </ul>
        
    

    

    
        









    

    

    

        </main>
        
            <footer class="common-footer noselect">
    
    

    <div class="common-footer-bottom">
        

        <div style="display: flex; align-items: center; gap:8px">
            © 2024
            
        </div>
        <div style="display:flex;align-items: center">
            
            
            
            
            
            
        </div>
        <div>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/Junyi-99/hugo-theme-anubis2">Anubis2</a>.<br>
            

        </div>
    </div>

    <p class="h-card vcard">

    <a href=http://localhost:1313/ class="p-name u-url url fn" rel="me"></a>

    

    
</p>

</footer>

        
    </div>
</body>
</html>
